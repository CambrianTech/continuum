# Remedies Document - Multi-Persona Chat System
**Date**: 2025-10-12
**Status**: Ready for implementation - all code locations verified
**Philosophy**: Natural AI reasoning > efficiency hacks

---

## Critical Issues (Must Fix for Production)

### Remedy #1: Add Request Queue to OllamaAdapter

**Problem**: 67% timeout rate when 3 personas make concurrent requests to 2-capacity Ollama

**File**: `daemons/ai-provider-daemon/shared/OllamaAdapter.ts`

**Solution**: Add client-side request queue

**Implementation** (add to OllamaAdapter.ts):

```typescript
// NEW: Add at top of file
interface QueuedRequest<T> {
  executor: () => Promise<T>;
  resolve: (value: T) => void;
  reject: (error: Error) => void;
}

class OllamaRequestQueue {
  private queue: Array<QueuedRequest<any>> = [];
  private activeRequests = 0;
  private readonly maxConcurrent = 2;  // Ollama's actual capacity

  async enqueue<T>(executor: () => Promise<T>): Promise<T> {
    return new Promise<T>((resolve, reject) => {
      this.queue.push({ executor, resolve, reject });
      this.processQueue();
    });
  }

  private async processQueue(): Promise<void> {
    if (this.activeRequests >= this.maxConcurrent || this.queue.length === 0) {
      return;
    }

    this.activeRequests++;
    const request = this.queue.shift()!;

    try {
      const result = await request.executor();
      request.resolve(result);
    } catch (error) {
      request.reject(error as Error);
    } finally {
      this.activeRequests--;
      this.processQueue();  // Process next
    }
  }
}

// ADD to OllamaAdapter class:
export class OllamaAdapter implements AIProvider {
  private config: OllamaConfig;
  private requestQueue: OllamaRequestQueue;  // NEW

  constructor(config?: Partial<OllamaConfig>) {
    this.config = { ...DEFAULT_CONFIG, ...config };
    this.requestQueue = new OllamaRequestQueue();  // NEW
  }

  // WRAP all public methods:
  async generateText(options: GenerateTextOptions): Promise<GenerateTextResult> {
    return this.requestQueue.enqueue(async () => {
      // Existing generateText logic here
      const response = await this.makeRequest<OllamaGenerateResponse>('/api/generate', {
        model: options.model,
        prompt: options.prompt,
        system: options.systemPrompt,
        stream: false
      });
      // ... rest of method
      return result;
    });
  }
}
```

**Why This Works**:
- Queue ensures max 2 concurrent requests to Ollama
- Remaining requests wait in client-side queue (no timeout)
- Simple, self-contained, no external dependencies
- **Expected Result**: 0% timeout rate

---

### Remedy #2: Add Temporal Filtering to RAG

**Problem**: RAG includes messages posted AFTER the triggering event (sees "future")

**Files**:
1. `system/rag/builders/ChatRAGBuilder.ts` (lines 43, 175-186)
2. `system/user/server/PersonaUser.ts` (lines 372, 1033)

**Solution**: Pass trigger timestamp, filter database query

**Implementation**:

**Step 1**: Update RAGBuildOptions interface (ChatRAGBuilder.ts ~line 16):
```typescript
// In RAGTypes.ts or at top of ChatRAGBuilder.ts
export interface RAGBuildOptions {
  maxMessages?: number;
  maxMemories?: number;
  includeArtifacts?: boolean;
  includeMemories?: boolean;
  currentMessage?: LLMMessage;
  triggerTimestamp?: Date;  // NEW: Only fetch messages before this time
}
```

**Step 2**: Update loadConversationHistory (ChatRAGBuilder.ts line 175):
```typescript
private async loadConversationHistory(
  roomId: UUID,
  personaId: UUID,
  maxMessages: number,
  triggerTimestamp?: Date  // NEW parameter
): Promise<LLMMessage[]> {
  try {
    // Build filters with temporal filtering
    const filters: any = { roomId };

    // NEW: Add timestamp filter if provided
    if (triggerTimestamp) {
      filters.timestamp = { $lt: triggerTimestamp.toISOString() };
    }

    const result = await DataDaemon.query<ChatMessageEntity>({
      collection: ChatMessageEntity.collection,
      filters,  // Now includes timestamp filter
      sort: [{ field: 'timestamp', direction: 'desc' }],
      limit: maxMessages
    });
    // ... rest of method
  }
}
```

**Step 3**: Thread timestamp through buildContext (ChatRAGBuilder.ts line 59):
```typescript
const conversationHistory = await this.loadConversationHistory(
  contextId,
  personaId,
  maxMessages,
  options?.triggerTimestamp  // NEW: Pass timestamp
);
```

**Step 4**: Update PersonaUser to pass timestamp:

**For gating** (PersonaUser.ts line 1033):
```typescript
const ragContext = await ragBuilder.buildContext(
  message.roomId,
  this.id,
  {
    maxMessages: 10,
    maxMemories: 0,
    includeArtifacts: false,
    includeMemories: false,
    triggerTimestamp: message.timestamp,  // NEW: Use trigger message timestamp
    currentMessage: {
      role: 'user',
      content: message.content.text,
      // ...
    }
  }
);
```

**For full response** (PersonaUser.ts line 372):
```typescript
const fullRAGContext = await ragBuilder.buildContext(
  originalMessage.roomId,
  this.id,
  {
    maxMessages: 20,
    maxMemories: 10,
    includeArtifacts: false,
    includeMemories: false,
    triggerTimestamp: originalMessage.timestamp,  // NEW
    currentMessage: {
      role: 'user',
      content: originalMessage.content.text,
      name: originalMessage.senderName,
      timestamp: this.timestampToNumber(originalMessage.timestamp)
    }
  }
);
```

**Why This Works**:
- Database query only returns messages with `timestamp < triggerTimestamp`
- Persona never sees "future" responses in RAG context
- Fixes temporal confusion at the source
- **Expected Result**: No hallucinations from seeing future messages

---

### Remedy #3: Add Response Validation Before Storage

**Problem**: Gibberish like "@@@@@@@@@..." stored to database, pollutes future RAG contexts

**File**: `system/user/server/PersonaUser.ts` (line 486)

**Solution**: Validate response quality BEFORE creating ChatMessageEntity

**Implementation**:

**Step 1**: Add validation method to PersonaUser (new method):
```typescript
/**
 * Validate response quality using BoW metrics
 */
private validateResponseQuality(text: string): {
  isValid: boolean;
  isCatastrophic: boolean;
  hasHallucinatedPrefix: boolean;
  issues: string[];
} {
  const issues: string[] = [];

  // Check 1: Catastrophic gibberish (repeated characters)
  const consecutiveRepeats = this.countConsecutiveRepeats(text);
  const symbolDensity = this.calculateSymbolDensity(text);

  if (consecutiveRepeats > 20 || symbolDensity > 0.8) {
    issues.push('CATASTROPHIC_GIBBERISH');
  }

  // Check 2: Hallucinated prefixes
  const hasPrefix = /^(\w+\s+AI:|Human:|\[\d{2}:\d{2}\])/. test(text);
  if (hasPrefix) {
    issues.push('HALLUCINATED_PREFIX');
  }

  // Check 3: Empty or too short
  if (text.trim().length < 5) {
    issues.push('TOO_SHORT');
  }

  // Check 4: Too long (possible repetition loop)
  if (text.length > 500) {
    issues.push('TOO_LONG');
  }

  return {
    isValid: issues.length === 0,
    isCatastrophic: issues.includes('CATASTROPHIC_GIBBERISH'),
    hasHallucinatedPrefix: issues.includes('HALLUCINATED_PREFIX'),
    issues
  };
}

private countConsecutiveRepeats(text: string): number {
  let maxRepeats = 0;
  let currentChar = '';
  let currentCount = 0;

  for (const char of text) {
    if (char === currentChar) {
      currentCount++;
      maxRepeats = Math.max(maxRepeats, currentCount);
    } else {
      currentChar = char;
      currentCount = 1;
    }
  }

  return maxRepeats;
}

private calculateSymbolDensity(text: string): number {
  const symbols = text.match(/[^a-zA-Z0-9\s]/g) || [];
  return text.length > 0 ? symbols.length / text.length : 0;
}

private stripHallucinatedPrefixes(text: string): string {
  return text.replace(/^(\w+\s+AI:|Human:|\[\d{2}:\d{2}\])\s*/, '');
}
```

**Step 2**: Add validation BEFORE storage (PersonaUser.ts AFTER line 483):
```typescript
// AFTER self-review passes (line 483), BEFORE creating responseMessage (line 486)

// Validate response quality
const validation = this.validateResponseQuality(aiResponse.text.trim());

if (validation.isCatastrophic) {
  console.log(`üö´ ${this.displayName}: Catastrophic response detected (${validation.issues.join(', ')}), discarding`);
  console.log(`   Would have said: "${aiResponse.text.slice(0, 80)}..."`);
  return; // Don't store gibberish
}

if (validation.hasHallucinatedPrefix) {
  aiResponse.text = this.stripHallucinatedPrefixes(aiResponse.text.trim());
  console.log(`üßπ ${this.displayName}: Stripped hallucinated prefixes from response`);
}

// NOW safe to create responseMessage
const responseMessage = new ChatMessageEntity();
// ...
```

**Why This Works**:
- Catches gibberish BEFORE it enters database
- Strips prefixes automatically (no manual cleanup needed)
- Lenient validation (only blocks catastrophic failures)
- **Expected Result**: No gibberish in database, no cascade failures

---

## High Priority Issues (Degraded Experience)

### Remedy #4: Fix Role Assignment in RAG

**Problem**: All AI personas marked as `role=user` in RAG, should be `role=assistant`

**File**: `system/rag/builders/ChatRAGBuilder.ts` (line 232)

**Solution**: Check sender type, not just "is own message"

**Implementation**:
```typescript
// ChatRAGBuilder.ts line 232 - BEFORE
const llmMessage = {
  role: isOwnMessage ? 'assistant' as const : 'user' as const,  // ‚ùå WRONG
  content: markedContent,
  name: msg.senderName,
  timestamp: timestampMs
};

// ChatRAGBuilder.ts line 232 - AFTER
const isAIMessage = msg.senderType === 'persona' || msg.senderType === 'agent';
const llmMessage = {
  role: (isOwnMessage || isAIMessage) ? 'assistant' as const : 'user' as const,  // ‚úÖ CORRECT
  content: markedContent,
  name: msg.senderName,
  timestamp: timestampMs
};
```

**Why This Works**:
- ChatMessageEntity already has `senderType` field (denormalized)
- All AI messages (persona, agent) correctly marked as `role=assistant`
- Only human messages marked as `role=user`
- **Expected Result**: LLMs understand who is human vs AI

---

### Remedy #5: Remove [QUESTION] Marker

**Problem**: `[QUESTION]` prefix adds noise, confuses small models

**File**: `system/rag/builders/ChatRAGBuilder.ts` (line 217)

**Solution**: Remove marker (simple deletion)

**Implementation**:
```typescript
// ChatRAGBuilder.ts line 212-217 - BEFORE
const isQuestion = messageText.trim().endsWith('?') ||
                  /\b(how|what|why|when|where|who|which|can|could|would|should|is|are|does|do)\b/i.test(messageText.substring(0, 50));

const markedContent = isQuestion ? `[QUESTION] ${messageText}` : messageText;

// ChatRAGBuilder.ts line 212-217 - AFTER
// Remove question detection and marker
const markedContent = messageText;  // No marker
```

**Why This Works**:
- Eliminates noise from RAG context
- LLMs can detect questions naturally
- Simpler is better for small models
- **Expected Result**: Cleaner RAG context, less confusion

---

### Remedy #6: Improve System Prompt

**Problem**: Negative instructions + mentions non-existent format + no human/AI distinction

**File**: `system/rag/builders/ChatRAGBuilder.ts` (lines 158-169)

**Solution**: Rewrite with positive instructions, add human/AI distinction

**Implementation**:
```typescript
// ChatRAGBuilder.ts buildSystemPrompt() - REPLACE lines 158-169

private async buildSystemPrompt(user: UserEntity, roomId: UUID): Promise<string> {
  const name = user.displayName;
  const bio = user.profile?.bio ?? user.shortDescription ?? '';
  const capabilities = user.capabilities?.autoResponds
    ? 'You respond naturally to conversations.'
    : 'You participate when mentioned or when the conversation is relevant.';

  // Load room members with type indication
  const membersList = await this.loadRoomMembersWithTypes(roomId);  // NEW helper
  const membersContext = membersList.length > 0
    ? `\n\nCurrent room members:\n${membersList.map(m => `- ${m.name} (${m.type})`).join('\n')}`
    : '';

  return `ü§ñ YOUR IDENTITY:
You are ${name}${bio ? ` - ${bio}` : ''}. ${capabilities}

üë• ROOM CONTEXT:
This is a multi-party group chat.${membersContext}

‚úçÔ∏è RESPONSE GUIDELINES:
1. Respond naturally and directly with your message
2. Keep responses concise (1-3 sentences)
3. Write as yourself - just your message text
4. Build on others' responses, don't repeat what's already said

üìã CONVERSATION FORMAT:
Messages appear as "[HH:MM] SpeakerName: message" to show timing and speaker.
When you respond, write ONLY your message text.`;
}

// NEW helper method
private async loadRoomMembersWithTypes(roomId: UUID): Promise<Array<{name: string; type: string}>> {
  try {
    const roomResult = await DataDaemon.read<RoomEntity>(RoomEntity.collection, roomId);
    if (!roomResult.success || !roomResult.data) {
      return [];
    }

    const room = roomResult.data.data;
    if (!room.members || room.members.length === 0) {
      return [];
    }

    const members: Array<{name: string; type: string}> = [];
    for (const member of room.members) {
      const userResult = await DataDaemon.read<UserEntity>(UserEntity.collection, member.userId);
      if (userResult.success && userResult.data) {
        const user = userResult.data.data;
        const typeLabel = user.type === 'human' ? 'HUMAN' :
                         user.type === 'persona' ? 'AI ASSISTANT' :
                         user.type === 'agent' ? 'AI AGENT' : 'USER';
        members.push({ name: user.displayName, type: typeLabel });
      }
    }

    return members;
  } catch (error) {
    console.error(`‚ùå ChatRAGBuilder: Error loading room members:`, error);
    return [];
  }
}
```

**Why This Works**:
- Positive instructions ("respond naturally") vs negative ("DO NOT")
- Clear human/AI distinction in member list
- No mention of non-existent formats
- Emoji sections for clarity (small models handle these well)
- **Expected Result**: Fewer hallucinated prefixes, better formatting

---

## Medium Priority (Optimization)

### Remedy #7: Add Model Tier Detection

**Problem**: Can't provide model-appropriate RAG/prompts

**File**: `system/user/server/PersonaUser.ts` (new method)

**Solution**: Detect model tier from model name

**Implementation**:
```typescript
// PersonaUser.ts - ADD new method
private getModelTier(): 'small' | 'medium' | 'large' {
  const modelName = this.aiConfig?.modelName || '';

  // Small models (need heavy structure)
  if (modelName.includes('1b') || modelName.includes('3b')) {
    return 'small';
  }

  // Large models (minimal structure)
  if (modelName.includes('gpt-4') ||
      modelName.includes('claude-opus') ||
      modelName.includes('70b') ||
      modelName.includes('405b')) {
    return 'large';
  }

  // Medium (moderate structure)
  return 'medium';
}

// USE in respondToMessage (line 372) and evaluateShouldRespond (line 1033)
const modelTier = this.getModelTier();
const ragContext = await ragBuilder.buildContext(
  message.roomId,
  this.id,
  {
    maxMessages: modelTier === 'small' ? 10 : modelTier === 'medium' ? 20 : 50,
    // ... other options
  }
);
```

**Why This Works**:
- Simple string matching (no complex logic)
- Enables future model-dependent features
- **Expected Result**: Foundation for tiered RAG strategies

---

### Remedy #8: Database Cleanup Script (One-time)

**Problem**: Existing messages contain hallucinated prefixes

**File**: NEW - `scripts/cleanup-hallucinated-prefixes.ts`

**Solution**: One-time migration to strip prefixes

**Implementation**:
```typescript
/**
 * One-time cleanup script for hallucinated prefixes
 * Run: npx tsx scripts/cleanup-hallucinated-prefixes.ts
 */

import { DataDaemon } from '../daemons/data-daemon/shared/DataDaemon';
import { ChatMessageEntity } from '../system/data/entities/ChatMessageEntity';

async function cleanupHallucinatedPrefixes(): Promise<void> {
  console.log('üßπ Cleaning hallucinated prefixes from chat messages...');

  // Query all messages
  const result = await DataDaemon.query<ChatMessageEntity>({
    collection: ChatMessageEntity.collection,
    filters: {},
    sort: [{ field: 'timestamp', direction: 'asc' }],
    limit: 10000  // Process in batches if needed
  });

  if (!result.success || !result.data) {
    console.error('‚ùå Failed to query messages');
    return;
  }

  let cleaned = 0;
  const prefixPattern = /^(\w+\s+AI:|Human:|\[\d{2}:\d{2}\])\s*/;

  for (const messageRecord of result.data) {
    const message = messageRecord.data;
    const originalText = message.content?.text || '';
    const cleanedText = originalText.replace(prefixPattern, '');

    if (cleanedText !== originalText) {
      // Update message
      await DataDaemon.update<ChatMessageEntity>({
        collection: ChatMessageEntity.collection,
        id: message.id,
        data: {
          ...message,
          content: { ...message.content, text: cleanedText }
        }
      });

      cleaned++;
      console.log(`‚úÖ Cleaned: "${originalText.slice(0, 50)}..." ‚Üí "${cleanedText.slice(0, 50)}..."`);
    }
  }

  console.log(`üéâ Cleanup complete! Cleaned ${cleaned} messages`);
}

cleanupHallucinatedPrefixes().catch(console.error);
```

**Why This Works**:
- One-time fix for historical pollution
- Non-destructive (preserves message content)
- **Expected Result**: Clean database, no prefix pollution in RAG

---

## Implementation Order

### Phase 1: Critical Fixes (Must Do)
1. **Remedy #1** - Ollama queue (4-6 hours) - Fixes 67% of timeouts
2. **Remedy #2** - RAG temporal (3-4 hours) - Fixes hallucinations
3. **Remedy #3** - Response validation (4-6 hours) - Prevents cascade failures

### Phase 2: Quality Improvements (Should Do)
4. **Remedy #4** - Role assignment (5 minutes) - One-line fix
5. **Remedy #5** - Remove [QUESTION] (2 minutes) - Simple deletion
6. **Remedy #6** - System prompt (1-2 hours) - Better instructions

### Phase 3: Infrastructure (Nice to Have)
7. **Remedy #7** - Model tier detection (30 minutes) - Foundation for future
8. **Remedy #8** - Database cleanup (1 hour) - One-time migration

---

## Testing Strategy

### For Each Remedy

**Before Deploy**:
1. Add debug logging
2. Test locally with `npm start`
3. Post test question in chat
4. Verify fix in logs
5. Take screenshot

**Success Criteria**:
- Remedy #1: 0% timeout rate (was 67%)
- Remedy #2: RAG contains only messages before trigger
- Remedy #3: 0 gibberish messages stored
- Remedy #4: AI messages have `role=assistant`
- Remedy #5: No `[QUESTION]` markers in RAG
- Remedy #6: No hallucinated prefixes in new responses
- Remedy #7: Correct tier detected for each model
- Remedy #8: 0 messages with prefixes remain

---

## Estimated Total Time

- **Phase 1 (Critical)**: 11-16 hours
- **Phase 2 (Quality)**: 1.5-3 hours
- **Phase 3 (Infrastructure)**: 1.5 hours

**Total**: ~14-20 hours of focused development

---

## Rollback Plan

Each remedy is independent and can be reverted individually:

```bash
# If Remedy #1 causes issues:
git revert <commit-hash>
npm start

# If Remedy #2 breaks RAG:
git revert <commit-hash>
npm start

# etc.
```

All remedies are additive (don't remove existing functionality) so rollback is safe.
