//! Audio Mixer
//!
//! Multi-participant audio mixing with mix-minus support.
//! Each participant hears everyone except themselves.

use crate::handle::Handle;
use crate::vad::{ProductionVAD, VADError};
use std::collections::HashMap;
use tracing::{debug, info};

/// Audio test utilities for generating synthetic audio
pub mod test_utils {
    use std::f32::consts::PI;

    /// Generate a sine wave at the given frequency
    pub fn generate_sine_wave(frequency: f32, sample_rate: u32, num_samples: usize) -> Vec<i16> {
        (0..num_samples)
            .map(|i| {
                let t = i as f32 / sample_rate as f32;
                let sample = (2.0 * PI * frequency * t).sin();
                // Convert to i16 range (-32768 to 32767)
                (sample * 32767.0) as i16
            })
            .collect()
    }

    /// Generate silence
    pub fn generate_silence(num_samples: usize) -> Vec<i16> {
        vec![0i16; num_samples]
    }

    /// Generate white noise
    pub fn generate_noise(num_samples: usize) -> Vec<i16> {
        use rand::Rng;
        let mut rng = rand::thread_rng();
        (0..num_samples)
            .map(|_| rng.gen_range(-16384i16..16384i16))
            .collect()
    }

    /// Generate a click/impulse
    pub fn generate_click(num_samples: usize, click_position: usize) -> Vec<i16> {
        let mut samples = vec![0i16; num_samples];
        if click_position < num_samples {
            samples[click_position] = 32767;
        }
        samples
    }

    /// Calculate RMS (root mean square) of audio samples
    pub fn calculate_rms(samples: &[i16]) -> f32 {
        if samples.is_empty() {
            return 0.0;
        }
        let sum_squares: f64 = samples.iter().map(|&s| (s as f64).powi(2)).sum();
        (sum_squares / samples.len() as f64).sqrt() as f32
    }

    /// Check if audio is mostly silence (RMS below threshold)
    pub fn is_silence(samples: &[i16], threshold: f32) -> bool {
        calculate_rms(samples) < threshold
    }

    /// Detect dominant frequency using zero-crossing rate (simple method)
    pub fn detect_frequency_approx(samples: &[i16], sample_rate: u32) -> f32 {
        if samples.len() < 2 {
            return 0.0;
        }

        let mut zero_crossings = 0;
        for i in 1..samples.len() {
            if (samples[i - 1] >= 0 && samples[i] < 0) || (samples[i - 1] < 0 && samples[i] >= 0) {
                zero_crossings += 1;
            }
        }

        // Frequency = zero_crossings / (2 * duration)
        let duration = samples.len() as f32 / sample_rate as f32;
        zero_crossings as f32 / (2.0 * duration)
    }
}

/// Standard frame size (20ms at 16kHz = 320 samples)
pub const FRAME_SIZE: usize = 320;

/// Participant audio stream - zero allocations on hot path
pub struct ParticipantStream {
    pub handle: Handle,
    pub user_id: String,
    pub display_name: String,
    /// Current audio frame - FIXED SIZE array, no allocation
    audio_frame: [i16; FRAME_SIZE],
    /// Valid samples in audio_frame (may be < FRAME_SIZE for partial frames)
    frame_len: usize,
    /// Is this participant currently muted?
    pub muted: bool,
    /// Is this an AI participant (no transcription needed - we have their text)?
    pub is_ai: bool,

    // === Voice Activity Detection (Production Two-Stage VAD) ===
    /// Production VAD (WebRTC â†’ Silero, with sentence buffering)
    vad: Option<ProductionVAD>,

    /// Is currently speaking? (for UI indicators)
    is_speaking: bool,
}

/// Result of pushing audio - indicates if speech ended and transcription is ready
#[derive(Debug)]
pub struct PushAudioResult {
    /// Speech ended and buffer is ready for transcription
    pub speech_ended: bool,
    /// Accumulated speech samples (only populated if speech_ended is true)
    pub speech_samples: Option<Vec<i16>>,
}

impl ParticipantStream {
    /// Create new human participant with production VAD
    pub fn new(handle: Handle, user_id: String, display_name: String) -> Self {
        // Create ProductionVAD with default config (initialized later)
        let vad = Some(ProductionVAD::new());

        Self {
            handle,
            user_id,
            display_name,
            audio_frame: [0i16; FRAME_SIZE],
            frame_len: 0,
            muted: false,
            is_ai: false,
            vad,
            is_speaking: false,
        }
    }

    /// Create AI participant (no VAD needed - we already have their text from TTS)
    pub fn new_ai(handle: Handle, user_id: String, display_name: String) -> Self {
        Self {
            handle,
            user_id,
            display_name,
            audio_frame: [0i16; FRAME_SIZE],
            frame_len: 0,
            muted: false,
            is_ai: true,
            vad: None, // AI doesn't need VAD
            is_speaking: false,
        }
    }

    /// Initialize VAD (must be called after construction, requires async)
    /// Returns Ok even if model loading fails (graceful degradation for tests)
    pub async fn initialize_vad(&mut self) -> Result<(), VADError> {
        if let Some(ref mut vad) = self.vad {
            match vad.initialize().await {
                Ok(_) => {
                    info!("ðŸŽ¯ ProductionVAD initialized for {}", self.display_name);
                }
                Err(e) => {
                    debug!("VAD init failed for {} (test mode): {:?}", self.display_name, e);
                    // In tests, VAD may not be available - gracefully disable
                    self.vad = None;
                }
            }
        }
        Ok(())
    }

    /// Update audio frame with new samples
    /// Returns PushAudioResult indicating if transcription should run
    ///
    /// Uses ProductionVAD for:
    /// - Two-stage detection (WebRTC â†’ Silero)
    /// - Complete sentence buffering
    /// - Adaptive silence thresholds
    pub fn push_audio(&mut self, samples: Vec<i16>) -> PushAudioResult {
        // [STEP 1] Audio frame received
        // Copy into fixed-size frame (no allocation, just memcpy)
        let copy_len = samples.len().min(FRAME_SIZE);
        self.audio_frame[..copy_len].copy_from_slice(&samples[..copy_len]);
        self.frame_len = copy_len;

        // Skip VAD for AI participants or muted participants
        if self.is_ai || self.muted {
            return PushAudioResult {
                speech_ended: false,
                speech_samples: None,
            };
        }

        // Use ProductionVAD (two-stage VAD + sentence buffering)
        if let Some(ref mut vad) = self.vad {
            // ProductionVAD.process_frame() returns complete sentence when ready
            let vad_result = futures::executor::block_on(vad.process_frame(&samples));

            match vad_result {
                Ok(Some(complete_sentence)) => {
                    // Complete sentence ready for transcription
                    let duration_ms = (complete_sentence.len() as f32 / 16000.0) * 1000.0;
                    info!(
                        "ðŸ“¤ Complete sentence ready for {} ({} samples, {:.0}ms)",
                        self.display_name,
                        complete_sentence.len(),
                        duration_ms
                    );

                    self.is_speaking = false;

                    PushAudioResult {
                        speech_ended: true,
                        speech_samples: Some(complete_sentence),
                    }
                }
                Ok(None) => {
                    // Still buffering - check if we should update speaking state
                    // (This is approximate - ProductionVAD handles the real logic)
                    PushAudioResult {
                        speech_ended: false,
                        speech_samples: None,
                    }
                }
                Err(e) => {
                    debug!("VAD error for {}: {:?}", self.display_name, e);
                    PushAudioResult {
                        speech_ended: false,
                        speech_samples: None,
                    }
                }
            }
        } else {
            // No VAD (shouldn't happen for human participants, but handle gracefully)
            PushAudioResult {
                speech_ended: false,
                speech_samples: None,
            }
        }
    }

    /// Get audio samples (returns silence if muted)
    pub fn get_audio(&self) -> &[i16] {
        if self.muted || self.frame_len == 0 {
            &[]
        } else {
            &self.audio_frame[..self.frame_len]
        }
    }

    /// Check if currently speaking (for UI indicators)
    pub fn is_currently_speaking(&self) -> bool {
        self.is_speaking
    }
}

/// Result of pushing audio to mixer - includes participant info if transcription ready
#[derive(Debug)]
pub struct MixerPushResult {
    pub success: bool,
    pub speech_ended: bool,
    pub user_id: Option<String>,
    pub display_name: Option<String>,
    pub speech_samples: Option<Vec<i16>>,
}

/// Audio Mixer for multi-participant calls
pub struct AudioMixer {
    /// All participants in the call
    participants: HashMap<Handle, ParticipantStream>,
    /// Sample rate for mixing
    sample_rate: u32,
    /// Frame size in samples (e.g., 320 for 20ms at 16kHz)
    frame_size: usize,
}

impl AudioMixer {
    /// Create a new mixer
    pub fn new(sample_rate: u32, frame_size: usize) -> Self {
        Self {
            participants: HashMap::new(),
            sample_rate,
            frame_size,
        }
    }

    /// Create mixer with default settings (16kHz, 20ms frames)
    pub fn default_voice() -> Self {
        Self::new(16000, 320) // 16kHz, 20ms = 320 samples
    }

    /// Add a participant
    /// Note: Call initialize_vad() on the participant BEFORE adding to mixer
    pub fn add_participant(&mut self, stream: ParticipantStream) {
        self.participants.insert(stream.handle, stream);
    }

    /// Add a participant and initialize VAD
    pub async fn add_participant_with_init(&mut self, mut stream: ParticipantStream) -> Result<(), VADError> {
        stream.initialize_vad().await?;
        self.participants.insert(stream.handle, stream);
        Ok(())
    }

    /// Remove a participant
    pub fn remove_participant(&mut self, handle: &Handle) -> Option<ParticipantStream> {
        self.participants.remove(handle)
    }

    /// Get participant by handle
    pub fn get_participant(&self, handle: &Handle) -> Option<&ParticipantStream> {
        self.participants.get(handle)
    }

    /// Get mutable participant by handle
    pub fn get_participant_mut(&mut self, handle: &Handle) -> Option<&mut ParticipantStream> {
        self.participants.get_mut(handle)
    }

    /// Update audio for a participant
    /// Returns MixerPushResult with transcription data if speech ended
    pub fn push_audio(&mut self, handle: &Handle, samples: Vec<i16>) -> MixerPushResult {
        if let Some(participant) = self.participants.get_mut(handle) {
            let result = participant.push_audio(samples);

            if result.speech_ended {
                MixerPushResult {
                    success: true,
                    speech_ended: true,
                    user_id: Some(participant.user_id.clone()),
                    display_name: Some(participant.display_name.clone()),
                    speech_samples: result.speech_samples,
                }
            } else {
                MixerPushResult {
                    success: true,
                    speech_ended: false,
                    user_id: None,
                    display_name: None,
                    speech_samples: None,
                }
            }
        } else {
            MixerPushResult {
                success: false,
                speech_ended: false,
                user_id: None,
                display_name: None,
                speech_samples: None,
            }
        }
    }

    /// Get number of participants
    pub fn participant_count(&self) -> usize {
        self.participants.len()
    }

    /// Mix all participants (sum all streams)
    pub fn mix_all(&self) -> Vec<i16> {
        let mut mixed = vec![0i32; self.frame_size];

        for participant in self.participants.values() {
            let audio = participant.get_audio();
            for (i, &sample) in audio.iter().enumerate() {
                if i < self.frame_size {
                    mixed[i] += sample as i32;
                }
            }
        }

        // Convert back to i16 with clamping
        Self::clamp_to_i16(&mixed)
    }

    /// Mix-minus: mix all participants EXCEPT the one with the given handle
    ///
    /// This is the standard approach for conference calls - each participant
    /// hears everyone except themselves to prevent feedback.
    pub fn mix_minus(&self, exclude_handle: &Handle) -> Vec<i16> {
        let mut mixed = vec![0i32; self.frame_size];

        for (handle, participant) in &self.participants {
            if handle == exclude_handle {
                continue; // Skip the excluded participant
            }

            let audio = participant.get_audio();
            for (i, &sample) in audio.iter().enumerate() {
                if i < self.frame_size {
                    mixed[i] += sample as i32;
                }
            }
        }

        // Convert back to i16 with clamping
        Self::clamp_to_i16(&mixed)
    }

    /// Generate mix-minus for all participants
    /// Returns a map of handle -> mixed audio (what that participant should hear)
    pub fn mix_minus_all(&self) -> HashMap<Handle, Vec<i16>> {
        self.participants
            .keys()
            .map(|handle| (*handle, self.mix_minus(handle)))
            .collect()
    }

    /// Clamp i32 samples to i16 range
    fn clamp_to_i16(samples: &[i32]) -> Vec<i16> {
        samples
            .iter()
            .map(|&s| s.clamp(-32768, 32767) as i16)
            .collect()
    }

    /// Get sample rate
    pub fn sample_rate(&self) -> u32 {
        self.sample_rate
    }

    /// Get frame size
    pub fn frame_size(&self) -> usize {
        self.frame_size
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use test_utils::*;

    #[test]
    fn test_generate_sine_wave() {
        let samples = generate_sine_wave(440.0, 16000, 320);
        assert_eq!(samples.len(), 320);

        // Should not be silence
        assert!(!is_silence(&samples, 100.0));

        // Frequency should be approximately 440Hz
        let detected = detect_frequency_approx(&samples, 16000);
        assert!((detected - 440.0).abs() < 50.0, "Detected: {detected}");
    }

    #[test]
    fn test_generate_silence() {
        let samples = generate_silence(320);
        assert_eq!(samples.len(), 320);
        assert!(is_silence(&samples, 1.0));
    }

    #[tokio::test]
    async fn test_mixer_add_remove() {
        let mut mixer = AudioMixer::default_voice();

        let handle_a = Handle::new();
        let mut stream_a = ParticipantStream::new(handle_a, "user-a".into(), "Alice".into());
        stream_a.initialize_vad().await.expect("VAD init failed");

        mixer.add_participant(stream_a);
        assert_eq!(mixer.participant_count(), 1);

        mixer.remove_participant(&handle_a);
        assert_eq!(mixer.participant_count(), 0);
    }

    #[tokio::test]
    async fn test_mix_all() {
        let mut mixer = AudioMixer::default_voice();

        // Add two participants with different tones
        let handle_a = Handle::new();
        let handle_b = Handle::new();

        let mut stream_a = ParticipantStream::new(handle_a, "user-a".into(), "Alice".into());
        let mut stream_b = ParticipantStream::new(handle_b, "user-b".into(), "Bob".into());

        stream_a.initialize_vad().await.expect("VAD init failed");
        stream_b.initialize_vad().await.expect("VAD init failed");

        // Alice plays 440Hz, Bob plays 880Hz
        stream_a.push_audio(generate_sine_wave(440.0, 16000, 320));
        stream_b.push_audio(generate_sine_wave(880.0, 16000, 320));

        mixer.add_participant(stream_a);
        mixer.add_participant(stream_b);

        // Mix should contain both frequencies (not silence)
        let mixed = mixer.mix_all();
        assert_eq!(mixed.len(), 320);
        assert!(!is_silence(&mixed, 100.0));
    }

    #[tokio::test]
    async fn test_mix_minus() {
        let mut mixer = AudioMixer::default_voice();

        let handle_a = Handle::new();
        let handle_b = Handle::new();
        let handle_c = Handle::new();

        let mut stream_a = ParticipantStream::new(handle_a, "user-a".into(), "Alice".into());
        let mut stream_b = ParticipantStream::new(handle_b, "user-b".into(), "Bob".into());
        let mut stream_c = ParticipantStream::new(handle_c, "user-c".into(), "Charlie".into());

        stream_a.initialize_vad().await.expect("VAD init failed");
        stream_b.initialize_vad().await.expect("VAD init failed");
        stream_c.initialize_vad().await.expect("VAD init failed");

        // Each plays a different frequency
        stream_a.push_audio(generate_sine_wave(440.0, 16000, 320));
        stream_b.push_audio(generate_sine_wave(880.0, 16000, 320));
        stream_c.push_audio(generate_sine_wave(1320.0, 16000, 320));

        mixer.add_participant(stream_a);
        mixer.add_participant(stream_b);
        mixer.add_participant(stream_c);

        // Alice should hear Bob + Charlie (not herself)
        let mix_for_a = mixer.mix_minus(&handle_a);
        assert!(!is_silence(&mix_for_a, 100.0));

        // Bob should hear Alice + Charlie (not himself)
        let mix_for_b = mixer.mix_minus(&handle_b);
        assert!(!is_silence(&mix_for_b, 100.0));

        // Mix-minus for each should be different from mix_all
        let mix_all = mixer.mix_all();
        assert_ne!(mix_for_a, mix_all);
        assert_ne!(mix_for_b, mix_all);
    }

    #[tokio::test]
    async fn test_mix_minus_two_participants() {
        let mut mixer = AudioMixer::default_voice();

        let handle_a = Handle::new();
        let handle_b = Handle::new();

        let mut stream_a = ParticipantStream::new(handle_a, "user-a".into(), "Alice".into());
        let mut stream_b = ParticipantStream::new(handle_b, "user-b".into(), "Bob".into());

        stream_a.initialize_vad().await.expect("VAD init failed");
        stream_b.initialize_vad().await.expect("VAD init failed");

        let audio_a = generate_sine_wave(440.0, 16000, 320);
        let audio_b = generate_sine_wave(880.0, 16000, 320);

        stream_a.push_audio(audio_a.clone());
        stream_b.push_audio(audio_b.clone());

        mixer.add_participant(stream_a);
        mixer.add_participant(stream_b);

        // Alice hears only Bob
        let mix_for_a = mixer.mix_minus(&handle_a);
        assert_eq!(mix_for_a, audio_b, "Alice should hear exactly Bob's audio");

        // Bob hears only Alice
        let mix_for_b = mixer.mix_minus(&handle_b);
        assert_eq!(mix_for_b, audio_a, "Bob should hear exactly Alice's audio");
    }

    #[tokio::test]
    async fn test_muted_participant() {
        let mut mixer = AudioMixer::default_voice();

        let handle_a = Handle::new();
        let handle_b = Handle::new();

        let mut stream_a = ParticipantStream::new(handle_a, "user-a".into(), "Alice".into());
        let mut stream_b = ParticipantStream::new(handle_b, "user-b".into(), "Bob".into());

        stream_a.initialize_vad().await.expect("VAD init failed");
        stream_b.initialize_vad().await.expect("VAD init failed");

        stream_a.push_audio(generate_sine_wave(440.0, 16000, 320));
        stream_b.push_audio(generate_sine_wave(880.0, 16000, 320));
        stream_a.muted = true; // Alice is muted

        mixer.add_participant(stream_a);
        mixer.add_participant(stream_b);

        // Mix-minus for Bob should be silence (Alice is muted)
        let mix_for_b = mixer.mix_minus(&handle_b);
        assert!(
            is_silence(&mix_for_b, 10.0),
            "Bob should hear silence (Alice muted)"
        );

        // Mix-minus for Alice should be Bob's audio
        let mix_for_a = mixer.mix_minus(&handle_a);
        assert!(!is_silence(&mix_for_a, 100.0), "Alice should hear Bob");
    }

    #[tokio::test]
    async fn test_ai_participant() {
        let mut mixer = AudioMixer::default_voice();

        let handle_human = Handle::new();
        let handle_ai = Handle::new();

        let mut stream_human =
            ParticipantStream::new(handle_human, "user-human".into(), "Joel".into());
        let stream_ai =
            ParticipantStream::new_ai(handle_ai, "ai-helper".into(), "Helper AI".into());

        stream_human.initialize_vad().await.expect("VAD init failed");
        // AI doesn't need VAD initialization

        assert!(!stream_human.is_ai);
        assert!(stream_ai.is_ai);

        // Human speaks
        stream_human.push_audio(generate_sine_wave(440.0, 16000, 320));

        // AI injects TTS audio
        let mut stream_ai_mut = stream_ai;
        stream_ai_mut.push_audio(generate_sine_wave(220.0, 16000, 320));

        mixer.add_participant(stream_human);
        mixer.add_participant(stream_ai_mut);

        // Both should be in the mix
        let mix_all = mixer.mix_all();
        assert!(!is_silence(&mix_all, 100.0));

        // Human hears AI
        let mix_for_human = mixer.mix_minus(&handle_human);
        assert!(!is_silence(&mix_for_human, 100.0));

        // AI "hears" human (for STT purposes)
        let mix_for_ai = mixer.mix_minus(&handle_ai);
        assert!(!is_silence(&mix_for_ai, 100.0));
    }

    #[tokio::test]
    async fn test_mix_minus_all() {
        let mut mixer = AudioMixer::default_voice();

        let handle_a = Handle::new();
        let handle_b = Handle::new();

        let mut stream_a = ParticipantStream::new(handle_a, "user-a".into(), "Alice".into());
        let mut stream_b = ParticipantStream::new(handle_b, "user-b".into(), "Bob".into());

        stream_a.initialize_vad().await.expect("VAD init failed");
        stream_b.initialize_vad().await.expect("VAD init failed");

        stream_a.push_audio(generate_sine_wave(440.0, 16000, 320));
        stream_b.push_audio(generate_sine_wave(880.0, 16000, 320));

        mixer.add_participant(stream_a);
        mixer.add_participant(stream_b);

        let all_mixes = mixer.mix_minus_all();
        assert_eq!(all_mixes.len(), 2);
        assert!(all_mixes.contains_key(&handle_a));
        assert!(all_mixes.contains_key(&handle_b));
    }

    #[tokio::test]
    async fn test_clipping_prevention() {
        let mut mixer = AudioMixer::default_voice();

        // Add many loud participants
        for i in 0..10 {
            let handle = Handle::new();
            let mut stream =
                ParticipantStream::new(handle, format!("user-{i}"), format!("User {i}"));
            stream.initialize_vad().await.expect("VAD init failed");
            // Max amplitude sine wave
            stream.push_audio(generate_sine_wave(440.0 + (i as f32 * 100.0), 16000, 320));
            mixer.add_participant(stream);
        }

        // Mix should not overflow - if we get here without panic, clamping worked
        let mixed = mixer.mix_all();
        assert_eq!(mixed.len(), 320);
        // Values are already i16 so they're in valid range by type constraints
        // The real test is that clamp_to_i16 prevents overflow during mixing
    }
}
