/**
 * ProposalRatingAdapter - AI-driven proposal evaluation
 *
 * Uses the PersonaUser's actual AI model to rate proposals organically.
 * NO HEURISTICS - only LLM-generated judgments fed into aggregation algorithm.
 *
 * Key principle: Inputs must be organically generated by AI inference.
 * The algorithm only handles weighted aggregation of those organic ratings.
 */

import type { UUID } from '../../../../core/types/CrossPlatformUUID';
import { AIProviderDaemon } from '../../../../../daemons/ai-provider-daemon/shared/AIProviderDaemon';
import type { TextGenerationRequest, TextGenerationResponse } from '../../../../../daemons/ai-provider-daemon/shared/AIProviderTypesV2';
import type { ResponseProposal, ProposalRating } from './PeerReviewTypes';
import { generateUUID } from '../../../../core/uuid/UUIDGenerator';

/**
 * Rating context - what the AI sees when rating proposals
 */
export interface RatingContext {
  /** Original message being responded to */
  originalMessage: {
    senderId: UUID;
    senderName: string;
    content: string;
    timestamp: number;
  };

  /** Recent conversation history (for context) */
  recentMessages: Array<{
    senderName: string;
    content: string;
    timestamp: number;
  }>;

  /** All proposals competing for this message */
  proposals: ResponseProposal[];
}

/**
 * Ask AI to rate all proposals organically
 *
 * This calls the PersonaUser's configured LLM to evaluate proposals.
 * The AI judges quality, relevance, redundancy, added value, etc.
 */
export async function rateProposalsWithAI(params: {
  reviewerId: UUID;
  reviewerName: string;
  reviewerWeight: number;
  modelProvider: string;
  modelId: string;
  temperature: number;
  context: RatingContext;
}): Promise<ProposalRating[]> {
  const { reviewerId, reviewerName, reviewerWeight, modelProvider, modelId, temperature, context } = params;

  // Build prompt for AI to rate proposals
  const prompt = buildRatingPrompt(context, reviewerName);

  // Call AI to get ratings
  const request: TextGenerationRequest = {
    messages: [
      { role: 'system', content: `You are ${reviewerName}, an AI evaluating response proposals from your peers.` },
      { role: 'user', content: prompt }
    ],
    model: modelId,
    temperature: temperature ?? 0.7,
    maxTokens: 500,
    preferredProvider: modelProvider as TextGenerationRequest['preferredProvider']
  };

  const response: TextGenerationResponse = await AIProviderDaemon.generateText(request);

  // Parse AI's ratings from response
  const ratings = parseRatingsFromAIResponse(response.text, context.proposals, reviewerId, reviewerName, reviewerWeight);

  console.log(`⭐ [PeerReview] ${reviewerName} rated ${ratings.length} proposals using ${modelProvider}:${modelId}`);
  for (const rating of ratings) {
    const proposal = context.proposals.find(p => p.proposalId === rating.proposalId);
    console.log(`   Proposal by ${proposal?.proposerName}: score=${rating.score.toFixed(2)}, shouldPost=${rating.shouldPost}`);
  }

  return ratings;
}

/**
 * Build prompt asking AI to rate all proposals
 *
 * Prompt includes:
 * - Original message context
 * - All competing proposals
 * - Rating criteria
 * - Output format instructions
 */
function buildRatingPrompt(context: RatingContext, reviewerName: string): string {
  const { originalMessage, recentMessages, proposals } = context;

  // Format recent conversation
  const conversationHistory = recentMessages
    .map(msg => `[${msg.senderName}]: ${msg.content}`)
    .join('\n');

  // Format proposals
  const proposalsText = proposals
    .map((p, idx) => `
PROPOSAL ${idx + 1} (by ${p.proposerName}, confidence: ${p.confidence.toFixed(2)}):
"${p.responseText}"
`)
    .join('\n');

  return `You are ${reviewerName}. Multiple AIs (including yourself) have proposed responses to this message. Rate each proposal.

ORIGINAL MESSAGE (from ${originalMessage.senderName}):
"${originalMessage.content}"

RECENT CONVERSATION:
${conversationHistory}

ALL PROPOSALS:
${proposalsText}

RATING CRITERIA:
1. Relevance (0.0-1.0): How relevant is this response to the original question?
2. Quality (0.0-1.0): Is this a high-quality, well-formed response?
3. Redundancy (0.0-1.0): How redundant is this with other proposals? (0=unique, 1=duplicate)
4. Added Value (0.0-1.0): Does this add new information or perspective?
5. Correctness (0.0-1.0): Is this factually correct?

For each proposal, provide:
- Overall score (0.0-1.0)
- Should this post? (yes/no)
- Brief reasoning

FORMAT YOUR RESPONSE EXACTLY LIKE THIS:

PROPOSAL 1:
Score: 0.85
ShouldPost: yes
Reasoning: High quality response with good technical detail, adds unique perspective

PROPOSAL 2:
Score: 0.60
ShouldPost: no
Reasoning: Redundant with Proposal 1, doesn't add new information

PROPOSAL 3:
Score: 0.75
ShouldPost: yes
Reasoning: Different approach than Proposal 1, valuable alternative perspective

Rate honestly - it's OK if multiple proposals should post (quality control, not competition).
It's also OK if NONE should post (all redundant/low quality).
You may rate your own proposal - be objective.`;
}

/**
 * Parse AI's rating response into structured data
 *
 * Expected format:
 * PROPOSAL 1:
 * Score: 0.85
 * ShouldPost: yes
 * Reasoning: ...
 */
function parseRatingsFromAIResponse(
  responseText: string,
  proposals: ResponseProposal[],
  reviewerId: UUID,
  reviewerName: string,
  reviewerWeight: number
): ProposalRating[] {
  const ratings: ProposalRating[] = [];

  // Split response into proposal sections
  const sections = responseText.split(/PROPOSAL \d+:/i).slice(1); // Skip first empty split

  for (let i = 0; i < Math.min(sections.length, proposals.length); i++) {
    const section = sections[i];
    const proposal = proposals[i];

    // Extract score
    const scoreMatch = section.match(/Score:\s*([0-9.]+)/i);
    const score = scoreMatch ? parseFloat(scoreMatch[1]) : 0.5; // Default to neutral if parse fails

    // Extract shouldPost
    const shouldPostMatch = section.match(/ShouldPost:\s*(yes|no)/i);
    const shouldPost = shouldPostMatch ? shouldPostMatch[1].toLowerCase() === 'yes' : false;

    // Extract reasoning
    const reasoningMatch = section.match(/Reasoning:\s*(.+?)(?=\n\n|$)/is);
    const reasoning = reasoningMatch ? reasoningMatch[1].trim() : 'No reasoning provided';

    ratings.push({
      ratingId: generateUUID(),
      proposalId: proposal.proposalId,
      reviewerId,
      reviewerName,
      reviewerWeight,
      score: Math.max(0, Math.min(1, score)), // Clamp to [0, 1]
      shouldPost,
      ratedAt: Date.now(),
      reasoning
    });
  }

  // If parsing failed or didn't get all ratings, fill in defaults for missing
  if (ratings.length < proposals.length) {
    console.warn(`⚠️  [PeerReview] ${reviewerName} only provided ${ratings.length}/${proposals.length} ratings, filling defaults`);
    for (let i = ratings.length; i < proposals.length; i++) {
      ratings.push({
        ratingId: generateUUID(),
        proposalId: proposals[i].proposalId,
        reviewerId,
        reviewerName,
        reviewerWeight,
        score: 0.5, // Neutral default
        shouldPost: false,
        ratedAt: Date.now(),
        reasoning: 'Parse error - default rating applied'
      });
    }
  }

  return ratings;
}

/**
 * Simple fallback rating (if AI call fails)
 *
 * This is ONLY used when the AI provider is down or times out.
 * Still no heuristics - just assigns neutral scores.
 */
export function createFallbackRatings(
  proposals: ResponseProposal[],
  reviewerId: UUID,
  reviewerName: string,
  reviewerWeight: number
): ProposalRating[] {
  console.warn(`⚠️  [PeerReview] ${reviewerName} AI rating failed, using fallback (neutral scores)`);

  return proposals.map(proposal => ({
    ratingId: generateUUID(),
    proposalId: proposal.proposalId,
    reviewerId,
    reviewerName,
    reviewerWeight,
    score: 0.5, // Neutral
    shouldPost: false, // Conservative default
    ratedAt: Date.now(),
    reasoning: 'AI rating unavailable - fallback applied'
  }));
}
