/**
 * Standalone test for OllamaLoRAAdapter - Integration Test (Phase 7.1)
 *
 * Purpose: Test Ollama/llama.cpp adapter in isolation without full system integration
 * Philosophy: "main() type interface to make it easier. Helps you isolate problems"
 *
 * Usage:
 *   npx tsx system/genome/fine-tuning/server/adapters/test-ollama.ts
 *
 * What This Tests:
 * - OllamaLoRAAdapter initialization
 * - Capability reporting (rank, epochs, local training)
 * - Cost/time estimation (free - local training)
 * - llama.cpp finetune command integration
 * - Dataset export to plain text format
 * - Error handling
 *
 * What This Tests (Phase 7.1):
 * - Actual llama.cpp finetune execution
 * - Real training with local model
 * - Adapter file creation and storage
 * - Progress monitoring
 */

import { OllamaLoRAAdapter } from './OllamaLoRAAdapter';
import { TrainingDatasetBuilder } from '../TrainingDatasetBuilder';
import type { LoRATrainingRequest, TrainingDataset, TrainingExample } from '../../shared/FineTuningTypes';
import type { UUID } from '../../../../core/types/CrossPlatformUUID';

/**
 * Main test function
 */
async function main(): Promise<void> {
  console.log('ðŸ§¬ OllamaLoRAAdapter Standalone Test (Phase 7.1)');
  console.log('===============================================\n');

  try {
    // Step 1: Initialize adapter
    console.log('ðŸ“¦ Step 1: Initializing OllamaLoRAAdapter...');
    const adapter = new OllamaLoRAAdapter();
    console.log(`âœ… Adapter initialized: providerId="${adapter.providerId}"\n`);

    // Step 2: Check if fine-tuning supported
    console.log('ðŸ” Step 2: Checking fine-tuning support...');
    const supported = adapter.supportsFineTuning();
    console.log(`   supportsFineTuning(): ${supported}`);
    if (supported) {
      console.log('   âœ… Local training is supported (llama.cpp/Ollama available)\n');
    } else {
      console.log('   âš ï¸  llama.cpp finetune not found in PATH');
      console.log('   Install with: brew install llama.cpp OR compile from source\n');
    }

    // Step 3: Get capabilities
    console.log('âš™ï¸  Step 3: Getting fine-tuning capabilities...');
    const capabilities = adapter.getFineTuningCapabilities();
    console.log(`   LoRA Rank: ${capabilities.minRank}-${capabilities.maxRank} (default: ${capabilities.defaultRank})`);
    console.log(`   LoRA Alpha: ${capabilities.minAlpha}-${capabilities.maxAlpha} (default: ${capabilities.defaultAlpha})`);
    console.log(`   Epochs: ${capabilities.minEpochs}-${capabilities.maxEpochs} (default: ${capabilities.defaultEpochs})`);
    console.log(`   Learning Rate: ${capabilities.minLearningRate}-${capabilities.maxLearningRate} (default: ${capabilities.defaultLearningRate})`);
    console.log(`   Batch Size: ${capabilities.minBatchSize}-${capabilities.maxBatchSize} (default: ${capabilities.defaultBatchSize})`);
    console.log(`   Cost per example: $${capabilities.costPerExample} (FREE - local training!)`);
    console.log(`   Estimated training time: ${capabilities.estimatedTrainingTime}ms per example per epoch (GPU)`);
    console.log(`   Requires GPU: ${capabilities.requiresGPU} (recommended for performance)`);
    console.log(`   Requires Internet: ${capabilities.requiresInternet} (fully offline)`);
    console.log(`   Supported models: ALL models that Ollama can load\n`);

    // Step 4: Get strategy
    console.log('ðŸŽ¯ Step 4: Getting training strategy...');
    const strategy = adapter.getFineTuningStrategy();
    console.log(`   Strategy: ${strategy}`);
    console.log(`   (Ollama uses local llama.cpp for private training)\n`);

    // Step 5: Create minimal test dataset
    console.log('ðŸ“‹ Step 5: Creating minimal test dataset (5 examples)...');
    const testDataset = createMinimalDataset();
    console.log(`   Created ${testDataset.examples.length} training examples`);
    console.log(`   Persona: ${testDataset.metadata.personaName}`);
    console.log(`   Trait Type: ${testDataset.metadata.traitType}\n`);

    // Step 6: Estimate training cost (FREE!)
    console.log('ðŸ’° Step 6: Estimating training cost...');
    const exampleCount = testDataset.examples.length;
    const cost = adapter.estimateTrainingCost(exampleCount);
    console.log(`   ${exampleCount} examples Ã— $${capabilities.costPerExample} = $${cost.toFixed(6)}`);
    console.log(`   Local training is FREE (no API costs)`);
    console.log(`   Only electricity costs (negligible)\n`);

    // Step 7: Estimate training time
    console.log('â±ï¸  Step 7: Estimating training time...');
    const epochs = capabilities.defaultEpochs;
    const estimatedTimeMs = adapter.estimateTrainingTime(exampleCount, epochs);
    console.log(`   ${exampleCount} examples Ã— ${epochs} epochs Ã— ${capabilities.estimatedTrainingTime}ms = ${estimatedTimeMs}ms`);
    console.log(`   (~${(estimatedTimeMs / 1000).toFixed(2)} seconds with GPU)`);
    console.log(`   (CPU training: ~10x slower)\n`);

    // Step 8: Create training request
    console.log('ðŸ“ Step 8: Creating training request...');
    const request: LoRATrainingRequest = {
      personaId: testDataset.metadata.personaId,
      personaName: testDataset.metadata.personaName,
      traitType: testDataset.metadata.traitType,
      baseModel: 'llama3.2:3b', // Use Llama 3.2 3B model
      dataset: testDataset,
      rank: capabilities.defaultRank,
      alpha: capabilities.defaultAlpha,
      epochs: capabilities.defaultEpochs,
      learningRate: capabilities.defaultLearningRate,
      batchSize: capabilities.defaultBatchSize
    };
    console.log(`   Base Model: ${request.baseModel}`);
    console.log(`   LoRA Rank: ${request.rank}`);
    console.log(`   LoRA Alpha: ${request.alpha}`);
    console.log(`   Epochs: ${request.epochs}`);
    console.log(`   Learning Rate: ${request.learningRate}`);
    console.log(`   Batch Size: ${request.batchSize}\n`);

    // Step 9: Test dataset export
    console.log('ðŸ“„ Step 9: Testing plain text export for llama.cpp...');
    const jsonl = TrainingDatasetBuilder.exportToJSONL(testDataset);
    const lines = jsonl.trim().split('\n');
    console.log(`   Exported ${lines.length} JSONL lines`);
    console.log(`   (Will be converted to plain text with special tokens)\n`);

    // Step 10: Cost comparison
    console.log('ðŸ’µ Step 10: Cost comparison (Ollama vs cloud providers)...');
    const ollamaCost = cost;
    const openaiCost = 0.015; // Estimate for 5 examples
    const anthropicCost = 0.020; // Estimate for 5 examples
    const deepseekCost = 0.000555; // DeepSeek pricing

    console.log(`   Ollama Local:  $${ollamaCost.toFixed(6)} (FREE)`);
    console.log(`   DeepSeek API:  $${deepseekCost.toFixed(6)} (cheapest cloud)`);
    console.log(`   OpenAI API:    $${openaiCost.toFixed(6)} (27x more than DeepSeek)`);
    console.log(`   Anthropic API: $${anthropicCost.toFixed(6)} (36x more than DeepSeek)`);
    console.log();
    console.log(`   For 1000 examples:`);
    console.log(`   Ollama:    $0.00 (always free)`);
    console.log(`   DeepSeek:  $${(deepseekCost * 200).toFixed(4)}`);
    console.log(`   OpenAI:    $${(openaiCost * 200).toFixed(4)}`);
    console.log(`   Anthropic: $${(anthropicCost * 200).toFixed(4)}\n`);

    // Step 11: REAL TRAINING TEST (Phase 7.1)
    console.log('ðŸš€ Step 11: REAL TRAINING TEST (Phase 7.1)...');
    console.log('   This will actually train a LoRA adapter with llama.cpp!');
    console.log('   Make sure you have:');
    console.log('   - Ollama installed (brew install ollama)');
    console.log('   - llama.cpp finetune binary available');
    console.log(`   - Model downloaded (ollama pull ${request.baseModel})`);
    console.log();

    const shouldTrain = process.env.RUN_REAL_TRAINING === 'true';
    if (shouldTrain) {
      console.log('   Starting real training...\n');
      const result = await adapter.trainLoRA(request);

      if (result.success) {
        console.log('   âœ… Training completed successfully!');
        console.log(`   Adapter saved to: ${result.modelPath}`);
        console.log(`   Training time: ${result.metrics?.trainingTime}ms`);
        console.log(`   Final loss: ${result.metrics?.finalLoss}`);
        console.log(`   Examples processed: ${result.metrics?.examplesProcessed}`);
        console.log(`   Epochs: ${result.metrics?.epochs}\n`);
      } else {
        console.log('   âŒ Training failed:');
        console.log(`   Error: ${result.error}\n`);
      }
    } else {
      console.log('   âš ï¸  Skipping real training (set RUN_REAL_TRAINING=true to enable)');
      console.log('   This test validates the adapter structure only.\n');
    }

    // Success summary
    console.log('âœ… OllamaLoRAAdapter Standalone Test: PASSED');
    console.log('===========================================');
    console.log('âœ“ Adapter initialized successfully');
    console.log('âœ“ Capabilities reported correctly');
    console.log('âœ“ Strategy identified as local-llama-cpp');
    console.log('âœ“ Cost estimation works (FREE local training)');
    console.log('âœ“ Time estimation works (50ms per example per epoch)');
    console.log('âœ“ Dataset export works');
    console.log('âœ“ Cost comparison shows Ollama advantage (FREE)');
    if (shouldTrain) {
      console.log('âœ“ Real training completed successfully');
    }
    console.log();
    console.log('Benefits of Ollama/llama.cpp:');
    console.log('- 100% free (no API costs)');
    console.log('- 100% private (no data leaves your machine)');
    console.log('- Supports any GGUF model');
    console.log('- Works offline');
    console.log('- Fast with GPU');

  } catch (error) {
    console.error('âŒ OllamaLoRAAdapter Test FAILED');
    console.error('==================================');
    if (error instanceof Error) {
      console.error(`Error: ${error.message}`);
      console.error(`Stack: ${error.stack}`);
    } else {
      console.error(`Error: ${String(error)}`);
    }
    process.exit(1);
  }
}

/**
 * Create minimal test dataset (5 examples for MVP)
 * Philosophy: "simplest possible models/datasets - just needs to finish"
 */
function createMinimalDataset(): TrainingDataset {
  const personaId = 'test-persona-ollama-1234-1234-1234-123456789012' as UUID;
  const personaName = 'Test Ollama AI';

  const examples: TrainingExample[] = [
    {
      messages: [
        { role: 'system', content: `You are ${personaName}, a helpful AI assistant specialized in local AI.` },
        { role: 'user', content: 'What is Ollama?' },
        { role: 'assistant', content: 'Ollama is a tool for running large language models locally on your machine, providing privacy and offline access.' }
      ]
    },
    {
      messages: [
        { role: 'system', content: `You are ${personaName}, a helpful AI assistant specialized in local AI.` },
        { role: 'user', content: 'What are the benefits of local AI?' },
        { role: 'assistant', content: 'Local AI offers complete privacy, no API costs, offline capability, and full control over your data.' }
      ]
    },
    {
      messages: [
        { role: 'system', content: `You are ${personaName}, a helpful AI assistant specialized in local AI.` },
        { role: 'user', content: 'What is LoRA fine-tuning?' },
        { role: 'assistant', content: 'LoRA (Low-Rank Adaptation) is a parameter-efficient method that adds small trainable layers to frozen models.' }
      ]
    },
    {
      messages: [
        { role: 'system', content: `You are ${personaName}, a helpful AI assistant specialized in local AI.` },
        { role: 'user', content: 'Why use llama.cpp?' },
        { role: 'assistant', content: 'llama.cpp enables efficient inference and training on consumer hardware without requiring expensive GPUs.' }
      ]
    },
    {
      messages: [
        { role: 'system', content: `You are ${personaName}, a helpful AI assistant specialized in local AI.` },
        { role: 'user', content: 'What is GGUF format?' },
        { role: 'assistant', content: 'GGUF is a file format for storing quantized LLMs efficiently, optimized for llama.cpp.' }
      ]
    }
  ];

  return {
    examples,
    metadata: {
      personaId,
      personaName,
      traitType: 'conversational',
      createdAt: Date.now(),
      source: 'conversations',
      totalExamples: examples.length
    }
  };
}

// Run main if executed directly
if (require.main === module) {
  main().catch((error) => {
    console.error('Fatal error:', error);
    process.exit(1);
  });
}

// Export for testing
export { main, createMinimalDataset };
