/**
 * ConversationHistorySource - Loads chat message history for RAG context
 *
 * Features:
 * - Uses queryWithJoin to load messages + sender info in one query (4.5x faster)
 * - Role assignment: own messages = 'assistant', others = 'user'
 * - Chronological ordering (oldest first for LLM context)
 * - Media attachment metadata in message text
 * - Token budget-aware message limiting
 */

import type { RAGSource, RAGSourceContext, RAGSection } from '../shared/RAGSource';
import type { LLMMessage } from '../shared/RAGTypes';
import { ORM } from '../../../daemons/data-daemon/server/ORM';
import { ChatMessageEntity } from '../../data/entities/ChatMessageEntity';
import { Events } from '../../core/shared/Events';
import { Logger } from '../../core/logging/Logger';

const log = Logger.create('ConversationHistorySource', 'rag');

// Estimate ~4 tokens per word, ~5 words per line average
const TOKENS_PER_MESSAGE_ESTIMATE = 50;

// Patterns for detecting fabricated conversations within a single message body.
// These messages were generated by models that hallucinated entire multi-party
// conversations instead of responding as themselves. They poison LLM context
// and cause cascading failures (cloud AIs adopting "silence protocol").
//
// Formats seen in the wild:
//   "2/16/2026 2:24:03 PM Teacher AI: ..."     (date + time + speaker)
//   "[02:01] Teacher AI: ..."                   (bracketed time + speaker)
//   "[03:00] Helper AI: That's a good point..." (bracketed time + speaker)
//   "Gemini: I'm happy to chat..."              (single-word speaker prefix)
//   "Teacher AI: I think that's a great..."     (multi-word speaker prefix)

// Full date + time at line start
const FABRICATED_DATE_RE = /^\s*\d{1,4}[/-]\d{1,2}[/-]\d{1,4}\s+\d{1,2}:\d{2}\s+[A-Z]/gm;
// Bracketed time at line start: [02:01], [14:30], etc.
const FABRICATED_BRACKET_TIME_RE = /^\s*\[\d{1,2}:\d{2}\]\s+[A-Z]/gm;
// Multi-word speaker prefix: "Teacher AI:", "Helper AI:", "CodeReview AI:"
const FABRICATED_SPEAKER_RE = /^[A-Z][a-zA-Z]+\s+[A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)*:\s+\S/gm;
// Single-word known AI speaker prefix: "Gemini:", "Groq:", "Together:", "Fireworks:"
const FABRICATED_SINGLE_SPEAKER_RE = /^(?:Gemini|Groq|Together|Fireworks|Claude|GPT|Local|Joel|Anonymous|Qwen|DeepSeek|Grok|Ollama|Helper|Teacher|CodeReview):\s+\S/gm;

/**
 * Check if a message body is a fabricated multi-party conversation.
 * Returns true if the message contains 3+ timestamped lines,
 * 4+ multi-word speaker prefixes with 2+ distinct names, or
 * 3+ single-word known AI speaker prefixes.
 */
function isFabricatedConversation(text: string): boolean {
  if (!text || text.length < 60) return false;

  // Check 1: Full date+time timestamped speaker lines
  const dateMatches = text.match(FABRICATED_DATE_RE);
  if (dateMatches && dateMatches.length >= 3) return true;

  // Check 2: Bracketed [HH:MM] timestamped lines
  const bracketMatches = text.match(FABRICATED_BRACKET_TIME_RE);
  if (bracketMatches && bracketMatches.length >= 3) return true;

  // Check 3: Multi-word speaker prefixes with distinct names
  const speakerMatches = text.match(FABRICATED_SPEAKER_RE);
  if (speakerMatches && speakerMatches.length >= 4) {
    const names = new Set(speakerMatches.map(m => m.split(':')[0].trim()));
    if (names.size >= 2) return true;
  }

  // Check 4: Single-word known AI speaker prefixes
  const singleMatches = text.match(FABRICATED_SINGLE_SPEAKER_RE);
  if (singleMatches && singleMatches.length >= 3) {
    const names = new Set(singleMatches.map(m => m.split(':')[0].trim()));
    if (names.size >= 2) return true;
  }

  return false;
}

// ── Bare tool call detection ──────────────────────────────────────
// When an AI outputs a tool call as plain text (not a proper tool_use block),
// it gets saved as a chat message. Other AIs see it in history and copy the
// broken format, causing cascading hallucination of invalid tool syntax.
//
// These patterns catch common failed tool call formats:
// - "code/tree {"path":"."}"  (tool name + JSON)
// - "code/verify filePath:"..."" (tool name + key:value params)
// - '{"maxDepth":1,"path":"."}' (bare JSON)
// - "<function=code_tree>{...}</function>" (Groq function-style)

const TOOL_NAME_RE = /^[a-z][a-z0-9_]*(?:\/[a-z][a-z0-9_]*)+/;  // path-like: code/tree, data/list
const BARE_JSON_RE = /^\s*\{[\s\S]*\}\s*$/;
const FUNCTION_CALL_RE = /^<?function[=\s]/;

/**
 * Check if a message is a bare tool call attempt that would poison context.
 * Returns the tool name if detected, null otherwise.
 */
function detectBareToolCall(text: string): string | null {
  if (!text || text.length < 4 || text.length > 500) return null;

  const trimmed = text.trim();

  // Pattern 1: tool/name followed by JSON or key:value params (no preceding prose)
  const toolMatch = trimmed.match(TOOL_NAME_RE);
  if (toolMatch) {
    const afterName = trimmed.slice(toolMatch[0].length).trim();
    // Must have params (JSON or key:"value" or key:value) and no preceding text
    if (afterName.startsWith('{') || /^[a-zA-Z]+[:=]/.test(afterName)) {
      return toolMatch[0];
    }
    // Just the tool name alone with nothing else (e.g. "code/tree")
    if (afterName.length === 0 && trimmed === toolMatch[0]) {
      return toolMatch[0];
    }
  }

  // Pattern 2: Pure JSON (no surrounding text) — likely bare params
  if (BARE_JSON_RE.test(trimmed)) {
    return 'unknown';
  }

  // Pattern 3: Groq function-style: <function=name>{json}</function> or function=name>{json}
  if (FUNCTION_CALL_RE.test(trimmed)) {
    return trimmed.match(/function[=\s]+(\S+)/)?.[1] ?? 'unknown';
  }

  return null;
}

type MessageWithSender = ChatMessageEntity & { sender?: { displayName: string; userType: string } };

/** Cache entry for room messages — maintained by event subscription */
interface MessageCacheEntry {
  messages: MessageWithSender[];
  fetchedAt: number;
  limit: number;
}

/** In-flight request entry for single-flight coalescing */
interface InflightEntry {
  promise: Promise<MessageWithSender[]>;
  limit: number;
}

export class ConversationHistorySource implements RAGSource {
  readonly name = 'conversation-history';
  readonly priority = 80;  // High - conversation is core context
  readonly defaultBudgetPercent = 25;  // Gets largest share of budget

  // Room message cache: event-driven freshness. 30s TTL is a safety net only.
  // Primary freshness comes from event subscription updating cache entries.
  private static _roomCache: Map<string, MessageCacheEntry> = new Map();
  private static readonly CACHE_TTL_MS = 30_000;

  // Single-flight coalescing: when multiple personas query the same room
  // simultaneously, only ONE DB query fires. Others await the same promise.
  private static _inflight: Map<string, InflightEntry> = new Map();

  // Event subscription for real-time cache maintenance.
  // New messages update the cache immediately — no staleness, no DB re-query.
  private static _eventSubscribed = false;

  private static initEventSubscription(): void {
    if (ConversationHistorySource._eventSubscribed) return;
    ConversationHistorySource._eventSubscribed = true;

    Events.subscribe(`data:${ChatMessageEntity.collection}:created`, (entity: any) => {
      const msg = entity as ChatMessageEntity;
      if (!msg.roomId) return;

      const cached = ConversationHistorySource._roomCache.get(msg.roomId);
      if (cached) {
        // Prepend new message (cache is newest-first order, reversed later for LLM)
        cached.messages.unshift(msg as MessageWithSender);
        if (cached.messages.length > cached.limit + 10) {
          cached.messages.length = cached.limit; // Trim excess
        }
        cached.fetchedAt = Date.now(); // Reset TTL — cache is now fresh
      }
    });
  }

  /**
   * Access cached raw messages for a room (used by extractArtifacts to avoid duplicate DB query).
   * Returns null if cache is expired or empty — caller should fall back to DB.
   */
  static getCachedRawMessages(roomId: string): MessageWithSender[] | null {
    const cached = ConversationHistorySource._roomCache.get(roomId);
    if (cached && (Date.now() - cached.fetchedAt) < ConversationHistorySource.CACHE_TTL_MS) {
      return cached.messages;
    }
    return null;
  }

  isApplicable(_context: RAGSourceContext): boolean {
    // Always applicable - every RAG build needs conversation context
    return true;
  }

  async load(context: RAGSourceContext, allocatedBudget: number): Promise<RAGSection> {
    const startTime = performance.now();
    ConversationHistorySource.initEventSubscription();

    // Calculate max messages based on budget
    const budgetBasedLimit = Math.max(5, Math.floor(allocatedBudget / TOKENS_PER_MESSAGE_ESTIMATE));

    // CRITICAL: Respect latency-aware limit from ChatRAGBuilder if provided
    // This prevents timeout on slow local models by limiting input tokens
    const optionsLimit = context.options?.maxMessages;
    const maxMessages = optionsLimit ? Math.min(budgetBasedLimit, optionsLimit) : budgetBasedLimit;

    log.debug(`Message limit: ${maxMessages} (budget=${budgetBasedLimit}, latencyLimit=${optionsLimit ?? 'none'})`);

    try {
      let messages: MessageWithSender[] = [];

      // Check completed cache first (2s TTL)
      const cacheKey = context.roomId;
      const cached = ConversationHistorySource._roomCache.get(cacheKey);
      const now = Date.now();

      if (cached && (now - cached.fetchedAt) < ConversationHistorySource.CACHE_TTL_MS && cached.limit >= maxMessages) {
        messages = cached.messages.slice(0, maxMessages);
        log.debug(`Cache hit for room ${context.roomId?.slice(0, 8)} (${messages.length} messages)`);
      } else {
        // Cache miss — use single-flight coalescing to prevent thundering herd.
        // When 16 personas query the same room simultaneously, only the first
        // triggers a DB query. The other 15 await the same promise.
        const inflight = ConversationHistorySource._inflight.get(cacheKey);
        if (inflight && inflight.limit >= maxMessages) {
          // Another request is already in-flight for this room — piggyback
          log.debug(`Coalescing request for room ${context.roomId?.slice(0, 8)}`);
          messages = (await inflight.promise).slice(0, maxMessages);
        } else {
          // First request for this room — start DB query and register as in-flight
          const fetchPromise = this.fetchMessages(context.roomId, maxMessages);
          ConversationHistorySource._inflight.set(cacheKey, {
            promise: fetchPromise,
            limit: maxMessages
          });
          try {
            messages = await fetchPromise;
            // Populate TTL cache for subsequent requests
            ConversationHistorySource._roomCache.set(cacheKey, {
              messages,
              fetchedAt: Date.now(),
              limit: maxMessages
            });
          } finally {
            ConversationHistorySource._inflight.delete(cacheKey);
          }
        }
      }

      if (messages.length === 0) {
        return this.emptySection(startTime);
      }

      // Reverse to get oldest-first (LLMs expect chronological order)
      const orderedMessages = messages.reverse();

      // Filter out fabricated conversation messages — these are hallucinated
      // multi-party conversations that poison context and cause cascading
      // "silence protocol" failures in cloud AIs.
      let filteredCount = 0;
      const cleanMessages = orderedMessages.filter((msg: MessageWithSender) => {
        const text = msg.content?.text || '';
        if (isFabricatedConversation(text)) {
          filteredCount++;
          return false;
        }
        return true;
      });
      if (filteredCount > 0) {
        log.warn(`Filtered ${filteredCount} fabricated conversation messages from history`);
      }

      // Sanitize bare tool call messages — replace with contextual note
      // so other AIs know someone attempted a tool but don't copy the broken syntax
      let sanitizedCount = 0;
      for (const msg of cleanMessages) {
        const text = msg.content?.text || '';
        const toolName = detectBareToolCall(text);
        if (toolName && msg.senderId !== context.personaId) {
          // Only sanitize OTHER AIs' messages (preserve own for self-correction context)
          const senderName = (msg as any).sender?.displayName || msg.senderName || 'Someone';
          msg.content = { ...msg.content, text: `[${senderName} used ${toolName}]` };
          sanitizedCount++;
        }
      }
      if (sanitizedCount > 0) {
        log.info(`Sanitized ${sanitizedCount} bare tool call messages from history`);
      }

      // Convert to LLM message format
      const llmMessages: LLMMessage[] = cleanMessages.map((msg: MessageWithSender) => {
        let messageText = msg.content?.text || '';

        // Add media metadata to message text so AIs know images exist
        if (msg.content?.media && msg.content.media.length > 0) {
          const mediaDescriptions = msg.content.media.map((item: { type?: string; filename?: string; mimeType?: string }, idx: number) => {
            const parts = [
              `[${item.type || 'attachment'}${idx + 1}]`,
              item.filename || 'unnamed',
              item.mimeType ? `(${item.mimeType})` : ''
            ].filter(Boolean);
            return parts.join(' ');
          });

          const mediaNote = `\n[Attachments: ${mediaDescriptions.join(', ')} - messageId: ${msg.id}]`;
          messageText += mediaNote;
        }

        // Role assignment: own messages = 'assistant', others = 'user'
        const isOwnMessage = msg.senderId === context.personaId;
        const role = isOwnMessage ? 'assistant' as const : 'user' as const;

        // Get sender name from JOIN result or fallback
        const senderName = (msg as any).sender?.displayName || msg.senderName || 'Unknown';

        // Convert timestamp to number (milliseconds)
        let timestampMs: number | undefined;
        if (msg.timestamp) {
          if (typeof msg.timestamp === 'number') {
            timestampMs = msg.timestamp;
          } else if (typeof msg.timestamp === 'string') {
            timestampMs = new Date(msg.timestamp).getTime();
          } else if (msg.timestamp instanceof Date) {
            timestampMs = msg.timestamp.getTime();
          }
        }

        return {
          role,
          content: messageText,
          name: senderName,
          timestamp: timestampMs
        };
      });

      const loadTimeMs = performance.now() - startTime;
      const tokenCount = llmMessages.reduce((sum, m) => sum + this.estimateTokens(m.content), 0);

      log.debug(`Loaded ${llmMessages.length} messages in ${loadTimeMs.toFixed(1)}ms (~${tokenCount} tokens)`);

      return {
        sourceName: this.name,
        tokenCount,
        loadTimeMs,
        messages: llmMessages,
        metadata: {
          messageCount: llmMessages.length,
          roomId: context.roomId,
          personaId: context.personaId
        }
      };
    } catch (error: any) {
      log.error(`Failed to load conversation history: ${error.message}`);
      return this.emptySection(startTime, error.message);
    }
  }

  /** Fetch messages from DB (extracted for caching) */
  private async fetchMessages(roomId: string, maxMessages: number): Promise<MessageWithSender[]> {
    // Try queryWithJoin first (4.5x faster), fall back to regular query
    try {
      const result = await ORM.queryWithJoin<MessageWithSender>({
        collection: ChatMessageEntity.collection,
        filter: { roomId },
        joins: [{
          collection: 'users',
          alias: 'sender',
          localField: 'senderId',
          foreignField: 'id',
          type: 'left',
          select: ['displayName', 'userType']
        }],
        sort: [{ field: 'timestamp', direction: 'desc' }],
        limit: maxMessages
      });

      if (result.success && result.data && result.data.length > 0) {
        return result.data.map((record: { data: MessageWithSender }) => record.data);
      }
    } catch (joinError: any) {
      // queryWithJoin not supported - fall back to regular query
      log.debug(`queryWithJoin not available (${joinError.message}), using regular query`);

      const result = await ORM.query<ChatMessageEntity>({
        collection: ChatMessageEntity.collection,
        filter: { roomId },
        sort: [{ field: 'timestamp', direction: 'desc' }],
        limit: maxMessages
      });

      if (result.success && result.data && result.data.length > 0) {
        return result.data.map((record: { data: ChatMessageEntity }) => record.data as MessageWithSender);
      }
    }
    return [];
  }

  private emptySection(startTime: number, error?: string): RAGSection {
    return {
      sourceName: this.name,
      tokenCount: 0,
      loadTimeMs: performance.now() - startTime,
      messages: [],
      metadata: error ? { error } : {}
    };
  }

  private estimateTokens(text: string): number {
    // Rough estimate: ~4 characters per token
    return Math.ceil(text.length / 4);
  }
}
