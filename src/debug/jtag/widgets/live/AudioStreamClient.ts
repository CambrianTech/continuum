/**
 * AudioStreamClient - WebSocket client for real-time audio streaming
 *
 * Connects to the Rust streaming-core call server for:
 * - Joining/leaving calls
 * - Streaming microphone audio
 * - Receiving mix-minus audio (everyone except self)
 * - Playing back received audio
 */

// Import generated types from Rust (single source of truth)
// Generated by: cargo test -p streaming-core
import type { CallMessage } from '../../shared/generated/CallMessage';

// Audio constants - SINGLE SOURCE OF TRUTH
import {
  AUDIO_SAMPLE_RATE,
  AUDIO_FRAME_SIZE,
  CALL_SERVER_URL,
} from '../../shared/AudioConstants';

/** Transcription result from Whisper STT */
export interface TranscriptionResult {
  userId: string;
  displayName: string;
  text: string;
  confidence: number;
  language: string;
}

interface AudioStreamClientOptions {
  /** WebSocket server URL (default: CALL_SERVER_URL from AudioConstants) */
  serverUrl?: string;
  /** Sample rate for audio (default: AUDIO_SAMPLE_RATE from AudioConstants) */
  sampleRate?: number;
  /** Frame size in samples (default: AUDIO_FRAME_SIZE from AudioConstants) */
  frameSize?: number;
  /** Callback when participant joins */
  onParticipantJoined?: (userId: string, displayName: string) => void;
  /** Callback when participant leaves */
  onParticipantLeft?: (userId: string) => void;
  /** Callback for connection status changes */
  onConnectionChange?: (connected: boolean) => void;
  /** Callback for microphone audio level (0.0 to 1.0, called ~30x/sec) */
  onMicLevel?: (level: number) => void;
  /** Callback when speech is transcribed (VAD-triggered, Whisper STT) */
  onTranscription?: (result: TranscriptionResult) => void;
}

export class AudioStreamClient {
  private ws: WebSocket | null = null;
  private audioContext: AudioContext | null = null;
  private mediaStream: MediaStream | null = null;

  // Mic capture worklet (off main thread)
  private micWorkletNode: AudioWorkletNode | null = null;

  // Playback worklet (off main thread) - decodes AND plays
  private playbackWorkletNode: AudioWorkletNode | null = null;

  private micWorkletReady = false;
  private playbackWorkletReady = false;

  // Speaker (output) state
  private speakerMuted = false;
  private speakerVolume = 1.0;

  // Mic mute state (tracked locally for defense in depth)
  private micMuted = false;

  private serverUrl: string;
  private sampleRate: number;
  private frameSize: number;
  private options: AudioStreamClientOptions;

  private callId: string | null = null;
  private userId: string | null = null;
  private displayName: string | null = null;

  constructor(options: AudioStreamClientOptions = {}) {
    this.serverUrl = options.serverUrl || CALL_SERVER_URL;
    this.sampleRate = options.sampleRate || AUDIO_SAMPLE_RATE;
    this.frameSize = options.frameSize || AUDIO_FRAME_SIZE;
    this.options = options;
  }

  /**
   * Connect to the call server and join a call
   */
  async join(callId: string, userId: string, displayName: string): Promise<void> {
    this.callId = callId;
    this.userId = userId;
    this.displayName = displayName;

    // Initialize audio context and playback worklet FIRST (before WebSocket)
    // This ensures we're ready to play audio as soon as it arrives
    await this.initializePlayback();

    // Create WebSocket connection
    return new Promise((resolve, reject) => {
      try {
        this.ws = new WebSocket(this.serverUrl);
        // CRITICAL: Set binary type to arraybuffer for raw audio data
        // This eliminates base64 encoding overhead (~33%) for real-time audio
        this.ws.binaryType = 'arraybuffer';

        this.ws.onopen = () => {
          console.log('AudioStreamClient: Connected to call server');
          this.options.onConnectionChange?.(true);

          // Send join message (browser clients are always human, not AI)
          const joinMsg: CallMessage = {
            type: 'Join',
            call_id: callId,
            user_id: userId,
            display_name: displayName,
            is_ai: false,
          };
          this.ws?.send(JSON.stringify(joinMsg));
          resolve();
        };

        this.ws.onmessage = (event) => {
          // Binary frames are raw audio data (i16 PCM, little-endian)
          if (event.data instanceof ArrayBuffer) {
            this.handleBinaryAudio(event.data);
          } else {
            // Text frames are JSON (transcriptions, join/leave notifications)
            this.handleMessage(event.data);
          }
        };

        this.ws.onerror = (error) => {
          console.error('AudioStreamClient: WebSocket error:', error);
          reject(error);
        };

        this.ws.onclose = () => {
          console.log('AudioStreamClient: Disconnected from call server');
          this.options.onConnectionChange?.(false);
          this.cleanup();
        };
      } catch (error) {
        reject(error);
      }
    });
  }

  /**
   * Initialize playback worklet - runs on AUDIO THREAD, not main thread
   * Decoding and playback all happen off main thread
   */
  private async initializePlayback(): Promise<void> {
    if (!this.audioContext) {
      this.audioContext = new AudioContext({ sampleRate: this.sampleRate });
    }

    // Load playback worklet module
    if (!this.playbackWorkletReady) {
      try {
        const workletUrl = new URL('./audio-playback-worklet.js', import.meta.url).href;
        await this.audioContext.audioWorklet.addModule(workletUrl);
        this.playbackWorkletReady = true;
      } catch (error) {
        console.error('AudioStreamClient: Failed to load playback worklet:', error);
        throw error;
      }
    }

    // Create playback worklet node - ALL audio processing on audio thread
    this.playbackWorkletNode = new AudioWorkletNode(this.audioContext, 'playback-processor');

    // Connect to destination (speakers)
    this.playbackWorkletNode.connect(this.audioContext.destination);

    // Set initial mute/volume state
    this.playbackWorkletNode.port.postMessage({ type: 'mute', muted: this.speakerMuted });
    this.playbackWorkletNode.port.postMessage({ type: 'volume', volume: this.speakerVolume });

    console.log('AudioStreamClient: Playback worklet ready (off main thread)');
  }

  /**
   * Leave the current call
   */
  leave(): void {
    if (this.ws && this.ws.readyState === WebSocket.OPEN) {
      const leaveMsg: CallMessage = { type: 'Leave' };
      this.ws.send(JSON.stringify(leaveMsg));
    }
    this.cleanup();
  }

  /**
   * Start streaming microphone audio
   * Uses AudioWorklet for processing on audio thread (NOT main thread)
   */
  async startMicrophone(): Promise<void> {
    if (!this.ws || this.ws.readyState !== WebSocket.OPEN) {
      throw new Error('Not connected to call server');
    }

    // Ensure audio context exists
    if (!this.audioContext) {
      this.audioContext = new AudioContext({ sampleRate: this.sampleRate });
    }

    // Load mic capture worklet module (runs on audio rendering thread, not main thread)
    if (!this.micWorkletReady) {
      try {
        const workletUrl = new URL('./audio-worklet-processor.js', import.meta.url).href;
        await this.audioContext.audioWorklet.addModule(workletUrl);
        this.micWorkletReady = true;
      } catch (error) {
        console.error('AudioStreamClient: Failed to load mic worklet:', error);
        throw error;
      }
    }

    // Get microphone stream
    this.mediaStream = await navigator.mediaDevices.getUserMedia({
      audio: {
        sampleRate: this.sampleRate,
        channelCount: 1,
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
      },
    });

    // Create audio processing pipeline using AudioWorklet (off main thread)
    const source = this.audioContext.createMediaStreamSource(this.mediaStream);

    // Create AudioWorkletNode - processing happens on audio thread
    this.micWorkletNode = new AudioWorkletNode(this.audioContext, 'microphone-processor');

    // Listen for audio frames from the worklet (via MessagePort)
    // Worklet sends { frame: Float32Array, level: number }
    this.micWorkletNode.port.onmessage = (event) => {
      const { frame, level } = event.data;
      // Send audio to server
      this.sendAudioFrame(frame);
      // Notify UI of mic level for visual feedback
      this.options.onMicLevel?.(level);
    };

    // Connect: mic -> worklet -> (nowhere, we just capture)
    source.connect(this.micWorkletNode);
    // Don't connect to destination - we're just capturing, not playing back locally

    console.log('AudioStreamClient: Microphone streaming started (AudioWorklet - off main thread)');
  }

  /**
   * Stop streaming microphone audio
   */
  stopMicrophone(): void {
    if (this.micWorkletNode) {
      this.micWorkletNode.disconnect();
      this.micWorkletNode.port.close();
      this.micWorkletNode = null;
    }

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    console.log('AudioStreamClient: Microphone streaming stopped');
  }

  /**
   * Set mic mute status (your input to others)
   * Tracked both client-side (to stop sending) and server-side (to skip processing)
   */
  setMuted(muted: boolean): void {
    this.micMuted = muted; // Track locally to stop sending audio
    if (this.ws && this.ws.readyState === WebSocket.OPEN) {
      const muteMsg: CallMessage = { type: 'Mute', muted };
      this.ws.send(JSON.stringify(muteMsg));
      console.log(`AudioStreamClient: Mute set to ${muted}`);
    }
  }

  /**
   * Set speaker muted (your output - what you hear)
   */
  setSpeakerMuted(muted: boolean): void {
    this.speakerMuted = muted;
    // Send to playback worklet (runs on audio thread)
    if (this.playbackWorkletNode) {
      this.playbackWorkletNode.port.postMessage({ type: 'mute', muted });
    }
  }

  /**
   * Set speaker volume (0.0 to 1.0)
   */
  setSpeakerVolume(volume: number): void {
    this.speakerVolume = Math.max(0, Math.min(1, volume));
    // Send to playback worklet (runs on audio thread)
    if (this.playbackWorkletNode) {
      this.playbackWorkletNode.port.postMessage({ type: 'volume', volume: this.speakerVolume });
    }
  }

  /**
   * Check if connected
   */
  get isConnected(): boolean {
    return this.ws?.readyState === WebSocket.OPEN;
  }

  /**
   * Handle incoming JSON WebSocket messages (transcriptions, join/leave notifications)
   * Audio now comes as binary frames - see handleBinaryAudio()
   */
  private handleMessage(data: string): void {
    try {
      const msg = JSON.parse(data) as CallMessage;

      switch (msg.type) {
        case 'ParticipantJoined':
          this.options.onParticipantJoined?.(msg.user_id, msg.display_name);
          break;
        case 'ParticipantLeft':
          this.options.onParticipantLeft?.(msg.user_id);
          break;
        case 'Transcription':
          // [STEP 8] Browser received transcription from Rust WebSocket
          console.log(`[STEP 8] ðŸŽ§ Browser received transcription: "${msg.text.slice(0, 50)}..." from ${msg.display_name}`);
          this.options.onTranscription?.({
            userId: msg.user_id,
            displayName: msg.display_name,
            text: msg.text,
            confidence: msg.confidence,
            language: msg.language,
          });
          break;
        case 'MixedAudio':
          // DEPRECATED: Audio now comes as binary frames
          // Keep for backwards compatibility during transition
          this.handleMixedAudio(msg.data);
          break;
      }
    } catch (error) {
      console.error('AudioStreamClient: Failed to parse message:', error);
    }
  }

  /**
   * Send audio frame to server as BINARY WebSocket frame
   * Direct bytes transfer - no JSON, no base64 encoding overhead
   */
  private sendAudioFrame(samples: Float32Array): void {
    if (!this.ws || this.ws.readyState !== WebSocket.OPEN) return;
    if (this.micMuted) return; // Don't send audio when muted (client-side check)

    // Convert Float32 (-1 to 1) to Int16 (-32768 to 32767)
    const int16Data = new Int16Array(samples.length);
    for (let i = 0; i < samples.length; i++) {
      int16Data[i] = Math.max(-32768, Math.min(32767, Math.round(samples[i] * 32767)));
    }

    // Send raw bytes directly - WebSocket binary frame
    // Rust server receives as Message::Binary(data) and converts with bytes_to_i16()
    this.ws.send(int16Data.buffer);
  }

  /**
   * Handle binary audio frames from server
   * Raw i16 PCM data - no base64 decoding needed
   * This is the new high-performance path for real-time audio
   */
  private handleBinaryAudio(arrayBuffer: ArrayBuffer): void {
    // Ensure audio context is running (needed after user interaction)
    if (this.audioContext?.state === 'suspended') {
      this.audioContext.resume();
    }

    if (!this.playbackWorkletNode) return;

    // Direct ArrayBuffer to Int16Array view (zero-copy)
    const int16Data = new Int16Array(arrayBuffer);

    // Convert Int16 to Float32 for Web Audio API
    const samples = new Float32Array(int16Data.length);
    for (let i = 0; i < int16Data.length; i++) {
      samples[i] = int16Data[i] / 32768;
    }

    // Transfer Float32Array to worklet (zero-copy via transferable)
    this.playbackWorkletNode.port.postMessage(
      { type: 'audio', samples },
      [samples.buffer]  // Transfer ownership - zero-copy
    );
  }

  /**
   * Handle received mixed audio (DEPRECATED - for backwards compatibility)
   * Decode on main thread (fast), transfer Float32Array to worklet (zero-copy)
   */
  private handleMixedAudio(base64Data: string): void {
    // Ensure audio context is running (needed after user interaction)
    if (this.audioContext?.state === 'suspended') {
      this.audioContext.resume();
    }

    if (!this.playbackWorkletNode) return;

    // Decode base64 on main thread (atob not available in AudioWorkletGlobalScope)
    const binaryString = atob(base64Data);
    const bytes = new Uint8Array(binaryString.length);
    for (let i = 0; i < binaryString.length; i++) {
      bytes[i] = binaryString.charCodeAt(i);
    }
    const int16Data = new Int16Array(bytes.buffer);

    // Convert Int16 to Float32
    const samples = new Float32Array(int16Data.length);
    for (let i = 0; i < int16Data.length; i++) {
      samples[i] = int16Data[i] / 32768;
    }

    // Transfer Float32Array to worklet (zero-copy via transferable)
    this.playbackWorkletNode.port.postMessage(
      { type: 'audio', samples },
      [samples.buffer]  // Transfer ownership - zero-copy
    );
  }

  /**
   * Cleanup resources
   */
  private cleanup(): void {
    this.stopMicrophone();

    // Cleanup playback worklet
    if (this.playbackWorkletNode) {
      this.playbackWorkletNode.disconnect();
      this.playbackWorkletNode.port.close();
      this.playbackWorkletNode = null;
    }

    if (this.audioContext) {
      this.audioContext.close();
      this.audioContext = null;
    }

    if (this.ws) {
      this.ws.close();
      this.ws = null;
    }

    this.micWorkletReady = false;
    this.playbackWorkletReady = false;
    this.callId = null;
    this.userId = null;
    this.displayName = null;
  }
}
