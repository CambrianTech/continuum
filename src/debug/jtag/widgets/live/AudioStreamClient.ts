/**
 * AudioStreamClient - WebSocket client for real-time audio streaming
 *
 * Connects to the Rust streaming-core call server for:
 * - Joining/leaving calls
 * - Streaming microphone audio
 * - Receiving mix-minus audio (everyone except self)
 * - Playing back received audio
 */

// Import generated types from Rust (single source of truth)
// Generated by: cargo test -p streaming-core
import type { CallMessage } from '../../shared/generated/CallMessage';

interface AudioStreamClientOptions {
  /** WebSocket server URL (default: ws://127.0.0.1:50053) */
  serverUrl?: string;
  /** Sample rate for audio (default: 16000) */
  sampleRate?: number;
  /** Frame size in samples (default: 512 - must be power of 2 for Web Audio API) */
  frameSize?: number;
  /** Callback when participant joins */
  onParticipantJoined?: (userId: string, displayName: string) => void;
  /** Callback when participant leaves */
  onParticipantLeft?: (userId: string) => void;
  /** Callback for connection status changes */
  onConnectionChange?: (connected: boolean) => void;
}

export class AudioStreamClient {
  private ws: WebSocket | null = null;
  private audioContext: AudioContext | null = null;
  private mediaStream: MediaStream | null = null;
  private processorNode: ScriptProcessorNode | null = null;
  private playbackQueue: Float32Array[] = [];
  private isPlaying = false;
  private nextPlaybackTime = 0; // Scheduled playback time for seamless audio

  // Max queue depth to prevent memory growth in long sessions
  // At 20ms per frame, 50 frames = 1 second of buffer
  private static readonly MAX_PLAYBACK_QUEUE = 50;

  private serverUrl: string;
  private sampleRate: number;
  private frameSize: number;
  private options: AudioStreamClientOptions;

  private callId: string | null = null;
  private userId: string | null = null;
  private displayName: string | null = null;

  constructor(options: AudioStreamClientOptions = {}) {
    this.serverUrl = options.serverUrl || 'ws://127.0.0.1:50053';
    this.sampleRate = options.sampleRate || 16000;
    this.frameSize = options.frameSize || 512;  // Must be power of 2 for Web Audio API
    this.options = options;
  }

  /**
   * Connect to the call server and join a call
   */
  async join(callId: string, userId: string, displayName: string): Promise<void> {
    this.callId = callId;
    this.userId = userId;
    this.displayName = displayName;

    // Create WebSocket connection
    return new Promise((resolve, reject) => {
      try {
        this.ws = new WebSocket(this.serverUrl);

        this.ws.onopen = () => {
          console.log('AudioStreamClient: Connected to call server');
          this.options.onConnectionChange?.(true);

          // Send join message
          const joinMsg: CallMessage = {
            type: 'Join',
            call_id: callId,
            user_id: userId,
            display_name: displayName,
          };
          this.ws?.send(JSON.stringify(joinMsg));
          resolve();
        };

        this.ws.onmessage = (event) => {
          this.handleMessage(event.data);
        };

        this.ws.onerror = (error) => {
          console.error('AudioStreamClient: WebSocket error:', error);
          reject(error);
        };

        this.ws.onclose = () => {
          console.log('AudioStreamClient: Disconnected from call server');
          this.options.onConnectionChange?.(false);
          this.cleanup();
        };
      } catch (error) {
        reject(error);
      }
    });
  }

  /**
   * Leave the current call
   */
  leave(): void {
    if (this.ws && this.ws.readyState === WebSocket.OPEN) {
      const leaveMsg: CallMessage = { type: 'Leave' };
      this.ws.send(JSON.stringify(leaveMsg));
    }
    this.cleanup();
  }

  /**
   * Start streaming microphone audio
   */
  async startMicrophone(): Promise<void> {
    if (!this.ws || this.ws.readyState !== WebSocket.OPEN) {
      throw new Error('Not connected to call server');
    }

    // Create audio context
    this.audioContext = new AudioContext({ sampleRate: this.sampleRate });

    // Get microphone stream
    this.mediaStream = await navigator.mediaDevices.getUserMedia({
      audio: {
        sampleRate: this.sampleRate,
        channelCount: 1,
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
      },
    });

    // Create audio processing pipeline
    const source = this.audioContext.createMediaStreamSource(this.mediaStream);

    // Use ScriptProcessorNode to capture audio frames
    // Note: This is deprecated but AudioWorklet requires more setup
    this.processorNode = this.audioContext.createScriptProcessor(this.frameSize, 1, 1);

    this.processorNode.onaudioprocess = (event) => {
      const inputData = event.inputBuffer.getChannelData(0);
      this.sendAudioFrame(inputData);
    };

    source.connect(this.processorNode);
    this.processorNode.connect(this.audioContext.destination);

    console.log('AudioStreamClient: Microphone streaming started');
  }

  /**
   * Stop streaming microphone audio
   */
  stopMicrophone(): void {
    if (this.processorNode) {
      this.processorNode.disconnect();
      this.processorNode = null;
    }

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    console.log('AudioStreamClient: Microphone streaming stopped');
  }

  /**
   * Set mute status
   */
  setMuted(muted: boolean): void {
    if (this.ws && this.ws.readyState === WebSocket.OPEN) {
      const muteMsg: CallMessage = { type: 'Mute', muted };
      this.ws.send(JSON.stringify(muteMsg));
    }
  }

  /**
   * Check if connected
   */
  get isConnected(): boolean {
    return this.ws?.readyState === WebSocket.OPEN;
  }

  /**
   * Handle incoming WebSocket messages
   */
  private handleMessage(data: string): void {
    try {
      const msg = JSON.parse(data) as CallMessage;

      switch (msg.type) {
        case 'MixedAudio':
          this.handleMixedAudio(msg.data);
          break;
        case 'ParticipantJoined':
          this.options.onParticipantJoined?.(msg.user_id, msg.display_name);
          break;
        case 'ParticipantLeft':
          this.options.onParticipantLeft?.(msg.user_id);
          break;
      }
    } catch (error) {
      console.error('AudioStreamClient: Failed to parse message:', error);
    }
  }

  /**
   * Send audio frame to server
   */
  private sendAudioFrame(samples: Float32Array): void {
    if (!this.ws || this.ws.readyState !== WebSocket.OPEN) return;

    // Convert Float32 (-1 to 1) to Int16 (-32768 to 32767)
    const int16Data = new Int16Array(samples.length);
    for (let i = 0; i < samples.length; i++) {
      int16Data[i] = Math.max(-32768, Math.min(32767, Math.round(samples[i] * 32767)));
    }

    // Encode as base64
    const bytes = new Uint8Array(int16Data.buffer);
    const base64 = btoa(String.fromCharCode(...bytes));

    const audioMsg: CallMessage = { type: 'Audio', data: base64 };
    this.ws.send(JSON.stringify(audioMsg));
  }

  /**
   * Handle received mixed audio
   */
  private handleMixedAudio(base64Data: string): void {
    // Decode base64 to Int16Array
    const binaryString = atob(base64Data);
    const bytes = new Uint8Array(binaryString.length);
    for (let i = 0; i < binaryString.length; i++) {
      bytes[i] = binaryString.charCodeAt(i);
    }
    const int16Data = new Int16Array(bytes.buffer);

    // Convert Int16 to Float32
    const float32Data = new Float32Array(int16Data.length);
    for (let i = 0; i < int16Data.length; i++) {
      float32Data[i] = int16Data[i] / 32768;
    }

    // Queue for playback with bounded size to prevent memory growth
    if (this.playbackQueue.length >= AudioStreamClient.MAX_PLAYBACK_QUEUE) {
      // Drop oldest frame to prevent memory growth
      this.playbackQueue.shift();
    }
    this.playbackQueue.push(float32Data);
    this.processPlaybackQueue();
  }

  /**
   * Process playback queue with scheduled timing for seamless audio
   *
   * Uses AudioContext.currentTime to schedule buffers back-to-back
   * without gaps. This is the correct way to handle streaming audio.
   */
  private processPlaybackQueue(): void {
    if (this.playbackQueue.length === 0) return;
    if (!this.audioContext) {
      this.audioContext = new AudioContext({ sampleRate: this.sampleRate });
    }

    // Ensure audioContext is running (needed after user interaction)
    if (this.audioContext.state === 'suspended') {
      this.audioContext.resume();
    }

    // Initialize nextPlaybackTime if not set or if we've fallen behind
    const now = this.audioContext.currentTime;
    if (this.nextPlaybackTime < now) {
      // Add small buffer (50ms) to prevent underrun
      this.nextPlaybackTime = now + 0.05;
    }

    // Schedule all queued buffers
    while (this.playbackQueue.length > 0) {
      const samples = this.playbackQueue.shift()!;

      // Create audio buffer
      const buffer = this.audioContext.createBuffer(1, samples.length, this.sampleRate);
      buffer.getChannelData(0).set(samples);

      // Calculate duration of this buffer
      const duration = samples.length / this.sampleRate;

      // Schedule to play at exact time (seamless)
      const source = this.audioContext.createBufferSource();
      source.buffer = buffer;
      source.connect(this.audioContext.destination);
      source.start(this.nextPlaybackTime);

      // Update next playback time for perfect continuity
      this.nextPlaybackTime += duration;
    }
  }

  /**
   * Cleanup resources
   */
  private cleanup(): void {
    this.stopMicrophone();

    if (this.audioContext) {
      this.audioContext.close();
      this.audioContext = null;
    }

    if (this.ws) {
      this.ws.close();
      this.ws = null;
    }

    this.playbackQueue = [];
    this.isPlaying = false;
    this.nextPlaybackTime = 0;
    this.callId = null;
    this.userId = null;
    this.displayName = null;
  }
}
