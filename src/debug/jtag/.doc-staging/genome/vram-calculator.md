# VRAM Calculator Integration Plan

**Goal**: Integrate apxml.com-style VRAM calculator into Continuum's content tab system for LoRA training planning.

**Inspired by**: https://apxml.com/tools/vram-calculator

---

## Architecture: VSCode-Style Content Tabs

### Current System (from ContentTypes.ts)
```typescript
interface ContentInfo {
  id: string;
  name: string;
  type: 'room' | 'user_chat' | 'system';  // â† ADD 'tool' type
  path: string;                            // â† e.g., '/tools/vram-calculator'
  displayName: string;
  description?: string;
  isActive: boolean;
}
```

### URL Routing Pattern
- **Chat rooms**: `/rooms/general`, `/rooms/academy`
- **User chats**: `/users/{userId}/chat`
- **Tools**: `/tools/vram-calculator` â† NEW
- **Diagnostics**: `/diagnostics` (future)
- **Training monitor**: `/training/{sessionId}` (future)

### Tab Behavior (VSCode-style)
- **Multiple tabs** can be open simultaneously
- **Active tab** shows in main content area
- **URL rewrites** on tab switch (`/rooms/general` â†’ `/tools/vram-calculator`)
- **Tab persistence** across sessions

---

## VRAM Calculator Features (from apxml.com)

### Input Parameters
1. **Model Selection**
   - Dropdown with 100+ models (Llama, Qwen, Mistral, DeepSeek, Gemma, Phi, etc.)
   - Auto-populate: parameter count, architecture, context length
   - Source: Our `POPULAR-MODELS-BY-PROVIDER.md` + provider BaseConfigs

2. **Training Configuration**
   - LoRA rank (r): 1-128 (default: 16)
   - Batch size: 1-128 (default: 4)
   - Gradient accumulation steps: 1-32 (default: 1)
   - Sequence length: 512-131072 (default: 2048)
   - Precision: 4-bit, 8-bit, 16-bit, 32-bit (default: 4-bit)

3. **Hardware Selection**
   - **Apple Silicon**: M1/M2/M3 (8GB, 16GB, 24GB, 32GB, 64GB, 96GB, 128GB)
   - **NVIDIA Consumer**: RTX 3060 (12GB), 3090 (24GB), 4060 Ti (16GB), 4090 (24GB)
   - **NVIDIA Pro**: A100 (40GB/80GB), H100 (80GB)
   - **AMD**: Radeon VII (16GB), MI210 (64GB), MI300X (192GB)
   - **Custom**: Manual VRAM entry

4. **Optimization Toggles**
   - Flash Attention (45% VRAM savings)
   - Gradient Checkpointing (70% VRAM savings)
   - 8-bit Optimizer (75% VRAM savings)
   - CPU Offloading (dynamic VRAM savings)
   - LoRA+ (separate learning rates, minimal VRAM impact)

### Output Display

**Memory Breakdown (Pie Chart)**
```
Total: 5.75 GB
â”œâ”€ Base Model: 3.00 GB (52.1%)  â† Model weights in selected precision
â”œâ”€ Activations: 1.41 GB (24.5%)  â† Forward pass intermediate results
â”œâ”€ Framework: 1.31 GB (22.7%)    â† PyTorch/framework overhead
â””â”€ LoRA: 0.04 GB (0.7%)          â† LoRA adapter weights (tiny!)
```

**Performance Metrics**
- **Training speed**: ~18 tok/sec (for DeepSeek-R1 1.5B on M2 Pro)
- **Estimated time**: Calculate based on dataset size + tok/sec
- **Cost estimate**: For cloud providers ($/hour * estimated hours)

**Feasibility Check**
- âœ… **Fits in VRAM** (5.75 GB < 16 GB available)
- âš ï¸ **Tight fit** (90%+ VRAM utilization, may need tweaks)
- âŒ **Won't fit** (exceeds available VRAM, suggest optimizations)

**Recommendations**
- Reduce batch size to X
- Enable gradient checkpointing
- Use 4-bit quantization instead of 8-bit
- Switch to smaller model variant
- Try CPU offloading

---

## Implementation Plan

### Phase 1: Calculator Widget (UI Only)

**File Structure**:
```
widgets/
â””â”€â”€ tools/
    â””â”€â”€ vram-calculator/
        â”œâ”€â”€ shared/
        â”‚   â”œâ”€â”€ VramCalculatorTypes.ts       # Calculator interfaces
        â”‚   â””â”€â”€ VramCalculatorLogic.ts       # Memory calculation formulas
        â”œâ”€â”€ browser/
        â”‚   â””â”€â”€ VramCalculatorWidget.ts      # Main widget
        â””â”€â”€ public/
            â”œâ”€â”€ vram-calculator.css          # Calculator styling
            â””â”€â”€ vram-calculator.html         # Widget template
```

**Key Classes**:
```typescript
// VramCalculatorTypes.ts
interface VramCalculatorConfig {
  model: ModelSelection;
  training: TrainingConfig;
  hardware: HardwareSelection;
  optimizations: OptimizationFlags;
}

interface MemoryEstimate {
  baseModel: number;      // MB
  activations: number;    // MB
  framework: number;      // MB
  lora: number;           // MB
  total: number;          // MB
  breakdown: MemoryBreakdown;
}

interface PerformanceEstimate {
  tokensPerSecond: number;
  estimatedTimeSeconds: number;
  costEstimate?: number;  // USD
}

// VramCalculatorLogic.ts
class VramCalculator {
  calculateMemory(config: VramCalculatorConfig): MemoryEstimate;
  estimatePerformance(config: VramCalculatorConfig): PerformanceEstimate;
  checkFeasibility(estimate: MemoryEstimate, hardware: HardwareSelection): FeasibilityResult;
  suggestOptimizations(estimate: MemoryEstimate, hardware: HardwareSelection): Recommendation[];
}
```

**Memory Calculation Formulas** (from LOCAL-TRAINING-PHASE2.md):
```typescript
baseModelMemory = (parameterCount * bytesPerParam) / (1024^3)
loraMemory = (loraRank * 2 * sumOfLayerDimensions * bytesPerParam) / (1024^3)
optimizerMemory = (numTrainableParams * 8) / (1024^3)  // Adam optimizer
gradientsMemory = (numTrainableParams * bytesPerParam) / (1024^3)
activationsMemory = (batchSize * seqLength * hiddenDim * numLayers * bytesPerParam) / (1024^3)

totalMemory = baseModelMemory + loraMemory + optimizerMemory + gradientsMemory + activationsMemory
```

**Optimization Multipliers**:
```typescript
if (flashAttention) activationsMemory *= 0.55;      // 45% savings
if (gradientCheckpointing) activationsMemory *= 0.30;  // 70% savings
if (optimizer8bit) optimizerMemory *= 0.25;         // 75% savings
```

### Phase 2: Integration with Provider System

**Connect to Provider Adapters**:
```typescript
// Fetch models from all providers
const allModels = await Promise.all([
  openAIConfig.getAvailableModels(),
  deepseekConfig.getAvailableModels(),
  fireworksConfig.getAvailableModels(),
  // ... etc
]);

// Filter for fine-tuning capable models
const fineTuneModels = allModels
  .flat()
  .filter(m => m.capabilities?.includes('fine-tuning'));

// Populate calculator dropdown
populateModelSelector(fineTuneModels);
```

**Cost Estimation**:
```typescript
// Get provider pricing from BaseConfigs
const costPerHour = getProviderCost(selectedProvider, selectedModel);
const estimatedHours = totalTokens / tokensPerSecond / 3600;
const totalCost = costPerHour * estimatedHours;
```

### Phase 3: Content Routing Integration

**Update ContentTypes.ts**:
```typescript
interface ContentInfo {
  type: 'room' | 'user_chat' | 'system' | 'tool';  // â† ADD 'tool'
  path: string;  // '/tools/vram-calculator'
}
```

**Register Tool Content**:
```typescript
// In ContentInfoManager
async getContentByPath(path: string): Promise<ContentInfo | null> {
  const [, pathType, contentId] = path.split('/');

  if (pathType === 'chat') {
    return await this.getChatContentInfo(contentId);
  }

  if (pathType === 'tools') {
    return await this.getToolContentInfo(contentId);  // â† NEW
  }

  return null;
}

private async getToolContentInfo(toolId: string): Promise<ContentInfo> {
  const toolConfigs = {
    'vram-calculator': {
      name: 'vram-calculator',
      displayName: 'VRAM Calculator',
      description: 'Estimate memory requirements for LoRA fine-tuning',
      widgetType: 'vram-calculator-widget'
    }
  };

  const config = toolConfigs[toolId];
  return {
    id: toolId,
    name: config.name,
    type: 'tool',
    path: `/tools/${toolId}`,
    displayName: config.displayName,
    description: config.description,
    isActive: true,
    createdAt: new Date(),
    updatedAt: new Date()
  };
}
```

**Tab Opening**:
```typescript
// User clicks "VRAM Calculator" in sidebar or menu
openContent('/tools/vram-calculator');

// MainWidget creates new tab
const contentInfo = await contentManager.getContentByPath('/tools/vram-calculator');
const tab = createTab(contentInfo);
const widget = document.createElement('vram-calculator-widget');
tab.appendChild(widget);
```

### Phase 4: Training Monitor Integration

**Future enhancement** - When user starts training job:
```typescript
// Open training monitor tab automatically
const sessionId = trainingResult.sessionId;
openContent(`/training/${sessionId}`);

// Monitor shows:
// - Real-time logs
// - Progress bar
// - Actual VRAM usage vs estimate
// - Performance metrics (tok/sec)
// - Cost tracker
```

---

## UI Mockup (Text-based)

```
â”Œâ”€ continuum â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“‚ JTAG v1.0                                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [General] [VRAM Calculator] [Training Monitor]  â† Tabs like VSCode  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  VRAM Calculator for LoRA Fine-Tuning                                â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€ Model Selection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Model: [DeepSeek-R1 1.5B â–¼]                                â”‚      â”‚
â”‚  â”‚ Parameters: 1.5B  |  Architecture: Transformer             â”‚      â”‚
â”‚  â”‚ Context Length: 64K                                        â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€ Training Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ LoRA Rank (r):     [16     ] (1-128)                       â”‚      â”‚
â”‚  â”‚ Batch Size:        [4      ] (1-128)                       â”‚      â”‚
â”‚  â”‚ Sequence Length:   [2048   ] (512-131072)                  â”‚      â”‚
â”‚  â”‚ Precision:         [4-bit â–¼] (4-bit, 8-bit, 16-bit, 32-bit)â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€ Hardware Selection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Platform: [Apple Silicon â–¼]                                â”‚      â”‚
â”‚  â”‚ Device:   [M2 Pro (16GB) â–¼]                                â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€ Optimizations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ [âœ“] Flash Attention (45% VRAM savings)                     â”‚      â”‚
â”‚  â”‚ [âœ“] Gradient Checkpointing (70% VRAM savings)              â”‚      â”‚
â”‚  â”‚ [âœ“] 8-bit Optimizer (75% VRAM savings)                     â”‚      â”‚
â”‚  â”‚ [ ] CPU Offloading (dynamic VRAM savings)                  â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€ Memory Estimate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Total VRAM: 5.75 GB / 16 GB (35.9%)                        â”‚      â”‚
â”‚  â”‚                                                             â”‚      â”‚
â”‚  â”‚ [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘]    â”‚      â”‚
â”‚  â”‚                                                             â”‚      â”‚
â”‚  â”‚ Breakdown:                                                  â”‚      â”‚
â”‚  â”‚   Base Model:   3.00 GB (52.1%)                            â”‚      â”‚
â”‚  â”‚   Activations:  1.41 GB (24.5%)                            â”‚      â”‚
â”‚  â”‚   Framework:    1.31 GB (22.7%)                            â”‚      â”‚
â”‚  â”‚   LoRA Weights: 0.04 GB (0.7%)                             â”‚      â”‚
â”‚  â”‚                                                             â”‚      â”‚
â”‚  â”‚ âœ… Training will fit in available VRAM                     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€ Performance Estimate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Training Speed:  ~18 tokens/sec                            â”‚      â”‚
â”‚  â”‚ Dataset Size:    10,000 examples (avg 512 tokens)          â”‚      â”‚
â”‚  â”‚ Estimated Time:  ~1.5 hours                                â”‚      â”‚
â”‚  â”‚ Cost (DeepSeek): $0.006 ($0.004/hour Ã— 1.5h)              â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                                       â”‚
â”‚  [Start Training]  [Export Config]  [Save Preset]                   â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Sources

### Model Database
**Source**: `system/genome/fine-tuning/docs/POPULAR-MODELS-BY-PROVIDER.md`
```typescript
const modelDatabase = {
  'deepseek-r1-1.5b': {
    name: 'DeepSeek-R1 1.5B',
    provider: 'deepseek',
    parameters: 1.5e9,
    contextLength: 64 * 1024,
    architecture: 'transformer',
    supportedPrecisions: ['4-bit', '8-bit', '16-bit'],
    // ... more metadata
  },
  // ... 100+ more models
};
```

### Hardware Database
```typescript
const hardwareDatabase = {
  appleSilicon: {
    'm2-pro-16gb': { vram: 16 * 1024, bandwidth: 200, tokensPerSec: 18 },
    'm3-max-96gb': { vram: 96 * 1024, bandwidth: 400, tokensPerSec: 45 },
    // ...
  },
  nvidia: {
    'rtx-4090': { vram: 24 * 1024, bandwidth: 1008, tokensPerSec: 120 },
    'h100-80gb': { vram: 80 * 1024, bandwidth: 3350, tokensPerSec: 500 },
    // ...
  }
};
```

### Provider Costs
**Source**: `daemons/ai-provider-daemon/adapters/*/shared/*BaseConfig.ts`
```typescript
// From OpenAIBaseConfig.ts
costPer1kTokens: { input: 0.003, output: 0.006 }

// From DeepSeekBaseConfig.ts
costPer1kTokens: { input: 0.00027, output: 0.00108 }

// Calculate training cost
const tokensProcessed = datasetSize * avgTokensPerExample * epochs;
const costPerToken = provider.costPer1kTokens.input / 1000;
const totalCost = tokensProcessed * costPerToken;
```

---

## Benefits

### For Users
1. **Plan before spending** - Know exact VRAM requirements before starting training
2. **Hardware recommendations** - Find cheapest hardware that fits their needs
3. **Cost estimation** - Budget for cloud training costs
4. **Optimization guidance** - Learn which toggles to enable

### For Platform
1. **Differentiation** - No other LoRA marketplace has integrated VRAM calculator
2. **Education** - Demystifies LoRA training for newcomers
3. **Trust** - Shows we understand the technical details
4. **Upsell** - When user sees "won't fit", suggest cloud providers we support

### For LoRA Marketplace
1. **Seller enablement** - Helps sellers plan their training infrastructure
2. **Buyer transparency** - Buyers can see training costs in listings
3. **Quality signal** - High VRAM = more compute = potentially better adapters
4. **Discovery** - "Models trainable on your hardware" filter

---

## Future Enhancements

### Phase 5: Training Presets
```typescript
const presets = {
  'apple-m2-budget': {
    precision: '4-bit',
    batchSize: 2,
    loraRank: 8,
    flashAttention: true,
    gradientCheckpointing: true
  },
  'nvidia-4090-fast': {
    precision: '16-bit',
    batchSize: 16,
    loraRank: 32,
    flashAttention: true,
    gradientCheckpointing: false
  }
};
```

### Phase 6: Real-time Monitoring
- **During training**: Show actual VRAM usage vs estimate
- **Accuracy tracking**: Improve calculator formulas based on real data
- **Warnings**: Alert if VRAM usage exceeds estimate

### Phase 7: Multi-GPU Support
- **Calculate sharding**: How to split model across multiple GPUs
- **Communication overhead**: Estimate inter-GPU bandwidth requirements
- **Cost optimization**: When is multi-GPU cheaper than single large GPU?

---

## Testing Strategy

### Unit Tests
```bash
npx vitest tests/unit/VramCalculator.test.ts
```
Test cases:
- Memory calculation accuracy (compare to apxml.com results)
- Optimization multipliers correct
- Feasibility checks work
- Recommendations are sensible

### Integration Tests
```bash
npx vitest tests/integration/vram-calculator-widget.test.ts
```
Test cases:
- Widget loads in tab
- Model dropdown populates from providers
- Hardware selection works
- Real-time estimate updates on input change

### Visual Regression Tests
```bash
./jtag interface/screenshot --querySelector="vram-calculator-widget" --filename="calculator-baseline.png"
```

---

## Status: Planning Phase

**Next steps**:
1. Get user confirmation on integration approach
2. Create widget file structure
3. Implement VramCalculatorLogic.ts with calculation formulas
4. Build VramCalculatorWidget.ts UI
5. Update ContentTypes.ts for 'tool' content type
6. Wire up content routing
7. Test with real provider data

**Decision needed**: Should we build this now or wait until after more providers are added?
