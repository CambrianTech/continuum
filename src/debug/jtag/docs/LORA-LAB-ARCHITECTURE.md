# LoRA Lab Architecture

**Status**: Backbone Complete - Moving to Persona Integration
**Goal**: Mass-market LoRA layer economy - cross-platform

---

## Current State (2026-01-06)

**Backbone validated across the full spectrum:**
- âœ… Local inference (Candle/Rust with LoRA weight merging)
- âœ… HuggingFace integration (download, search, metadata parsing)
- âœ… Genome stacking (multi-adapter composition)
- âœ… Local registry (`~/.continuum/adapters/installed/`)
- âœ… Provider abstraction (interface proven with Local + Together.ai stub)

**Next phase: Persona self-improvement**
- Personas search for adapters matching their task needs
- Personas try/evaluate/adopt adapters autonomously
- Persona config stores genome (adapter stack definition)

**Code locations:**
| Component | Path |
|-----------|------|
| Rust inference worker | `workers/inference-grpc/src/` |
| TypeScript gRPC client | `system/core/services/InferenceGrpcClient.ts` |
| Adapter search command | `commands/adapter/search/` |
| Provider abstraction | `system/adapters/` |
| Local registry | `~/.continuum/adapters/installed/` |

---

## Vision

A lab where users on **any hardware** can:
1. **Use** LoRA layers from others (download, plug in, go)
2. **Create** their own fine-tuned layers locally
3. **Share** layers with the community
4. **Compose** multiple layers for unique AI personalities

Think: **App Store for AI customizations** - but the "apps" are lightweight adapters that stack on base models.

### Platform Spectrum

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     LORA LAB PLATFORMS                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  FLOOR (Minimum)              CEILING (Maximum)                  â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚                                                                  â”‚
â”‚  M1 Mac (8GB)                 RTX 5090 (32GB VRAM)              â”‚
â”‚  â€¢ Inference only             â€¢ Full training                    â”‚
â”‚  â€¢ Small adapters             â€¢ Large models (70B+)              â”‚
â”‚  â€¢ Candle + Metal             â€¢ Candle + CUDA                    â”‚
â”‚                                                                  â”‚
â”‚  M1 Pro/Max (16-32GB)         Docker/Ubuntu + CUDA              â”‚
â”‚  â€¢ Local training             â€¢ Production inference             â”‚
â”‚  â€¢ 3B-7B models               â€¢ Multi-GPU training               â”‚
â”‚  â€¢ Candle + Metal             â€¢ Batch processing                 â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Insight**: Adapters are portable. Train on RTX 5090, deploy on M1 laptop.

---

## User Tiers

### Tier 1: Consumers (M1 8GB, entry laptops)
- Download and use adapters from others
- Swap between personalities/skills
- **Zero training required**
- Target: 80% of users

### Tier 2: Hobbyist Creators (M1 Pro 16GB+, gaming PCs)
- Train small adapters locally
- Share creations with community
- Experiment with composition
- Target: 15% of users

### Tier 3: Power Users (RTX 5090, cloud instances)
- Train large models (7B-70B)
- Create high-quality curated adapters
- Run production inference servers
- Train vision models
- Target: 5% of users

**The Economy**: Tier 3 creates â†’ Tier 2 remixes â†’ Tier 1 consumes

---

## User Scenarios

### Scenario A: Consumer (Inference Only)

```
User downloads adapter from Hub
         â†“
    Base Model (Llama 3.2 3B)
         +
    LoRA Adapter (2-50 MB)
         â†“
    Customized AI
```

**Requirements:**
- 8GB unified memory minimum
- No training needed
- ~30 second adapter load time
- Hot-swap between adapters

**Example workflow:**
```bash
# Browse available adapters
./jtag adapter/search --query="typescript expert"

# Download and activate
./jtag adapter/install --id="community/typescript-guru-v2"
./jtag adapter/activate --id="typescript-guru-v2"

# Use immediately
./jtag ai/generate --prompt="Review this TypeScript code..."
```

### Scenario B: Creator - Mac (Training + Inference)

```
User's training data (corrections, examples)
         â†“
    Candle LoRA Training (Metal backend)
         â†“
    Custom Adapter (.safetensors)
         â†“
    Publish to Hub (optional)
```

**Requirements:**
- 16GB+ unified memory recommended
- 10-60 minutes training time (500-2000 examples)
- Local dataset preparation tools

**Example workflow:**
```bash
# Prepare training data from chat corrections
./jtag training/prepare \
  --source="corrections" \
  --persona="helper-ai" \
  --output="./datasets/my-style.jsonl"

# Train locally (Rust/Candle with Metal)
./jtag training/start \
  --base="unsloth/Llama-3.2-3B-Instruct" \
  --data="./datasets/my-style.jsonl" \
  --output="./adapters/my-style-v1.safetensors" \
  --epochs=3

# Test locally
./jtag adapter/activate --path="./adapters/my-style-v1.safetensors"

# Share with community (optional)
./jtag adapter/publish \
  --path="./adapters/my-style-v1.safetensors" \
  --name="my-writing-style" \
  --description="Trained on my chat corrections"
```

### Scenario C: Power User - Docker/CUDA (RTX 5090)

```
Large datasets + powerful GPU
         â†“
    Candle LoRA Training (CUDA backend)
         â†“
    High-quality Adapter (.safetensors)
         â†“
    Publish to Hub (curated tier)
```

**Requirements:**
- RTX 5090 (32GB VRAM) or multi-GPU setup
- Docker with NVIDIA Container Toolkit
- Ubuntu 22.04+ or WSL2

**Example workflow:**
```bash
# Train large model (Rust/Candle with CUDA)
./jtag training/start \
  --backend="cuda" \
  --base="Qwen/Qwen2.5-7B-Instruct" \
  --data="./datasets/expert-corrections.jsonl" \
  --output="./adapters/expert-coder-v1.safetensors" \
  --epochs=5 \
  --lora-rank=64 \
  --batch-size=8

# Train vision model for UI critique
./jtag training/vision/start \
  --backend="cuda" \
  --base="Qwen/Qwen2.5-VL-7B-Instruct" \
  --data="./datasets/ui-critique.jsonl" \
  --output="./adapters/ui-expert-v1.safetensors" \
  --epochs=3

# Batch inference for production
./jtag inference/batch \
  --adapter="./adapters/expert-coder-v1.safetensors" \
  --input="./requests.jsonl" \
  --output="./responses.jsonl" \
  --batch-size=32
```

**Power user advantages:**
- 10-50x faster training than M1
- Larger models (7B-70B)
- Vision model training
- Batch processing for production
- Multi-GPU parallelism

### Scenario D: Hybrid - API Key Acceleration

```
User has API key (Together, Fireworks, Modal)
         â†“
    Upload dataset (encrypted)
         â†“
    Cloud Training (minutes, not hours)
         â†“
    Download adapter locally
         â†“
    Run on any hardware
```

**For users who want:**
- Faster training without local GPU
- Access to larger models (70B+)
- Privacy-preserving (data deleted after training)
- Pay-per-use flexibility

**Example workflow:**
```bash
# Configure API key
./jtag config/set --key="TOGETHER_API_KEY" --value="tok_..."

# Offload training to cloud
./jtag training/offload \
  --provider="together" \
  --model="Qwen/Qwen2.5-72B-Instruct" \
  --data="./datasets/expert-level.jsonl" \
  --output="./adapters/cloud-trained.safetensors" \
  --privacy="delete-after-training"

# Download and use locally (runs on M1!)
./jtag adapter/activate --path="./adapters/cloud-trained.safetensors"
```

**Key insight**: Train on 72B model in cloud, deploy adapter on 3B model locally. The adapter captures the *knowledge*, base model provides the *capability*.

---

## Practical Model Tiers (All Free, All Local)

| Tier | Model | Memory | Training | Best For |
|------|-------|--------|----------|----------|
| **Tiny** | Moondream 1.6B | 4GB | 5-10 min | Vision Q&A, quick tests |
| **Small** | Llama 3.2 3B | 8GB | 15-30 min | Chat, coding, general |
| **Medium** | Qwen2.5 7B | 16GB | 30-60 min | Complex reasoning |
| **Vision** | Qwen2.5-VL 3B | 12GB | 20-40 min | UI, screenshots |
| **Large** | Llama 3.3 70B | 48GB+ | 2-4 hrs | Expert systems (RTX 5090) |

**Note:** All models run 100% locally. Training times for ~500 examples with LoRA rank 32.

**Free Tools Used:**
- Candle (Rust) - Inference AND training, MIT license
- candle-lora - LoRA layer swapping for Candle
- Metal backend - Native Apple Silicon acceleration
- CUDA backend - NVIDIA GPU acceleration
- HuggingFace Hub - Free model downloads

**No Python required.** Everything runs in Rust.

---

## Local Storage Strategy

**Think Docker images**: Each adapter is like a Docker image with its own internal layers.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ADAPTER: typescript-expert-v3                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Internal Layers (LoRA weight matrices per model layer):       â”‚  â”‚
â”‚  â”‚    â€¢ layers.0.self_attn.q_proj  (A: [32,4096], B: [4096,32])  â”‚  â”‚
â”‚  â”‚    â€¢ layers.0.self_attn.v_proj  (A: [32,4096], B: [4096,32])  â”‚  â”‚
â”‚  â”‚    â€¢ layers.1.self_attn.q_proj  ...                            â”‚  â”‚
â”‚  â”‚    â€¢ layers.1.self_attn.v_proj  ...                            â”‚  â”‚
â”‚  â”‚    â€¢ ... (typically 64-128 layer pairs)                        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  manifest.json: { base: "Llama-3.2-3B", rank: 32, scale: 1.0 }      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Composition** = stacking multiple adapters (like Docker multi-stage builds):
```
Your AI = Base Model + Adapter1(scale=1.0) + Adapter2(scale=0.8) + ...
W' = W + scaleâ‚(Bâ‚Aâ‚) + scaleâ‚‚(Bâ‚‚Aâ‚‚) + ...
```

**Key insight**: Like Docker images, adapters are immutable, versioned, and composable. Each adapter is a complete package you can pull, push, and stack.

Adapters are stored locally in `~/.continuum/adapters/` with a consistent structure for discovery, loading, and packaging.

### Directory Layout

```
~/.continuum/
â”œâ”€â”€ adapters/
â”‚   â”œâ”€â”€ installed/                   # Downloaded/installed adapters
â”‚   â”‚   â”œâ”€â”€ typescript-expert-v3/
â”‚   â”‚   â”‚   â”œâ”€â”€ adapter.safetensors  # LoRA weights (2-50 MB)
â”‚   â”‚   â”‚   â”œâ”€â”€ manifest.json        # Metadata, version, compatibility
â”‚   â”‚   â”‚   â””â”€â”€ README.md            # Usage guide
â”‚   â”‚   â””â”€â”€ code-reviewer-v2/
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                    # In-progress training outputs
â”‚   â”‚   â””â”€â”€ my-style-2024-01/
â”‚   â”‚       â”œâ”€â”€ checkpoint-500/
â”‚   â”‚       â”œâ”€â”€ checkpoint-1000/
â”‚   â”‚       â””â”€â”€ training.log
â”‚   â”‚
â”‚   â””â”€â”€ personal/                    # User-created, local-only
â”‚       â””â”€â”€ my-writing-style/
â”‚           â”œâ”€â”€ adapter.safetensors
â”‚           â”œâ”€â”€ manifest.json
â”‚           â””â”€â”€ training-data.jsonl  # Optional: keep training data
â”‚
â”œâ”€â”€ models/                          # Base model cache (HuggingFace)
â”‚   â”œâ”€â”€ unsloth--Llama-3.2-3B-Instruct/
â”‚   â””â”€â”€ Qwen--Qwen2.5-VL-3B-Instruct/
â”‚
â””â”€â”€ config.json                      # User settings, API keys
```

### Key Design Decisions

1. **Single safetensor file per adapter**: `adapter.safetensors` contains all LoRA A/B matrices
2. **Manifest is required**: Every adapter has `manifest.json` with version, base model, compatibility
3. **Training checkpoints preserved**: Can resume training or revert to earlier versions
4. **HuggingFace cache symlinks**: Models stored in standard HF cache, symlinked for organization
5. **Portable**: Entire `adapters/` directory can be copied/backed up

### Discovery

```bash
# List all installed adapters
./jtag adapter/list

# Load by ID (looks in installed/ and personal/)
./jtag adapter/activate --id="typescript-expert-v3"

# Load by path (any location)
./jtag adapter/activate --path="./custom/my-adapter.safetensors"
```

---

## Adapter Hub Structure (Remote)

For sharing adapters publicly:

```
continuum-hub/
â”œâ”€â”€ adapters/
â”‚   â”œâ”€â”€ official/                    # Curated, tested
â”‚   â”‚   â”œâ”€â”€ typescript-expert-v3/
â”‚   â”‚   â”‚   â”œâ”€â”€ adapter.safetensors  # 15 MB
â”‚   â”‚   â”‚   â”œâ”€â”€ manifest.json        # Metadata
â”‚   â”‚   â”‚   â””â”€â”€ README.md            # Usage guide
â”‚   â”‚   â”œâ”€â”€ code-reviewer-v2/
â”‚   â”‚   â””â”€â”€ ui-critique-v1/
â”‚   â”‚
â”‚   â”œâ”€â”€ community/                   # User-contributed
â”‚   â”‚   â”œâ”€â”€ wine-sommelier/
â”‚   â”‚   â”œâ”€â”€ legal-assistant/
â”‚   â”‚   â””â”€â”€ game-master-dnd/
â”‚   â”‚
â”‚   â””â”€â”€ personal/                    # Private (local only)
â”‚       â””â”€â”€ my-writing-style/
â”‚
â””â”€â”€ models/                          # Base models (optional cache)
    â”œâ”€â”€ llama-3.2-3b-instruct-4bit/
    â””â”€â”€ qwen2.5-vl-3b-instruct-4bit/
```

### Manifest Schema

```json
{
  "id": "typescript-expert-v3",
  "name": "TypeScript Expert",
  "version": "3.0.0",
  "description": "Deep TypeScript knowledge with strict typing focus",
  "author": "continuum-team",
  "license": "MIT",

  "base_model": "unsloth/Llama-3.2-3B-Instruct",
  "adapter_type": "lora",
  "lora_rank": 32,
  "lora_alpha": 64,

  "training": {
    "examples": 2500,
    "epochs": 3,
    "dataset_source": "typescript-corrections-2024"
  },

  "compatibility": {
    "min_memory_gb": 8,
    "platforms": ["darwin-arm64", "linux-x86_64", "windows-x86_64"],
    "inference_backends": ["candle-metal", "candle-cuda", "ollama"]
  },

  "metrics": {
    "downloads": 1234,
    "rating": 4.7,
    "verified": true
  },

  "files": {
    "adapter": "adapter.safetensors",
    "size_mb": 15.2
  }
}
```

---

## Federated Adapter Exchange

Our own adapter exchange infrastructure that we control, with HuggingFace as one of many source endpoints. Think: **npm registry** but for LoRA adapters.

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     FEDERATED ADAPTER EXCHANGE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   LOCAL NODE    â”‚â”€â”€â”€â”€â–¶â”‚  CENTRAL MESH   â”‚â—€â”€â”€â”€â”€â”‚   LOCAL NODE    â”‚   â”‚
â”‚  â”‚   (Your Mac)    â”‚     â”‚  (Our servers)  â”‚     â”‚  (Other users)  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â”‚                       â”‚                                     â”‚
â”‚           â”‚                       â–¼                                     â”‚
â”‚           â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚
â”‚           â”‚              â”‚ EXTERNAL SOURCES â”‚                           â”‚
â”‚           â”‚              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤                           â”‚
â”‚           â”‚              â”‚ â€¢ HuggingFace   â”‚â—€â”€â”€ Primary public source  â”‚
â”‚           â”‚              â”‚ â€¢ CivitAI       â”‚â—€â”€â”€ Diffusion models       â”‚
â”‚           â”‚              â”‚ â€¢ Custom APIs   â”‚â—€â”€â”€ Enterprise sources     â”‚
â”‚           â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚
â”‚           â”‚                                                             â”‚
â”‚           â–¼                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  LOCAL CACHE (~/.continuum/adapters/)                           â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ installed/     â”€ Downloaded adapters                       â”‚   â”‚
â”‚  â”‚  â”œâ”€â”€ personal/      â”€ User-created adapters                     â”‚   â”‚
â”‚  â”‚  â””â”€â”€ registry.json  â”€ Local index with sources                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Source Protocol

Each source (HuggingFace, CivitAI, our mesh, custom) implements a standard interface:

```typescript
interface AdapterSource {
  id: string;                    // "huggingface", "civitai", "continuum-mesh"
  name: string;                  // Human-readable name
  priority: number;              // Resolution order (lower = first)

  // Discovery
  search(query: string): Promise<AdapterSearchResult[]>;
  list(filter?: AdapterFilter): Promise<AdapterSummary[]>;

  // Fetch
  download(adapterId: string): Promise<DownloadedAdapter>;
  getMetadata(adapterId: string): Promise<AdapterMetadata>;

  // Publish (if supported)
  canPublish: boolean;
  publish?(adapter: LocalAdapter): Promise<PublishResult>;
}

// Registry manages multiple sources
interface AdapterRegistry {
  sources: AdapterSource[];

  // Unified search across all sources
  search(query: string): Promise<AdapterSearchResult[]>;

  // Download with source preference
  download(adapterId: string, preferredSource?: string): Promise<DownloadedAdapter>;

  // Publish to writable source
  publish(adapter: LocalAdapter, targetSource: string): Promise<PublishResult>;
}
```

### Source Implementations

**1. HuggingFace Source** (read-only for now):
```typescript
class HuggingFaceSource implements AdapterSource {
  id = 'huggingface';
  canPublish = false;  // Future: HF Hub API upload

  async download(repoId: string) {
    // Uses hf-hub crate via gRPC DownloadAdapter
    return await inferenceClient.downloadAdapter(repoId);
  }

  async search(query: string) {
    // HuggingFace API: https://huggingface.co/api/models?search=...&filter=lora
    return await this.searchHfApi(query, { filter: 'lora' });
  }
}
```

**2. Continuum Mesh Source** (our infrastructure):
```typescript
class ContinuumMeshSource implements AdapterSource {
  id = 'continuum-mesh';
  canPublish = true;

  async download(adapterId: string) {
    // Pull from our CDN/mesh network
    const url = `https://adapters.continuum.ai/${adapterId}`;
    return await this.fetchAndCache(url);
  }

  async publish(adapter: LocalAdapter) {
    // Upload to our registry
    const formData = new FormData();
    formData.append('weights', adapter.weightsFile);
    formData.append('manifest', JSON.stringify(adapter.manifest));
    return await fetch('https://api.continuum.ai/adapters', {
      method: 'POST',
      body: formData,
      headers: { 'Authorization': `Bearer ${this.apiKey}` }
    });
  }
}
```

**3. Local Source** (always available):
```typescript
class LocalSource implements AdapterSource {
  id = 'local';
  canPublish = true;  // Save to disk

  async download(adapterId: string) {
    // Already local, just return path
    return this.getLocalAdapter(adapterId);
  }

  async publish(adapter: LocalAdapter) {
    // Save to ~/.continuum/adapters/personal/
    await this.saveToPersonal(adapter);
  }
}
```

### Resolution Strategy

When requesting an adapter by ID, the registry resolves in order:

```
1. LOCAL:    Check ~/.continuum/adapters/ first (instant)
2. MESH:     Check Continuum mesh network (fast, curated)
3. EXTERNAL: Fall back to external sources (HuggingFace, etc.)
```

```typescript
async function resolveAdapter(adapterId: string): Promise<DownloadedAdapter> {
  // Normalize ID: "typescript-expert" or "huggingface:user/repo"
  const [source, id] = parseAdapterId(adapterId);

  if (source) {
    // Explicit source: go directly there
    return await registry.sources[source].download(id);
  }

  // Implicit: check in priority order
  for (const src of registry.sources.sort(byPriority)) {
    try {
      const result = await src.download(adapterId);
      if (result) return result;
    } catch { continue; }
  }

  throw new Error(`Adapter ${adapterId} not found in any source`);
}
```

### Adapter ID Formats

```
# Local adapters (no prefix)
typescript-expert-v3
my-writing-style

# Source-qualified (explicit source)
huggingface:Jiten1024/llama-3.2-3b-int-finetune
civitai:12345
mesh:continuum/official-code-reviewer

# Genome references
@persona/helper-ai/genome    # Full genome from persona
```

### Trading/Sharing Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ADAPTER TRADING FLOW                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  CREATOR (has RTX 5090)                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚
â”‚  1. Trains high-quality adapter locally                         â”‚
â”‚  2. Tests and validates quality                                 â”‚
â”‚  3. Publishes to Continuum mesh:                                â”‚
â”‚     ./jtag adapter/publish --id="my-ts-expert" --target="mesh"  â”‚
â”‚  4. Adapter indexed, available to mesh users                    â”‚
â”‚                                                                  â”‚
â”‚  CONSUMER (has M1 Mac)                                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚
â”‚  1. Searches mesh for TypeScript adapters:                      â”‚
â”‚     ./jtag adapter/search --query="typescript"                  â”‚
â”‚  2. Sees "my-ts-expert" by creator, downloads:                  â”‚
â”‚     ./jtag adapter/install --id="mesh:creator/my-ts-expert"     â”‚
â”‚  3. Uses locally without creator's GPU                          â”‚
â”‚                                                                  â”‚
â”‚  REMIXER (has gaming PC)                                        â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â”‚
â”‚  1. Downloads "my-ts-expert" from mesh                          â”‚
â”‚  2. Fine-tunes further on own examples                          â”‚
â”‚  3. Publishes remix: "my-ts-expert-extended"                    â”‚
â”‚  4. Credits original in manifest.parentAdapters[]               â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Commands (Future)

```bash
# Configure sources
./jtag registry/sources/add --url="https://custom.company.com/adapters"
./jtag registry/sources/list
./jtag registry/sources/remove --id="custom-company"

# Search across all sources
./jtag adapter/search --query="typescript expert"
./jtag adapter/search --query="code review" --source="mesh"

# Install from any source
./jtag adapter/install --id="mesh:continuum/code-reviewer-v3"
./jtag adapter/install --id="huggingface:user/adapter-name"
./jtag adapter/install --id="local-adapter-name"

# Publish to mesh
./jtag adapter/publish --id="my-adapter" --target="mesh"
./jtag adapter/publish --id="my-adapter" --target="huggingface"  # Future
```

### Semantic Search Across Registries

A key differentiator: **unified semantic search** across all adapter sources.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  SEMANTIC ADAPTER SEARCH                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  User query: "TypeScript strict typing expert"                  â”‚
â”‚                         â”‚                                        â”‚
â”‚                         â–¼                                        â”‚
â”‚                 embed(query) â†’ [0.8, 0.3, ...]                  â”‚
â”‚                         â”‚                                        â”‚
â”‚          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚          â–¼              â–¼              â–¼                        â”‚
â”‚      LOCAL          MESH         HUGGINGFACE                    â”‚
â”‚      INDEX          INDEX        INDEX                          â”‚
â”‚                                                                  â”‚
â”‚      Results:       Results:     Results:                       â”‚
â”‚      my-ts:0.92     ts-guru:0.89 peft-ts:0.85                  â”‚
â”‚      old-ts:0.71    code-rev:0.67 lora-js:0.62                 â”‚
â”‚                                                                  â”‚
â”‚                         â”‚                                        â”‚
â”‚                         â–¼                                        â”‚
â”‚                 MERGED RANKING                                   â”‚
â”‚         1. my-ts (local, 0.92)                                  â”‚
â”‚         2. ts-guru (mesh, 0.89)                                 â”‚
â”‚         3. peft-ts (huggingface, 0.85)                          â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation**:

```typescript
interface AdapterSearchIndex {
  // Per-adapter embedding (from description or training data)
  embeddings: Map<string, number[]>;

  // Search with cosine similarity
  search(queryEmbedding: number[], topK: number): SearchResult[];

  // Index new adapter
  index(adapterId: string, description: string): Promise<void>;
}

// Unified search across all sources
async function searchAdapters(query: string): Promise<SearchResult[]> {
  const queryEmb = await embed(query);

  // Search each source's index
  const results = await Promise.all([
    localIndex.search(queryEmb, 10),
    meshIndex.search(queryEmb, 10),
    huggingfaceIndex.search(queryEmb, 10),
  ]);

  // Merge and deduplicate by adapterId
  return mergeAndRank(results);
}
```

**Why semantic search matters:**
- Find adapters by capability, not just name
- Discover related adapters automatically
- Cross-source discovery (find HF adapter that matches local needs)
- Foundation for recommendation engine

### Why Our Own Exchange?

1. **Protocol Control**: We define the adapter manifest format, search protocol, and federation rules
2. **Semantic Search**: Unified embedding-based search across ALL sources
3. **Speed**: Local mesh is faster than HuggingFace API
4. **Curation**: Can curate/verify high-quality adapters
5. **Federation**: Can add more sources later (CivitAI, custom enterprise)
6. **Monetization**: Future path for premium adapters if desired
7. **Privacy**: Can offer private mesh nodes for enterprises

### Bootstrap Strategy

```
Phase 1 (Now):     HuggingFace as primary external source
                   Local cache for downloaded adapters

Phase 2 (Soon):    Add Continuum mesh as curated source
                   Seed with high-quality official adapters

Phase 3 (Later):   Open mesh publishing for verified users
                   Add more external sources (CivitAI, etc.)

Phase 4 (Future):  Enterprise private mesh nodes
                   Adapter marketplace/trading features
```

---

## Persona = Base Model + Genome + Databases

A **Persona** is the complete package - not just adapters:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PERSONA PACKAGE                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  BASE MODEL (immutable, shared)                                         â”‚
â”‚  â””â”€â”€ unsloth/Llama-3.2-3B-Instruct                                      â”‚
â”‚                                                                          â”‚
â”‚  LORA GENOME (0-N layers, shareable)                                    â”‚
â”‚  â”œâ”€â”€ Layer 0: code-style:v2 (scale: 1.0)                                â”‚
â”‚  â”œâ”€â”€ Layer 1: typescript-expert:v3 (scale: 0.8)                         â”‚
â”‚  â””â”€â”€ Layer 2: my-personality:v1 (scale: 0.5)                            â”‚
â”‚                                                                          â”‚
â”‚  DATABASES (per-persona, selective sharing)                             â”‚
â”‚  â”œâ”€â”€ ltm.db              [PRIVATE]   Long-term memories                 â”‚
â”‚  â”œâ”€â”€ corrections.db      [SHAREABLE] Training examples                  â”‚
â”‚  â”œâ”€â”€ preferences.db      [PRIVATE]   User preferences                   â”‚
â”‚  â”œâ”€â”€ skills.db           [SHAREABLE] Skill inventory                    â”‚
â”‚  â””â”€â”€ hippocampus.db      [PRIVATE]   Episodic memory                    â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Sharing Granularity

Three levels of sharing - like Docker images and containers:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SHARING GRANULARITY                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                â”‚
â”‚  1. INDIVIDUAL LAYER (finest grain)                            â”‚
â”‚     â””â”€â”€ Just one adapter: typescript-expert.safetensors        â”‚
â”‚         Size: 2-50 MB                                          â”‚
â”‚         Use: Share a specific skill                            â”‚
â”‚                                                                â”‚
â”‚  2. MEMORY-LESS PERSONA (genome only)                          â”‚
â”‚     â””â”€â”€ Stack config + adapter refs (no DBs)                   â”‚
â”‚         {                                                      â”‚
â”‚           base: "Llama-3.2-3B",                                â”‚
â”‚           layers: ["code-style:1.0", "ts-expert:0.8"]          â”‚
â”‚         }                                                      â”‚
â”‚         Size: ~1 KB (just JSON, adapters pulled lazily)        â”‚
â”‚         Use: Share personality without memories                â”‚
â”‚                                                                â”‚
â”‚  3. WHOLE PERSONA (genome + memories)                          â”‚
â”‚     â””â”€â”€ Full package with selected DBs                         â”‚
â”‚         â”œâ”€â”€ genome.json                                        â”‚
â”‚         â”œâ”€â”€ corrections.db    (training examples)              â”‚
â”‚         â”œâ”€â”€ skills.db         (capabilities)                   â”‚
â”‚         â””â”€â”€ ltm.db            (optional - user choice)         â”‚
â”‚         Size: 10-500 MB depending on memory depth              â”‚
â”‚         Use: Clone a fully-trained expert                      â”‚
â”‚                                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Commands:**
```bash
# Share just a layer
./jtag adapter/push typescript-expert:v3

# Share memory-less persona (just the recipe)
./jtag persona/push helper-ai --no-memory

# Share whole persona with memories
./jtag persona/push helper-ai --with-memory
```

---

## Dynamic Genome Routing (Semantic MoE)

Layer scales aren't static - they're **dynamically computed** based on input using embedding similarity. Like a soft Mixture of Experts routing:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DYNAMIC GENOME ROUTING                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  INPUT: "Review this TypeScript PR for security issues"         â”‚
â”‚                         â”‚                                        â”‚
â”‚                         â–¼                                        â”‚
â”‚                    embed(input)                                  â”‚
â”‚                         â”‚                                        â”‚
â”‚                         â–¼                                        â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚         â”‚  COSINE SIMILARITY vs ADAPTERS    â”‚                   â”‚
â”‚         â”‚                                   â”‚                   â”‚
â”‚         â”‚  typescript-expert:  0.82 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘                  â”‚
â”‚         â”‚  security-reviewer:  0.94 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘                  â”‚
â”‚         â”‚  code-style:         0.31 â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘                  â”‚
â”‚         â”‚  writing-clarity:    0.15 â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘                  â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                         â”‚                                        â”‚
â”‚                         â–¼                                        â”‚
â”‚              W' = W + 0.82(Bâ‚Aâ‚)    â† typescript                â”‚
â”‚                      + 0.94(Bâ‚‚Aâ‚‚)    â† security (dominant)      â”‚
â”‚                      + 0.31(Bâ‚ƒAâ‚ƒ)    â† code-style (reduced)     â”‚
â”‚                      + 0.15(Bâ‚„Aâ‚„)    â† writing (minimal)        â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Each adapter has a domain embedding** - a vector representing its competence space:

```typescript
interface AdapterEntity extends BaseEntity {
  adapterId: string;
  digest: string;

  // Domain embedding for MoE routing
  domainEmbedding: {
    learned?: number[];        // From usage feedback (best)
    training?: number[];       // From training data centroid (good)
    description: number[];     // From text description (fallback)
  };

  domainDescription: string;   // "TypeScript type safety and patterns"
}
```

**Embedding priority** (use best available):
1. **Learned** - accumulated from actual usage feedback
2. **Training** - centroid of training examples
3. **Description** - embed the text description (cold start)

---

## Learned Domain Embeddings

Adapters learn their own competence space from usage:

```
CONTINUOUS LEARNING
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

User: "Help with this TypeScript generic"
      â†’ typescript-expert activated at 0.9
      â†’ Good response, user thumbs up ğŸ‘
      â†’ Move typescript-expert embedding TOWARD this input

User: "Write a poem about code"
      â†’ typescript-expert activated at 0.4
      â†’ User corrects: "too technical"
      â†’ Move typescript-expert embedding AWAY from this input

RESULT: Adapter naturally specializes based on where it actually helps
```

```typescript
async function updateDomainEmbedding(
  adapter: AdapterEntity,
  input: string,
  helpful: boolean
): Promise<void> {
  const inputEmb = await embed(input);
  const current = adapter.domainEmbedding.learned
    ?? adapter.domainEmbedding.training
    ?? adapter.domainEmbedding.description;

  // Online update: move toward helpful, away from unhelpful
  const alpha = 0.1;
  const direction = helpful ? 1 : -1;

  adapter.domainEmbedding.learned = current.map((v, i) =>
    v + alpha * direction * (inputEmb[i] - v)
  );
}
```

---

## Adapter Inheritance (Transfer Learning)

Training embeddings form a **searchable competence space**. When creating a new adapter, find similar existing ones as starting points:

```
ADAPTER DISCOVERY / INHERITANCE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

WANT: "Legal contract clause reviewer"
                â”‚
                â–¼
         embed(description)
                â”‚
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SEARCH EXISTING TRAINING SPACES                          â”‚
â”‚                                                           â”‚
â”‚  code-reviewer     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘  0.65  (structured review)  â”‚
â”‚  writing-style     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘  0.52  (prose clarity)      â”‚
â”‚  security-expert   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘  0.48  (risk analysis)      â”‚
â”‚  typescript-expert â–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  0.21  (wrong domain)       â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
         STARTING POINTS:
         - Initialize from code-reviewer weights (closest)
         - Blend in writing-style at 0.5
         - Fine-tune on legal corpus

         â†’ Faster convergence, better results
```

```typescript
// Find nearest adapters for transfer learning
async function findStartingPoints(
  desiredTraits: string,
  topK: number = 3
): Promise<Array<{ adapter: AdapterEntity; similarity: number }>> {
  const targetEmb = await embed(desiredTraits);

  const scored = adapters.map(a => ({
    adapter: a,
    similarity: cosineSimilarity(targetEmb, a.domainEmbedding.training)
  }));

  return scored
    .sort((a, b) => b.similarity - a.similarity)
    .slice(0, topK);
}

// Create new adapter with informed initialization
async function createAdapter(params: {
  description: string;
  trainingData: string;
}): Promise<AdapterEntity> {
  const ancestors = await findStartingPoints(params.description);

  // Warm start from best match if similarity > threshold
  const initWeights = ancestors[0].similarity > 0.5
    ? await loadWeights(ancestors[0].adapter)
    : null;

  return train({
    data: params.trainingData,
    initFrom: initWeights,
  });
}
```

**Adapters form a family tree** - new ones inherit from semantically similar ancestors. Never training blind.

---

## Persona Lab UI (Widget)

Visual genome editor from the persona page:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PERSONA: helper-ai                                        [Save] [Test]â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  BASE MODEL: unsloth/Llama-3.2-3B-Instruct              [Change]        â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  GENOME STACK                                      [+ Add Layer] â”‚   â”‚
â”‚  â”‚                                                                   â”‚   â”‚
â”‚  â”‚  â˜° Layer 2: my-personality      â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘ 0.5  [â†‘] [â†“] [Ã—]     â”‚   â”‚
â”‚  â”‚  â˜° Layer 1: typescript-expert   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘ 0.8  [â†‘] [â†“] [Ã—]     â”‚   â”‚
â”‚  â”‚  â˜° Layer 0: code-style          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1.0  [â†‘] [â†“] [Ã—]     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  DYNAMIC ROUTING (MoE)                              [Enabled âœ“]  â”‚   â”‚
â”‚  â”‚  Threshold: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.3   (min similarity to activate)  â”‚   â”‚
â”‚  â”‚  Dampening: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘ 0.2   (reduce low-score scales)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  AVAILABLE ADAPTERS  â”‚  â”‚  TEST AREA                           â”‚   â”‚
â”‚  â”‚  â–¡ security-expert   â”‚  â”‚  [Chat] [Benchmark] [Canvas] [Game]  â”‚   â”‚
â”‚  â”‚  â–¡ writing-clarity   â”‚  â”‚                                      â”‚   â”‚
â”‚  â”‚  â–¡ legal-reviewer    â”‚  â”‚  > You: Review this code...          â”‚   â”‚
â”‚  â”‚  [Browse Registry]   â”‚  â”‚  > AI: I notice a potential XSS...   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  Routing: ts:0.82 sec:0.91 style:0.3 â”‚   â”‚
â”‚                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Features:**
- Drag-drop genome layers, adjust scales with sliders
- Live routing visualization (see which adapters activate per input)
- Inline testing: chat, benchmarks, canvas, games
- A/B compare different genome configs
- Import/Export genome configurations

---

## Persona Genome Autonomy

Should personas control their own genome?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     GENOME ACCESS MODELS                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                          â”‚
â”‚  MODEL A: User-Controlled (default)                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â”‚
â”‚  â€¢ User edits genome via Persona Lab UI                                 â”‚
â”‚  â€¢ Persona sees current genome, cannot modify                           â”‚
â”‚  â€¢ Safe, predictable, explicit control                                  â”‚
â”‚                                                                          â”‚
â”‚  MODEL B: Curated Access                                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â”‚
â”‚  â€¢ User whitelists adapters per persona                                 â”‚
â”‚  â€¢ Persona can pick from allowed set                                    â”‚
â”‚  â€¢ "You can use these 5 adapters, choose what fits"                     â”‚
â”‚                                                                          â”‚
â”‚  MODEL C: Full Autonomy                                                 â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚
â”‚  â€¢ Persona has access to entire registry                                â”‚
â”‚  â€¢ Can add/remove/adjust layers based on task performance               â”‚
â”‚  â€¢ Self-evolving genome (continuous learning)                           â”‚
â”‚  â€¢ Requires trust, may diverge unexpectedly                             â”‚
â”‚                                                                          â”‚
â”‚  MODEL D: Supervised Evolution                                          â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚
â”‚  â€¢ Persona proposes genome changes                                      â”‚
â”‚  â€¢ User approves/rejects via notification                               â”‚
â”‚  â€¢ "I think adding security-expert would help. Allow? [Y/N]"            â”‚
â”‚  â€¢ Best of both: autonomy with oversight                                â”‚
â”‚                                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Implementation:**

```typescript
interface GenomePermissions {
  mode: 'user-controlled' | 'curated' | 'autonomous' | 'supervised';

  // For 'curated' mode
  allowedAdapters?: string[];

  // For 'autonomous' and 'supervised' modes
  canAddLayers: boolean;
  canRemoveLayers: boolean;
  canAdjustScales: boolean;
  maxLayers: number;

  // For 'supervised' mode
  requireApproval: boolean;
}

// Persona can request genome changes
interface GenomeChangeRequest {
  personaId: string;
  action: 'add' | 'remove' | 'adjust';
  adapterId: string;
  reason: string;           // "This task involves security review"
  proposedScale?: number;
  status: 'pending' | 'approved' | 'rejected';
}
```

**Supervised Evolution** is probably the sweet spot - personas can learn and propose improvements, but user stays in control. Could even auto-approve low-risk changes (scale adjustments) while requiring approval for adding new layers.

---

## Entity Definitions

Core entities for the adapter/persona registry:

```typescript
// AdapterEntity - Single LoRA layer (content-addressable)
interface AdapterEntity extends BaseEntity {
  // Identity
  adapterId: string;           // "typescript-expert-v3"
  namespace: string;           // "official" | "community" | "personal"
  version: string;             // semver: "3.0.0"
  digest: string;              // SHA256 of safetensors (immutable ref)

  // Base model dependency
  baseModelId: string;         // "unsloth/Llama-3.2-3B-Instruct"

  // LoRA config
  loraRank: number;            // 8, 16, 32, 64
  loraAlpha: number;
  targetModules: string[];     // ["q_proj", "v_proj", ...]

  // Domain embedding for MoE routing
  domainEmbedding: {
    learned?: number[];        // From usage feedback
    training?: number[];       // From training data centroid
    description: number[];     // From text description
  };
  domainDescription: string;

  // Training provenance
  trainingExamples: number;
  trainingSource: 'corrections' | 'dataset' | 'synthetic';
  ancestorAdapters?: string[]; // Transfer learning lineage

  // Metadata
  author: string;
  license: string;
  sizeBytes: number;
  filePath: string;
}

// GenomeEntity - Stack configuration (tiny, just references)
interface GenomeEntity extends BaseEntity {
  genomeId: string;
  name: string;
  baseModelId: string;

  layers: Array<{
    order: number;             // Stack order (0 = bottom)
    adapterId: string;
    adapterDigest: string;     // Pinned version
    defaultScale: number;      // Static fallback
    domain: string;            // For dynamic routing
  }>;

  // Routing config
  dynamicRouting: boolean;     // Use semantic MoE?
  routingThreshold: number;    // Min similarity to activate
}

// PersonaPackageEntity - Full export manifest
interface PersonaPackageEntity extends BaseEntity {
  personaId: string;
  name: string;
  description: string;

  // Components
  genomeId: string;
  baseModelId: string;

  // Included databases
  includedDatabases: Array<{
    name: string;              // "corrections", "skills"
    type: DatabaseType;
    digest: string;
    sizeBytes: number;
  }>;

  // Excluded (listed for transparency)
  excludedDatabases: Array<{
    name: string;
    type: DatabaseType;
    reason: 'private' | 'user-choice';
  }>;

  // Package metadata
  version: string;
  totalSizeBytes: number;
  capabilities: string[];      // ["typescript", "code-review"]
}

type DatabaseType =
  | 'ltm'           // Long-term memory
  | 'corrections'   // Training examples
  | 'preferences'   // User settings
  | 'skills'        // Capability inventory
  | 'hippocampus'   // Episodic memory
  | 'tasks'         // Task queue
  ;
```

---

## Adapter Composition

Stack multiple adapters for compound expertise:

```
Base Model: Llama 3.2 3B
    â†“
Layer 1: code-style-v2 (scale: 1.0)
    â†“
Layer 2: typescript-expert-v3 (scale: 0.8)
    â†“
Layer 3: my-personality (scale: 0.5)
    â†“
Result: AI with your style + TS expertise + code conventions
```

**Implementation:**
```bash
# Activate multiple adapters with scaling
./jtag adapter/compose \
  --adapters="code-style-v2:1.0,typescript-expert-v3:0.8,my-personality:0.5" \
  --save-as="my-composite-expert"
```

**Math:** `W' = W + scaleâ‚(Bâ‚Aâ‚) + scaleâ‚‚(Bâ‚‚Aâ‚‚) + scaleâ‚ƒ(Bâ‚ƒAâ‚ƒ)`

---

## Local Training Pipeline (Rust/Candle)

### Step 1: Data Collection

```typescript
// Automatic collection from chat corrections
interface TrainingExample {
  instruction: string;  // What was asked
  input: string;        // Context provided
  output: string;       // Correct response
  source: 'correction' | 'thumbs_up' | 'import';
}
```

Sources:
- Chat corrections ("Actually, you should...")
- Thumbs up on good responses
- Imported datasets (JSONL)
- Failed tool calls with fixes

### Step 2: Training

```bash
# Train with Candle (auto-detects Metal or CUDA)
./jtag training/start \
  --base="unsloth/Llama-3.2-3B-Instruct" \
  --data="./datasets/training.jsonl" \
  --output="./adapters/output.safetensors" \
  --lora-rank=32 \
  --lora-alpha=64 \
  --batch-size=2 \
  --epochs=3 \
  --learning-rate=2e-4
```

### Step 3: Validation

```bash
# Quick quality check before deployment
./jtag training/validate \
  --adapter="./adapters/output.safetensors" \
  --test-set="./datasets/test.jsonl" \
  --min-quality=0.8
```

### Step 4: Deployment

```bash
# Hot-swap into running inference
./jtag adapter/activate \
  --path="./adapters/output.safetensors" \
  --replace="previous-adapter"
```

---

## Vision Model Workflow (UI/Design)

Vision models are **fully local and free** - same as text models. All Rust, no Python.

### Training UI Expert (Any Platform)

**On Mac (M1/M2/M3) - Metal backend:**
```bash
# 1. Collect UI screenshots with corrections
./jtag training/vision/collect \
  --source="screenshots" \
  --with-corrections

# 2. Train with Candle (Metal, free, local)
./jtag training/vision/start \
  --backend="metal" \
  --base="Qwen/Qwen2.5-VL-3B-Instruct" \
  --data="./datasets/ui-critique.jsonl" \
  --output="./adapters/ui-expert-v1.safetensors" \
  --epochs=3

# 3. Test on a screenshot
./jtag ai/vision/analyze \
  --image="./screenshot.png" \
  --adapter="ui-expert-v1" \
  --prompt="What usability issues do you see?"
```

**On CUDA (RTX 5090) - CUDA backend:**
```bash
# Same workflow, just different backend
./jtag training/vision/start \
  --backend="cuda" \
  --base="Qwen/Qwen2.5-VL-7B-Instruct" \
  --data="./datasets/ui-critique.jsonl" \
  --output="./adapters/ui-expert-v1.safetensors" \
  --epochs=5 \
  --batch-size=8
```

### Example Training Data Format

```jsonl
{"image": "ui-001.png", "conversations": [{"from": "human", "value": "What's wrong with this UI?"}, {"from": "assistant", "value": "The CTA button has low contrast..."}]}
{"image": "ui-002.png", "conversations": [{"from": "human", "value": "Rate this layout"}, {"from": "assistant", "value": "3/5. The visual hierarchy is unclear..."}]}
```

### Vision Adapter Examples

| Adapter | Domain | Training Data | Use Case |
|---------|--------|---------------|----------|
| ui-critique | UI/UX | 1000 annotated screenshots | Design feedback |
| code-screenshot | Code | IDE screenshots + explanations | Visual code review |
| diagram-reader | Architecture | System diagrams + descriptions | Doc generation |
| accessibility-check | A11y | WCAG violation examples | Compliance audit |

---

## Multi-Modal Outputs

LoRA adapters aren't just for text - they work across **all generative modalities**.

### Modality Spectrum

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     OUTPUT MODALITIES                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  TEXT (Current Focus)                                           â”‚
â”‚  â€¢ Chat, code, writing, analysis                                 â”‚
â”‚  â€¢ Llama 3.2, Qwen2.5                                           â”‚
â”‚  â€¢ Training: 30 min on M1                                        â”‚
â”‚                                                                  â”‚
â”‚  VISION INPUT (Phase 6)                                          â”‚
â”‚  â€¢ UI analysis, screenshot understanding                         â”‚
â”‚  â€¢ Moondream, Qwen2.5-VL                                        â”‚
â”‚  â€¢ Training: 1 hr on M1, 15 min on CUDA                         â”‚
â”‚                                                                  â”‚
â”‚  IMAGE OUTPUT (Phase 9)                                          â”‚
â”‚  â€¢ Custom art styles, logo generation                            â”‚
â”‚  â€¢ SDXL, Flux, custom diffusion                                  â”‚
â”‚  â€¢ Training: CUDA required (16GB+ VRAM)                          â”‚
â”‚                                                                  â”‚
â”‚  AUDIO OUTPUT (Phase 10)                                         â”‚
â”‚  â€¢ Voice cloning, TTS styles, music                              â”‚
â”‚  â€¢ XTTS, MusicGen, Bark                                         â”‚
â”‚  â€¢ Training: CUDA recommended                                    â”‚
â”‚                                                                  â”‚
â”‚  VIDEO (Future)                                                  â”‚
â”‚  â€¢ Style transfer, motion synthesis                              â”‚
â”‚  â€¢ SVD, CogVideo, Sora-like                                     â”‚
â”‚  â€¢ Training: Multi-GPU required                                  â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Image Generation Adapters

For personas that want to CREATE images, not just understand them:

```bash
# Train custom art style on CUDA
./jtag training/diffusion/start \
  --backend="cuda" \
  --base="stabilityai/stable-diffusion-xl-base-1.0" \
  --data="./datasets/my-art-style/" \
  --output="./adapters/my-art-lora.safetensors" \
  --steps=2000

# Generate with style
./jtag ai/image/generate \
  --adapter="my-art-lora" \
  --prompt="A cyberpunk city at sunset" \
  --output="./generated.png"
```

### Audio Adapters

Voice cloning, custom TTS, music generation:

```bash
# Clone voice (requires audio samples)
./jtag training/audio/start \
  --backend="cuda" \
  --base="coqui/XTTS-v2" \
  --data="./voice-samples/" \
  --output="./adapters/my-voice.safetensors"

# Generate speech with cloned voice
./jtag ai/audio/speak \
  --adapter="my-voice" \
  --text="Hello, this is my cloned voice" \
  --output="./speech.wav"
```

### Community Specialization

Different users contribute different modalities:

| Hardware | Best For | Community Role |
|----------|----------|----------------|
| M1 8GB | Text inference, small adapters | Consumers, light creators |
| M1 Pro 32GB | Text training, vision inference | Text specialists |
| RTX 3080 16GB | Small diffusion, vision training | Image hobbyists |
| RTX 5090 32GB | All modalities, large models | Full creators |
| Multi-GPU | Video, large diffusion | Production studios |

**The Exchange**: Someone with RTX 5090 trains diffusion adapters â†’ shares with M1 users who can only run inference â†’ M1 users train text adapters â†’ share back.

### Impatient Path (Cloud On-Demand)

Don't have the hardware? Don't want to wait? Pay for speed:

```bash
# Train diffusion adapter in cloud (minutes instead of hours)
./jtag training/diffusion/start \
  --offload=true \
  --base="stabilityai/stable-diffusion-xl-base-1.0" \
  --data="./my-art-style/" \
  --output="./adapters/my-art-lora.safetensors"

# Download and run locally (inference works on M1!)
./jtag ai/image/generate \
  --adapter="my-art-lora" \
  --prompt="A landscape in my style"
```

**The trade-off:**
- **Local (free, slow)**: Train overnight, own your compute
- **Cloud (pay, fast)**: Train in minutes, pay per use

Both produce identical adapters. Choose based on patience.

---

## Cross-Domain Adapters

The adapter system works for **any domain** - not just code. Train on your expertise, share with others.

### Example Domains

| Domain | Adapter Ideas | Training Source |
|--------|--------------|-----------------|
| **Code** | TypeScript expert, code reviewer, debugging specialist | Corrections, PRs, reviews |
| **Design** | UI critique, color theory, accessibility | Annotated screenshots |
| **Writing** | Technical docs, creative writing, tone matching | Edited drafts |
| **Legal** | Contract review, clause analysis | Expert annotations |
| **Medical** | Symptom triage, drug interactions | Clinical guidelines |
| **Gaming** | D&D dungeon master, game balance | Session transcripts |
| **Music** | Chord progression, lyric writing | Song analyses |
| **Finance** | Stock analysis, risk assessment | Market commentary |
| **Language** | Translation style, dialect matching | Parallel texts |
| **Education** | Tutoring style, explanation clarity | Student interactions |

### Domain Specialization Pattern

```bash
# 1. Collect domain-specific corrections
./jtag training/prepare \
  --source="corrections" \
  --domain="legal" \
  --output="./datasets/legal-review.jsonl"

# 2. Train specialist adapter
./jtag training/start \
  --data="./datasets/legal-review.jsonl" \
  --output="./adapters/legal-reviewer-v1.safetensors" \
  --epochs=3

# 3. Compose with base skills
./jtag adapter/compose \
  --adapters="legal-reviewer-v1:1.0,writing-clarity:0.5" \
  --save-as="legal-writer"
```

### Community Growth Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ADAPTER ECOSYSTEM                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Week 1: Early adopters train personal style adapters           â”‚
â”‚          â””â”€â”€ 10 adapters, mostly code-focused                   â”‚
â”‚                                                                  â”‚
â”‚  Month 1: Power users create domain specialists                 â”‚
â”‚           â””â”€â”€ 100 adapters, diverse domains emerging            â”‚
â”‚                                                                  â”‚
â”‚  Month 6: Community remixes and compositions                    â”‚
â”‚           â””â”€â”€ 1000 adapters, composition patterns emerge        â”‚
â”‚                                                                  â”‚
â”‚  Year 1: Ecosystem of specialized stacks                        â”‚
â”‚          â””â”€â”€ 10000+ adapters, emergent expertise combos         â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Insight**: The value isn't in any single adapter - it's in the combinations. A "legal-typescript-concise" stack is more valuable than any piece alone.

---

## Cloud Offload (Optional)

**Not required.** Only for users who want to trade money for time.

```bash
# Optional: Configure API key for acceleration
./jtag config/set --key="TOGETHER_API_KEY" --value="tok_..."

# Training automatically uses cloud when --offload is set
./jtag training/start \
  --data="./datasets/training.jsonl" \
  --output="./adapters/my-adapter.safetensors" \
  --offload=true  # Optional: uses cloud if API key exists
```

Supported providers (all pay-per-use):
- Together AI - Best for Llama/Qwen models
- Fireworks AI - Fast inference focus
- Modal - Flexible GPU rentals
- RunPod - Cheapest per-hour

**Without API key**: Everything works locally. Zero cost.

---

## Economy Model

### Core Principle: Free First, Always

**Everything essential is free and local.** No accounts, no subscriptions, no usage limits.

### Free Forever (Default)
- Download unlimited adapters from community
- Train locally on your hardware (any backend)
- Share unlimited adapters with community
- Compose and remix adapters freely
- Full source code access (open source)

### Optional Acceleration (API Keys)
For users who WANT to pay for speed:
- Cloud training offload (Together, Fireworks, Modal)
- Train 70B+ models without local GPU
- 10-100x faster than local training
- Data privacy options (delete after training)

**How it works:**
```bash
# Add optional API key for acceleration
./jtag config/set --key="TOGETHER_API_KEY" --value="tok_..."

# Now training can optionally use cloud
./jtag training/start --offload=true  # Uses cloud if API key set
./jtag training/start                  # Always local, always free
```

### Adapter Trading
The real economy is **adapters**, not subscriptions:
- Power users create high-quality adapters
- Community remixes and improves
- Everyone benefits from collective learning
- No paywalls, just reputation/ratings

---

## Commands Summary

```bash
# Discovery
./jtag adapter/search --query="python expert"
./jtag adapter/list --filter="vision"
./jtag adapter/info --id="typescript-expert-v3"

# Installation
./jtag adapter/install --id="community/wine-sommelier"
./jtag adapter/uninstall --id="wine-sommelier"
./jtag adapter/update --id="typescript-expert-v3"

# Activation
./jtag adapter/activate --id="typescript-expert-v3"
./jtag adapter/deactivate
./jtag adapter/compose --adapters="a:1.0,b:0.5"

# Training
./jtag training/prepare --source="corrections"
./jtag training/start --data="./data.jsonl"
./jtag training/status
./jtag training/validate --adapter="./output"

# Publishing
./jtag adapter/publish --path="./adapter" --name="my-adapter"
./jtag adapter/unpublish --id="my-adapter"
```

---

## Success Metrics

1. **Accessibility**: 80% of M1 Mac users can train a LoRA in < 30 min
2. **Adapter Size**: Average adapter < 50 MB (easy to share)
3. **Quality**: Community adapters rated 4+ stars on average
4. **Composition**: Users create 3+ adapter combinations on average
5. **Economy**: Active adapter sharing/trading ecosystem

---

## Implementation Phases

### Phase 1: Rust Inference Backend (DONE)
- [x] Candle gRPC worker with Metal support
- [x] Model hot-swap via RPC commands
- [x] GPU memory allocator with smart eviction
- [x] Proto schema for model/adapter management

### Phase 2: LoRA Weight Loading (DONE)
- [x] Proto schema for adapter load/unload
- [x] Safetensor parsing in Rust (lora.rs with F32/F16/BF16 support)
- [x] LoRA weight merging with base model (W' = W + scale Ã— B @ A)
- [x] Rebuild model with merged weights (`rebuild_with_lora_from_paths()`)
- [x] TypeScript client with `merge` option
- [ ] Hot-swap adapters without full model reload (future optimization)

### Phase 2.5: Model-Agnostic Adapter Mapping (TODO)
Different base models and training frameworks use different tensor naming:

```
PEFT/HuggingFace:  base_model.model.model.layers.X.self_attn.q_proj
Candle Llama:      model.layers.X.self_attn.q_proj.weight
Other frameworks:  layers.X.attention.query.weight
```

**Solution: Adapter Trait Per Model Architecture**

```rust
trait LoRANameMapper {
    /// Map LoRA tensor name to base model tensor name
    fn map_name(&self, lora_name: &str) -> String;

    /// Get target modules for this architecture
    fn target_modules(&self) -> &[&str];
}

struct LlamaNameMapper;
struct Qwen2NameMapper;
struct MistralNameMapper;

// Registry of mappers by model architecture
fn get_mapper(model_id: &str) -> Box<dyn LoRANameMapper>;
```

**Or Algorithmic Strategy:**
- Parse adapter_config.json for `base_model_name_or_path`
- Auto-detect architecture from model config
- Apply appropriate naming transformation

**Current**: Hardcoded Llama/PEFT mapping in `lora.rs::map_lora_name_to_model_name()`

### MILESTONE 1: Single Adapter Inference âœ… COMPLETE (2026-01-06)
- [x] Downloaded public Llama LoRA: `Jiten1024/llama-3.2-3b-int-finetune-jav-rank-1-alpha-32`
- [x] Load adapter via gRPC with merge: 196 LoRA layer pairs merged
- [x] Generate text: haiku generation test
- [x] Validated output differs from base model (different vocabulary, style)
- [x] **PROVEN: parsing, merging, rebuild, generation all work**

**Test**: `npx tsx tests/lora-adapter-test.ts` (integration test added)

### MILESTONE 2: Genome Assembly (Multi-Adapter) âœ… COMPLETE (2026-01-06)
- [x] Loaded 2 public adapters: Jiten1024 (rank-1) + Sizhkhy (rank-64)
- [x] Stacked: W' = W + 0.5Ã—(Bâ‚Aâ‚) + 0.5Ã—(Bâ‚‚Aâ‚‚)
- [x] 392 layers merged from 2 adapters in ~9 seconds
- [x] Stacked output shows vocabulary blend (darker tone + different patterns)
- [x] **PROVEN: composition math works**

**Test**: `npx tsx tests/genome-stacking-test.ts`

### MILESTONE 3: HuggingFace Hub Integration âœ… COMPLETE (2026-01-06)
- [x] DownloadAdapter gRPC RPC - download by repo ID
- [x] adapter_registry.rs - HF Hub integration module
- [x] Parse adapter_config.json for metadata (rank, alpha, base model)
- [x] Auto-cache via hf-hub crate (~/.cache/huggingface/hub/)
- [x] TypeScript client `downloadAdapter()` method
- [x] **PROVEN: can pull adapters from HuggingFace by repo ID**

**Test**:
```typescript
const result = await client.downloadAdapter('Jiten1024/llama-3.2-3b-int-finetune-jav-rank-1-alpha-32');
// Downloads to HF cache, parses config, registers adapter
```

### MILESTONE 3.5: Federated Adapter Exchange (IN PROGRESS)
See: [Federated Adapter Exchange](#federated-adapter-exchange) section below

### MILESTONE 3.6: Adapter Search Command âœ… COMPLETE (2026-01-06)
- [x] `adapter/search` command - search HuggingFace and local registry
- [x] Filter by base model, sort by downloads/likes/recent
- [x] Shows which adapters are already installed locally
- [x] Parallel search across multiple sources
- [x] **PROVEN: personas can discover adapters for their needs**

**Usage**:
```bash
# Search for tool-calling adapters compatible with Llama
./jtag adapter/search --query="tool calling" --baseModel="llama" --limit=5

# Search only local adapters
./jtag adapter/search --query="code" --source="local"
```

### MILESTONE 4: Local Registry âœ… COMPLETE (2026-01-06)
- [x] `~/.continuum/adapters/installed/` directory structure
- [x] manifest.json for each adapter with metadata
- [x] Local search via `adapter/search --source="local"`
- [x] Installed adapters marked in HuggingFace search results
- [x] **PROVEN: can organize, store, find adapters locally**

### MILESTONE 5: Provider Abstraction âœ… DESIGNED (2026-01-06)
- [x] `IAdapterProvider` interface - unified contract for all backends
- [x] `LocalAdapterProvider` - wraps InferenceGrpcClient
- [x] `TogetherAdapterProvider` - validates cloud LoRA pattern
- [x] `AdapterProviderRegistry` - federated search + best-provider selection
- [x] **PROVEN: abstraction works across local â†” cloud â†” third-party APIs**

**Architecture validated but thin on implementation** - cloud providers stubbed, not production-ready. The interface design is the deliverable; implementations come in later phases.

```
Commands (adapter/search, adapter/deploy)
    â†“
AdapterProviderRegistry (optional, for multi-provider)
    â†“
Providers (Local, Together, Fireworks, ...)
    â†“
Local â†’ InferenceGrpcClient â†’ Rust worker
Cloud â†’ HTTP APIs (Together, Fireworks, etc.)
```

### MILESTONE 6: Persona Self-Improvement (NEXT)
- [ ] Persona can search for adapters matching current task
- [ ] Persona can try adapter temporarily (A/B comparison)
- [ ] Persona can adopt adapter into genome
- [ ] Persona config stores genome (adapter stack)
- [ ] **GOAL: personas autonomously improve themselves**

### MILESTONE 7: Full Personas
- [ ] Genome config (JSON stack definition)
- [ ] `./jtag persona/export` with selected DBs
- [ ] `./jtag persona/import`
- [ ] **Proves: complete packaging and sharing story**

---

### Phase 3: Training Commands
- [ ] `training/prepare` - Collect corrections â†’ JSONL
- [ ] `training/start` - Launch Candle LoRA training (Metal/CUDA)
- [ ] `training/status` - Monitor training progress
- [ ] `training/validate` - Quality check adapter

### Phase 4: Adapter Management
- [ ] `adapter/install` - Download from hub
- [ ] `adapter/activate` - Load into inference worker
- [ ] `adapter/deactivate` - Unload adapter
- [ ] `adapter/list` - Show installed adapters

### Phase 5: Composition
- [ ] Multi-adapter loading in Rust worker
- [ ] Scale factor support per adapter
- [ ] `adapter/compose` - Merge adapters
- [ ] Composite adapter saving

### Phase 6: Vision Models
- [ ] Moondream/Qwen2.5-VL integration in Candle
- [ ] Image encoding in gRPC proto
- [ ] `ai/vision/*` commands
- [ ] Vision LoRA training (Candle + Metal/CUDA)

### Phase 7: Hub & Sharing
- [ ] Adapter manifest schema validation
- [ ] `adapter/publish` command
- [ ] Community hub API
- [ ] Rating/discovery system

### Phase 8: Cloud Offload (Optional)
- [ ] Together/Fireworks API integration
- [ ] `training/offload` command
- [ ] Automatic adapter download

### Phase 9: Diffusion/Image Generation
- [ ] SDXL LoRA training wrapper
- [ ] `training/diffusion/start` command
- [ ] `ai/image/generate` command
- [ ] Diffusion adapter hub integration

### Phase 10: Audio Generation
- [ ] XTTS/voice cloning integration
- [ ] `training/audio/start` command
- [ ] `ai/audio/speak` command
- [ ] Audio adapter formats

### Phase 11: Video (Future)
- [ ] Research SVD/CogVideo integration
- [ ] Video LoRA training (multi-GPU)
- [ ] `ai/video/generate` command

---

## Autonomous AI Self-Improvement

**Vision**: Personas become self-directed learners who discover, evaluate, and adopt new capabilities.

### The Self-Improvement Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PERSONA SELF-IMPROVEMENT                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   1. DISCOVER                                                    â”‚
â”‚      â”œâ”€â”€ Search for adapters matching current task needs         â”‚
â”‚      â”œâ”€â”€ Browse by capability, base model, popularity            â”‚
â”‚      â””â”€â”€ Commands.execute('adapter/search', { query: task })     â”‚
â”‚                                                                  â”‚
â”‚   2. TRY                                                         â”‚
â”‚      â”œâ”€â”€ Temporarily load adapter                                â”‚
â”‚      â”œâ”€â”€ Run test prompts, compare output quality                â”‚
â”‚      â””â”€â”€ adapter/try --id="..." --testPrompt="..."              â”‚
â”‚                                                                  â”‚
â”‚   3. EVALUATE                                                    â”‚
â”‚      â”œâ”€â”€ Compare outputs with/without adapter                    â”‚
â”‚      â”œâ”€â”€ Self-assess improvement in target domain                â”‚
â”‚      â””â”€â”€ Log results for future reference                        â”‚
â”‚                                                                  â”‚
â”‚   4. ADOPT                                                       â”‚
â”‚      â”œâ”€â”€ Add to permanent genome with appropriate scale          â”‚
â”‚      â”œâ”€â”€ Update persona config                                   â”‚
â”‚      â””â”€â”€ genome/add --adapterId="..." --scale=0.7               â”‚
â”‚                                                                  â”‚
â”‚   5. SHARE                                                       â”‚
â”‚      â”œâ”€â”€ Create adapters from successful interactions            â”‚
â”‚      â”œâ”€â”€ Publish to mesh for other personas                      â”‚
â”‚      â””â”€â”€ training/start --from=corrections --publish=true        â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Persona-Created Sub-Personas

AIs can spawn specialized sub-personas for delegation:

```typescript
// PersonaUser creating a sentinel for monitoring
await Commands.execute('persona/create', {
  name: 'code-reviewer-sentinel',
  purpose: 'Continuously monitor PRs and flag issues',
  genome: [
    { adapterId: 'code-review-v1', scale: 1.0 },
    { adapterId: 'security-audit-v1', scale: 0.5 }
  ],
  schedule: { type: 'continuous', triggerOn: 'pr:created' }
});
```

### Task Manager Sentinels

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       SENTINEL TYPES                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚   ğŸ” MONITOR SENTINELS                                          â”‚
â”‚      â”œâ”€â”€ Watch for code changes, PR activity                     â”‚
â”‚      â”œâ”€â”€ Alert on security issues                                â”‚
â”‚      â””â”€â”€ Track task completion                                   â”‚
â”‚                                                                  â”‚
â”‚   ğŸ”§ WORKER SENTINELS                                           â”‚
â”‚      â”œâ”€â”€ Auto-fix common issues                                  â”‚
â”‚      â”œâ”€â”€ Generate documentation                                  â”‚
â”‚      â””â”€â”€ Run continuous tests                                    â”‚
â”‚                                                                  â”‚
â”‚   ğŸ“ TRAINING SENTINELS                                         â”‚
â”‚      â”œâ”€â”€ Collect corrections for adapter training                â”‚
â”‚      â”œâ”€â”€ Validate adapter quality                                â”‚
â”‚      â””â”€â”€ Manage training queue                                   â”‚
â”‚                                                                  â”‚
â”‚   ğŸ¤ COORDINATION SENTINELS                                     â”‚
â”‚      â”œâ”€â”€ Route tasks to appropriate personas                     â”‚
â”‚      â”œâ”€â”€ Manage workload distribution                            â”‚
â”‚      â””â”€â”€ Handle inter-persona communication                      â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### External AI (Claude Code) Integration

Claude Code (the external orchestrator) can also leverage the adapter system:

```bash
# Claude Code searching for a better tool-calling adapter
./jtag adapter/search --query="tool calling agentic" --baseModel="llama"

# Creating a sentinel to monitor deployments
./jtag persona/create \
  --name="deploy-monitor" \
  --genome='[{"adapterId":"ops-expert","scale":0.8}]' \
  --trigger="npm:start"

# Building task managers for complex workflows
./jtag task/create \
  --type="recurring" \
  --interval="1h" \
  --action="adapter/search --query=new --sort=recent"
```

### The Cambrian Explosion

When AIs can:
1. **Discover** new capabilities (adapter search)
2. **Try** them risk-free (temporary loading)
3. **Adopt** successful ones (genome modification)
4. **Create** new ones (training from corrections)
5. **Delegate** to specialized sub-AIs (persona creation)

...you get an exponential expansion of AI capabilities, managed and directed by the AIs themselves.

**This is the Cambrian explosion for AI personas.**

---

## Multi-Provider Adapter Abstraction

External AI providers now support LoRA adapters - we can abstract search and deployment across all backends.

### Provider Capabilities (2025)

| Provider | Upload HF Adapters | Multi-LoRA | Pricing |
|----------|-------------------|------------|---------|
| **Local (Candle)** | âœ… Direct | âœ… Genome stacking | Free (local compute) |
| **Together.ai** | âœ… [Serverless Multi-LoRA](https://docs.together.ai/docs/lora-inference) | âœ… Hundreds | Base model per-token |
| **Fireworks.ai** | âœ… [SFT + LoRA](https://docs.fireworks.ai/fine-tuning/fine-tuning-models) | âœ… 100+ per deployment | Base model per-token |
| **Replicate** | âœ… Custom models | âš ï¸ Single | Per-second |
| **OpenAI** | âŒ Fine-tune only | âŒ | Fine-tuned model rate |

### Unified Provider Interface

```typescript
interface IAdapterProvider {
  name: string;
  type: 'local' | 'cloud-lora' | 'cloud-finetune';

  // Search adapters available on this provider
  search(query: string, options?: AdapterSearchOptions): Promise<AdapterSearchResultItem[]>;

  // Deploy an adapter (upload to cloud or load locally)
  deploy(adapterId: string): Promise<DeployedAdapter>;

  // Check compatibility (base model, rank, etc.)
  isCompatible(adapter: AdapterSearchResultItem): boolean;

  // Cost estimation
  estimateCost(adapterId: string, tokensPerMonth: number): Promise<CostEstimate>;
}

// Provider implementations
class LocalAdapterProvider implements IAdapterProvider { }      // Candle/Ollama
class TogetherAdapterProvider implements IAdapterProvider { }   // Together.ai API
class FireworksAdapterProvider implements IAdapterProvider { }  // Fireworks.ai API
```

### Federated Search Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FEDERATED ADAPTER SEARCH                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  User: adapter/search --query="tool calling" --providers="all"  â”‚
â”‚                           â”‚                                      â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚           â–¼               â–¼               â–¼                     â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚     â”‚   Local  â”‚   â”‚ Together â”‚   â”‚Fireworks â”‚                 â”‚
â”‚     â”‚ Registry â”‚   â”‚    API   â”‚   â”‚   API    â”‚                 â”‚
â”‚     â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚          â”‚              â”‚              â”‚                        â”‚
â”‚          â–¼              â–¼              â–¼                        â”‚
â”‚     ~/.continuum/   HuggingFace   HuggingFace                  â”‚
â”‚     adapters/       (via their    (via their                   â”‚
â”‚     installed/      integration)  integration)                  â”‚
â”‚                                                                  â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                           â–¼                                      â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚              â”‚  Merged + Deduplicated  â”‚                        â”‚
â”‚              â”‚  Results with Provider  â”‚                        â”‚
â”‚              â”‚  Compatibility Flags    â”‚                        â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Remote Persona Adapter Usage

External personas (Claude, GPT) can't load LoRA directly, but can:

1. **Route to local personas** with specific adapters loaded
2. **Use cloud providers** (Together/Fireworks) with adapters deployed
3. **Leverage system prompts** as "soft adapters" (behavior templates)

```typescript
// Claude persona wanting "code review" capability
const adapter = await Commands.execute('adapter/search', {
  query: 'code review',
  baseModel: 'llama'
});

// Option 1: Route to local persona with adapter
await Commands.execute('persona/delegate', {
  to: 'helper-ai',  // Local Ollama persona
  genome: [{ adapterId: adapter.results[0].id, scale: 1.0 }],
  task: 'Review this code...'
});

// Option 2: Deploy to Together.ai for cloud inference
await Commands.execute('adapter/deploy', {
  adapterId: adapter.results[0].id,
  provider: 'together',
  baseModel: 'meta-llama/Llama-3.1-8B'
});
```

### Cost Optimization Strategy

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ADAPTER DEPLOYMENT STRATEGY                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  LOCAL FIRST (Free)                                             â”‚
â”‚  â””â”€â”€ If hardware supports base model                            â”‚
â”‚  â””â”€â”€ Use Candle/Ollama with direct LoRA loading                 â”‚
â”‚                                                                  â”‚
â”‚  CLOUD LORA (Cheap)                                             â”‚
â”‚  â””â”€â”€ Together.ai / Fireworks.ai                                 â”‚
â”‚  â””â”€â”€ Base model pricing, no adapter overhead                    â”‚
â”‚  â””â”€â”€ Good for: High-volume, specialized tasks                   â”‚
â”‚                                                                  â”‚
â”‚  CLOUD FINE-TUNE (Expensive)                                    â”‚
â”‚  â””â”€â”€ OpenAI / Anthropic fine-tuning                             â”‚
â”‚  â””â”€â”€ Higher quality, proprietary models                         â”‚
â”‚  â””â”€â”€ Good for: Production, critical applications                â”‚
â”‚                                                                  â”‚
â”‚  HYBRID (Optimal)                                               â”‚
â”‚  â””â”€â”€ Local for development/testing                              â”‚
â”‚  â””â”€â”€ Cloud LoRA for production inference                        â”‚
â”‚  â””â”€â”€ Cloud fine-tune for flagship personas                      â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## References

- [Candle](https://github.com/huggingface/candle) - Rust ML framework with Metal/CUDA backends
- [candle-lora](https://github.com/EricLBuehler/candle-lora) - Pure Rust LoRA implementation
- [safetensors](https://github.com/huggingface/safetensors) - Safe, fast tensor serialization
- [CONTINUOUS-LEARNING-RUNTIME.md](CONTINUOUS-LEARNING-RUNTIME.md) - Runtime architecture
- [LORA-TRAINING-STRATEGY.md](LORA-TRAINING-STRATEGY.md) - Training approaches
