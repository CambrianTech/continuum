# Real-Time Voice Architecture

## Status

**Validated**: Browser â†’ WebSocket â†’ Rust â†’ WebSocket â†’ Browser audio pipeline works.
Current implementation has echo/latency issues due to hacky design, but proves the path is viable.

**Next**: Proper architecture with efficiency, concurrency, and natural conversation latency.

---

## The Room's Needs

A room with 3 humans + 5 AIs needs:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          ROOM                                    â”‚
â”‚                                                                  â”‚
â”‚  Humans: Joel, Alice, Bob (mics, cameras)                       â”‚
â”‚  AIs: Helper, Teacher, CodeReview, Claude, GPT (voices, avatars)â”‚
â”‚                                                                  â”‚
â”‚  Requirements:                                                   â”‚
â”‚  â€¢ Everyone hears everyone else (mix-minus)                      â”‚
â”‚  â€¢ One shared transcript (not N separate STT streams)           â”‚
â”‚  â€¢ AI turn-taking (not all 5 responding at once)                â”‚
â”‚  â€¢ Each AI has distinct voice                                    â”‚
â”‚  â€¢ <500ms latency for natural conversation                      â”‚
â”‚  â€¢ Video grid with avatars for AIs                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Concurrency Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    RUST STREAMING-CORE                          â”‚
â”‚                   (Real-time, zero-copy)                        â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ Audio Threadâ”‚  â”‚ STT Thread  â”‚  â”‚ TTS Thread  â”‚             â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚             â”‚
â”‚  â”‚ Mix-minus   â”‚  â”‚ Whisper     â”‚  â”‚ Kokoro      â”‚             â”‚
â”‚  â”‚ per client  â”‚  â”‚ streaming   â”‚  â”‚ streaming   â”‚             â”‚
â”‚  â”‚             â”‚  â”‚             â”‚  â”‚             â”‚             â”‚
â”‚  â”‚ 10ms frames â”‚  â”‚ Utterance   â”‚  â”‚ Chunk-based â”‚             â”‚
â”‚  â”‚ lock-free   â”‚  â”‚ detection   â”‚  â”‚ output      â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚         â”‚                â”‚                â”‚                     â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                          â”‚                                      â”‚
â”‚                    Ring Buffers                                 â”‚
â”‚                    (lock-free SPSC)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”‚ Events (utterance complete, etc.)
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TYPESCRIPT ORCHESTRATOR                       â”‚
â”‚                   (Conversation logic only)                      â”‚
â”‚                                                                  â”‚
â”‚  â€¢ Receives: "Joel said: How do I fix this bug?"                â”‚
â”‚  â€¢ Decides: Which AI(s) should respond?                         â”‚
â”‚  â€¢ Coordinates: Teacher first, then CodeReview if needed        â”‚
â”‚  â€¢ Sends: "Teacher, respond to this"                            â”‚
â”‚  â€¢ Receives: "Teacher says: Let me explain..."                  â”‚
â”‚  â€¢ Sends to TTS: "Speak this with Teacher's voice"              â”‚
â”‚                                                                  â”‚
â”‚  *** NO AUDIO BYTES EVER TOUCH THIS LAYER ***                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Key Design Decisions

### 1. Single STT Stream Per Room

```rust
// WRONG: N participants = N STT instances = N Ã— CPU
for participant in room.participants {
    whisper.transcribe(participant.audio); // ğŸ’€
}

// RIGHT: Mixed audio â†’ single STT â†’ shared transcript
let mixed = mixer.get_room_audio();
let transcript = whisper.transcribe(mixed);
room.broadcast_transcript(transcript);
```

### 2. AI Turn-Taking Arbitration

```typescript
interface TurnArbiter {
  // When human finishes speaking, who responds?
  selectResponder(
    utterance: Utterance,
    availableAIs: AI[],
    conversationContext: Context
  ): AI | null;

  // Can this AI interrupt?
  canInterrupt(ai: AI, currentSpeaker: Participant): boolean;
}

// Strategies:
// - RelevanceArbiter: Score by topic relevance
// - RoundRobinArbiter: Take turns
// - DirectedArbiter: User explicitly addresses an AI
// - SilenceArbiter: No one responds unless confidence high
```

### 3. Lock-Free Audio Pipeline

```rust
struct ParticipantOutput {
    buffer: RingBuffer<f32>,  // Lock-free SPSC
    write_pos: AtomicUsize,
    read_pos: AtomicUsize,
}

// Audio thread never blocks
fn audio_tick(&mut self) {
    for participant in &mut self.participants {
        if let Some(samples) = participant.input.try_read() {
            self.mixer.add(participant.id, samples);
        }
        let output = self.mixer.get_mix_minus(participant.id);
        participant.output.try_write(output);
    }
}
```

### 4. Latency Budget (500ms total)

```
â”œâ”€â”€ Network in:        50ms  (WebSocket)
â”œâ”€â”€ Audio buffering:   60ms  (3 Ã— 20ms frames)
â”œâ”€â”€ STT:              150ms  (streaming Whisper)
â”œâ”€â”€ AI decision:       50ms  (which AI responds)
â”œâ”€â”€ AI inference:       0ms  (async, pre-compute while STT)
â”œâ”€â”€ TTS first chunk:  100ms  (streaming Kokoro)
â”œâ”€â”€ Network out:       50ms  (WebSocket)
â””â”€â”€ Audio playout:     40ms  (2 Ã— 20ms frames)
                      â”€â”€â”€â”€â”€
                      500ms
```

### 5. Speculative Execution

```typescript
// While human is still speaking, AIs start thinking
onPartialTranscript(partial: string) {
  // "How do I fix this b..."
  this.speculativeResponses = this.ais.map(ai =>
    ai.generateSpeculative(partial)
  );
}

onUtteranceComplete(final: string) {
  // Use speculative if valid, else fresh (but had head start)
}
```

---

## Data Flow

```
Human speaks
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           WebSocket (binary)             â”‚
â”‚         Opus-encoded audio               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Rust Audio Thread               â”‚
â”‚  â€¢ Decode Opus                          â”‚
â”‚  â€¢ Add to mixer                         â”‚
â”‚  â€¢ Generate mix-minus for others        â”‚
â”‚  â€¢ Feed to STT thread                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚             â”‚
    â–¼             â–¼             â–¼
Mix-minus     STT Thread    Speculative
to others     (Whisper)     AI inference
    â”‚             â”‚             â”‚
    â”‚             â–¼             â”‚
    â”‚      Utterance complete   â”‚
    â”‚             â”‚             â”‚
    â”‚             â–¼             â”‚
    â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚      â”‚ Orchestrator â”‚â—„â”€â”€â”€â”˜
    â”‚      â”‚  (TypeScript)â”‚
    â”‚      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚             â”‚
    â”‚             â–¼
    â”‚      Select AI responder
    â”‚             â”‚
    â”‚             â–¼
    â”‚      AI generates text
    â”‚             â”‚
    â”‚             â–¼
    â”‚      TTS Thread (Kokoro)
    â”‚             â”‚
    â”‚             â–¼
    â”‚      Audio chunks
    â”‚             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚             â”‚
                  â–¼             â–¼
            To speaker      To others
            (AI voice)     (mix-minus)
```

---

## Interfaces

### TypeScript â†’ Rust

```typescript
interface AudioEngine {
  joinRoom(roomId: RoomId, userId: UserId): ParticipantHandle;
  leaveRoom(handle: ParticipantHandle): void;
  setMuted(handle: ParticipantHandle, muted: boolean): void;
  speak(text: string, voiceId: VoiceId): SpeechHandle;
  cancelSpeech(handle: SpeechHandle): void;
}
```

### Rust â†’ TypeScript (Events)

```typescript
interface ConversationOrchestrator {
  onUtterance(event: {
    speakerId: UserId;
    transcript: string;
    audioRef: AudioHandle;
    confidence: number;
  }): void;

  onPartialTranscript(event: {
    speakerId: UserId;
    partial: string;
  }): void;

  onSpeakingStateChange(event: {
    participantId: UserId;
    isSpeaking: boolean;
  }): void;
}
```

---

## File Structure (Target)

```
workers/streaming-core/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ audio/
â”‚   â”‚   â”œâ”€â”€ mixer.rs          # Mix-minus logic
â”‚   â”‚   â”œâ”€â”€ ring_buffer.rs    # Lock-free buffers
â”‚   â”‚   â””â”€â”€ codec.rs          # Opus encode/decode
â”‚   â”œâ”€â”€ stt/
â”‚   â”‚   â”œâ”€â”€ whisper.rs        # Whisper integration
â”‚   â”‚   â”œâ”€â”€ vad.rs            # Voice activity detection
â”‚   â”‚   â””â”€â”€ utterance.rs      # Utterance boundary detection
â”‚   â”œâ”€â”€ tts/
â”‚   â”‚   â”œâ”€â”€ kokoro.rs         # Kokoro TTS
â”‚   â”‚   â””â”€â”€ voice_bank.rs     # Voice profiles
â”‚   â”œâ”€â”€ room/
â”‚   â”‚   â”œâ”€â”€ state.rs          # Room state machine
â”‚   â”‚   â”œâ”€â”€ participant.rs    # Per-participant state
â”‚   â”‚   â””â”€â”€ events.rs         # Events to TypeScript
â”‚   â””â”€â”€ main.rs

system/conversation/
â”œâ”€â”€ Orchestrator.ts           # Turn-taking, AI selection
â”œâ”€â”€ TurnArbiter.ts            # Who speaks when
â”œâ”€â”€ SpeculativeEngine.ts      # Pre-compute responses
â””â”€â”€ VoiceProfile.ts           # AI voice configurations

commands/voice/
â”œâ”€â”€ room-join/                # Join voice in room
â”œâ”€â”€ room-leave/               # Leave voice
â”œâ”€â”€ mute/                     # Mute/unmute
â””â”€â”€ speak/                    # AI speaks (text â†’ TTS)
```

---

## Migration Path

### Phase 1: Fix Rust Streaming-Core (Current)
- [ ] Implement proper mix-minus (don't echo back to self)
- [ ] Add VAD (voice activity detection)
- [ ] Add utterance boundary detection
- [ ] Emit events to TypeScript on utterance complete

### Phase 2: Wire STT
- [ ] Whisper integration in Rust
- [ ] Streaming transcription
- [ ] Emit partial + final transcripts

### Phase 3: Orchestration Layer
- [ ] Create ConversationOrchestrator in TypeScript
- [ ] Implement TurnArbiter (simple relevance-based)
- [ ] Post transcripts to chat room
- [ ] Trigger AI responses

### Phase 4: Wire TTS
- [ ] Kokoro integration in Rust
- [ ] Voice profiles per persona
- [ ] Streaming audio output
- [ ] Mix AI audio into room

### Phase 5: Polish
- [ ] Speculative execution
- [ ] Interruption handling
- [ ] Latency optimization
- [ ] Video/avatar integration

---

## Persona Inbox Integration

**Key Insight**: Voice is a **modality**, not a domain. An AI in a voice call is doing `chat` domain work, just with audio I/O instead of text. This means:

- Same `InboxMessage` structure works for both text and voice
- Each voice call is a different `roomId` (multi-context support)
- AI can be in text chat + multiple voice calls simultaneously
- All contexts flow through the same priority queue

### Multi-Context Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PersonaInbox (Helper AI)                    â”‚
â”‚                      (Priority Queue)                            â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ [0.8] Voice: Joel asked "How do I fix this?" (call-abc)  â”‚   â”‚
â”‚  â”‚ [0.7] Text: @Helper can you review? (general room)       â”‚   â”‚
â”‚  â”‚ [0.5] Voice: Alice said "Any updates?" (call-xyz)        â”‚   â”‚
â”‚  â”‚ [0.3] Text: Random discussion (general room)             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚  CNS processes highest priority first                            â”‚
â”‚  Response routes to appropriate output (TTS vs text)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### InboxMessage Extension

```typescript
// QueueItemTypes.ts - add sourceModality
export interface InboxMessage extends BaseQueueItem {
  type: 'message';
  roomId: UUID;              // Room or voice session ID
  content: string;           // Text or transcribed audio
  senderId: UUID;
  senderName: string;
  senderType: 'human' | 'persona' | 'agent' | 'system';
  mentions?: boolean;

  // NEW: Modality tracking for response routing
  sourceModality?: 'text' | 'voice';   // Where input came from
  voiceSessionId?: UUID;               // Voice call context if applicable
}
```

### VoiceOrchestrator Integration

```typescript
/**
 * VoiceOrchestrator bridges Rust streaming-core â†” PersonaInbox
 *
 * Responsibilities:
 * 1. Receive transcription events from Rust
 * 2. Create InboxMessages for participating personas
 * 3. Turn arbitration (which AI responds)
 * 4. Route responses to TTS
 */
class VoiceOrchestrator {
  // Called when Rust emits utterance complete
  async onUtterance(event: UtteranceEvent): Promise<void> {
    const { sessionId, speakerId, transcript, speakerName } = event;

    // Get participating personas for this voice session
    const participants = await this.getVoiceParticipants(sessionId);
    const personas = participants.filter(p => p.type === 'persona');

    // Turn arbitration: which AI should respond?
    const responder = await this.arbiter.selectResponder(
      event,
      personas,
      this.getConversationContext(sessionId)
    );

    if (!responder) return; // No AI should respond

    // Create InboxMessage for selected responder
    const message: InboxMessage = {
      id: generateUUID(),
      type: 'message',
      domain: 'chat',
      roomId: sessionId,           // Voice session = room
      content: transcript,
      senderId: speakerId,
      senderName: speakerName,
      senderType: 'human',
      priority: this.calculateVoicePriority(event, responder),
      timestamp: Date.now(),
      sourceModality: 'voice',     // Route response to TTS
      voiceSessionId: sessionId
    };

    // Enqueue to responder's inbox
    await responder.inbox.enqueue(message);
  }

  // Called when persona generates response
  async onPersonaResponse(
    personaId: UUID,
    response: string,
    originalMessage: InboxMessage
  ): Promise<void> {
    if (originalMessage.sourceModality === 'voice') {
      // Route to TTS
      await this.speakResponse(
        originalMessage.voiceSessionId!,
        personaId,
        response
      );
    } else {
      // Normal text posting (existing flow)
    }
  }
}
```

### Arbitration Strategies

```typescript
interface TurnArbiter {
  selectResponder(
    event: UtteranceEvent,
    candidates: PersonaUser[],
    context: ConversationContext
  ): PersonaUser | null;
}

// Strategy 1: Directed addressing
class DirectedArbiter implements TurnArbiter {
  selectResponder(event, candidates, context): PersonaUser | null {
    // "Hey Teacher, explain this" â†’ Teacher responds
    for (const persona of candidates) {
      const nameLower = persona.displayName.toLowerCase();
      if (event.transcript.toLowerCase().includes(nameLower)) {
        return persona;
      }
    }
    return null;
  }
}

// Strategy 2: Topic relevance
class RelevanceArbiter implements TurnArbiter {
  selectResponder(event, candidates, context): PersonaUser | null {
    // Score each AI by topic expertise
    const scores = candidates.map(p => ({
      persona: p,
      score: this.calculateRelevance(event.transcript, p.expertise)
    }));

    const best = scores.sort((a, b) => b.score - a.score)[0];
    return best.score > 0.5 ? best.persona : null;
  }
}

// Strategy 3: Composite (directed first, then relevance)
class CompositeArbiter implements TurnArbiter {
  private directed = new DirectedArbiter();
  private relevance = new RelevanceArbiter();

  selectResponder(event, candidates, context): PersonaUser | null {
    // Direct address takes precedence
    const directed = this.directed.selectResponder(event, candidates, context);
    if (directed) return directed;

    // Fall back to topic relevance
    return this.relevance.selectResponder(event, candidates, context);
  }
}
```

### Response Routing

```typescript
// In PersonaResponseGenerator.ts - after generating response
async postResponse(
  response: string,
  originalMessage: InboxMessage,
  roomId: UUID
): Promise<void> {
  if (originalMessage.sourceModality === 'voice') {
    // Voice response: Send to TTS, then inject audio into call
    await VoiceOrchestrator.instance.speakResponse(
      originalMessage.voiceSessionId!,
      this.personaId,
      response
    );

    // Also post as text for transcript
    await this.postToChat(roomId, response, { isVoiceTranscript: true });
  } else {
    // Normal text response
    await this.postToChat(roomId, response);
  }
}
```

---

## What We Validated

Browser-side proof of concept confirmed:
- âœ… getUserMedia captures mic audio
- âœ… WebSocket streams audio to Rust
- âœ… Rust receives and processes audio
- âœ… Rust sends audio back via WebSocket
- âœ… Browser plays received audio

Issues with current implementation (expected, design was hacky):
- âŒ Echo (no mix-minus)
- âŒ High latency (~1 second)
- âŒ Choppy audio (buffer issues)
- âŒ No STT/TTS integration

These are all fixable with proper architecture above.
