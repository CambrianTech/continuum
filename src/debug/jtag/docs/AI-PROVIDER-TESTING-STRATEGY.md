# AIProviderWorker Testing Strategy - Test Every Level

## Philosophy: Test in Isolation, Then Integrate

Each component is testable independently BEFORE integrating with the next layer.

## Testing Pyramid

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   System Tests      â”‚  â† PersonaUser end-to-end
                    â”‚ (Full integration)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–²
                           â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  IPC Tests           â”‚  â† TypeScript â†” Rust
                â”‚ (Client â†” Worker)    â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–²
                           â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Integration Tests          â”‚  â† Rust worker with real APIs
            â”‚ (Worker + Real Ollama/APIs)  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–²
                           â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚        Unit Tests                  â”‚  â† Individual modules
          â”‚  (Providers, Protocol, Parsers)    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Level 1: Unit Tests (Rust - No External Dependencies)

Test individual provider modules with mocked HTTP/Ollama responses.

### Test Files Structure
```
workers/ai-provider/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ providers/
â”‚   â”‚   â”œâ”€â”€ ollama.rs
â”‚   â”‚   â”œâ”€â”€ anthropic.rs
â”‚   â”‚   â””â”€â”€ openai.rs
â”‚   â””â”€â”€ messages.rs
â””â”€â”€ tests/
    â”œâ”€â”€ unit/
    â”‚   â”œâ”€â”€ ollama_tests.rs
    â”‚   â”œâ”€â”€ anthropic_tests.rs
    â”‚   â”œâ”€â”€ openai_tests.rs
    â”‚   â””â”€â”€ messages_tests.rs
    â””â”€â”€ ...
```

### Example: Ollama Provider Unit Test
```rust
// tests/unit/ollama_tests.rs
#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_parse_ollama_response() {
        let json = r#"{
            "response": "Hello world",
            "done": true,
            "total_duration": 5000000
        }"#;

        let response = parse_ollama_response(json).unwrap();
        assert_eq!(response.text, "Hello world");
        assert_eq!(response.done, true);
    }

    #[tokio::test]
    async fn test_build_ollama_request() {
        let request = GenerateRequest {
            provider: "ollama".into(),
            model: "llama3.2".into(),
            prompt: "Test prompt".into(),
            stream: false,
        };

        let ollama_req = build_ollama_request(&request);
        assert_eq!(ollama_req.model, "llama3.2");
        assert_eq!(ollama_req.prompt, "Test prompt");
        assert_eq!(ollama_req.stream, false);
    }
}
```

### Run Unit Tests
```bash
cd workers/ai-provider
cargo test --lib

# Test specific module
cargo test --lib ollama_tests

# With output
cargo test --lib -- --nocapture
```

**Status**: âœ… Passes = Module logic correct, ready for integration

---

## Level 2: Integration Tests (Rust - Real APIs)

Test worker with real Ollama/API services running.

### Test Files Structure
```
workers/ai-provider/tests/
â””â”€â”€ integration/
    â”œâ”€â”€ ollama_integration_test.rs
    â”œâ”€â”€ anthropic_integration_test.rs
    â””â”€â”€ streaming_test.rs
```

### Example: Ollama Integration Test
```rust
// tests/integration/ollama_integration_test.rs
use ai_provider_worker::providers::ollama;

#[tokio::test]
#[ignore] // Requires Ollama running
async fn test_ollama_generate_real() {
    // This hits real Ollama service at localhost:11434
    let request = GenerateRequest {
        provider: "ollama".into(),
        model: "llama3.2:latest".into(),
        prompt: "Say hello in 3 words".into(),
        stream: false,
    };

    let response = ollama::generate(request, &default_provider())
        .await
        .expect("Ollama should generate response");

    assert!(!response.text.is_empty());
    assert!(response.text.len() < 100); // Should be short
    println!("Response: {}", response.text);
}

#[tokio::test]
#[ignore]
async fn test_ollama_streaming_real() {
    let request = GenerateRequest {
        provider: "ollama".into(),
        model: "llama3.2:latest".into(),
        prompt: "Count to 5".into(),
        stream: true,
    };

    let mut stream = ollama::generate_stream(request, &default_provider())
        .await
        .expect("Should create stream");

    let mut chunks = Vec::new();
    while let Some(chunk) = stream.next().await {
        let chunk = chunk.expect("Chunk should be valid");
        chunks.push(chunk);
    }

    assert!(chunks.len() > 1, "Should receive multiple chunks");
    println!("Received {} chunks", chunks.len());
}
```

### Run Integration Tests
```bash
# Ensure Ollama is running first
ollama serve &

# Run integration tests
cd workers/ai-provider
cargo test --test '*' -- --ignored

# Test specific integration
cargo test --test ollama_integration_test -- --ignored --nocapture
```

**Status**: âœ… Passes = Worker correctly talks to real APIs

---

## Level 3: Worker Standalone Tests (No System Integration)

Test the full worker binary with a simple TypeScript client (no system deps).

### Standalone Test Client
```typescript
// workers/ai-provider/test/standalone-client.ts
import { AIProviderWorkerClient } from './AIProviderWorkerClient';

async function testStandalone() {
  console.log('ğŸ§ª Testing AIProviderWorker standalone...');

  const client = new AIProviderWorkerClient('/tmp/test-ai-provider.sock');
  await client.connect();

  // Test 1: Ollama generation
  console.log('\nğŸ“ Test 1: Ollama generation');
  const response = await client.generate({
    provider: 'ollama',
    model: 'llama3.2:latest',
    prompt: 'Say hello',
    stream: false
  });
  console.log('âœ… Response:', response.text);

  // Test 2: Streaming
  console.log('\nğŸŒŠ Test 2: Streaming generation');
  const chunks: string[] = [];
  for await (const chunk of client.generateStream({
    provider: 'ollama',
    model: 'llama3.2:latest',
    prompt: 'Count to 3'
  })) {
    chunks.push(chunk);
    process.stdout.write(chunk);
  }
  console.log(`\nâœ… Received ${chunks.length} chunks`);

  // Test 3: Embeddings
  console.log('\nğŸ”¢ Test 3: Embeddings');
  const embeddings = await client.embeddings({
    provider: 'ollama',
    model: 'nomic-embed-text',
    text: 'This is a test'
  });
  console.log(`âœ… Embedding dimension: ${embeddings.length}`);

  // Test 4: Health check
  console.log('\nâ¤ï¸  Test 4: Health check');
  const health = await client.ping();
  console.log('âœ… Health:', health);

  await client.disconnect();
  console.log('\nâœ… All standalone tests passed!');
}

testStandalone().catch(console.error);
```

### Run Standalone Tests
```bash
# Terminal 1: Start worker manually
cd workers/ai-provider
cargo run --release -- /tmp/test-ai-provider.sock

# Terminal 2: Run test client
npx tsx workers/ai-provider/test/standalone-client.ts
```

**Status**: âœ… Passes = Worker IPC protocol works correctly

---

## Level 4: IPC Tests (TypeScript Client â†” Rust Worker)

Test TypeScript client library with real worker.

### IPC Test Suite
```typescript
// tests/integration/ai-provider-worker-ipc.test.ts
import { describe, it, expect, beforeAll, afterAll } from 'vitest';
import { AIProviderWorkerClient } from '@shared/ipc/ai-provider/AIProviderWorkerClient';
import { spawn, ChildProcess } from 'child_process';

describe('AIProviderWorker IPC', () => {
  let worker: ChildProcess;
  let client: AIProviderWorkerClient;
  const socketPath = '/tmp/test-ai-provider-ipc.sock';

  beforeAll(async () => {
    // Start worker
    worker = spawn(
      'workers/ai-provider/target/release/ai-provider-worker',
      [socketPath],
      { stdio: 'ignore', detached: true }
    );
    worker.unref();

    // Wait for socket
    await new Promise(resolve => setTimeout(resolve, 1000));

    // Connect client
    client = new AIProviderWorkerClient(socketPath);
    await client.connect();
  });

  afterAll(async () => {
    await client.disconnect();
    if (worker.pid) process.kill(worker.pid);
  });

  it('should generate text via Ollama', async () => {
    const response = await client.generate({
      provider: 'ollama',
      model: 'llama3.2:latest',
      prompt: 'Say hello',
      stream: false
    });

    expect(response.text).toBeTruthy();
    expect(response.text.length).toBeGreaterThan(0);
  });

  it('should stream tokens', async () => {
    const chunks: string[] = [];

    for await (const chunk of client.generateStream({
      provider: 'ollama',
      model: 'llama3.2:latest',
      prompt: 'Count to 3'
    })) {
      chunks.push(chunk);
    }

    expect(chunks.length).toBeGreaterThan(1);
    expect(chunks.join('')).toBeTruthy();
  });

  it('should generate embeddings', async () => {
    const embeddings = await client.embeddings({
      provider: 'ollama',
      model: 'nomic-embed-text',
      text: 'Test text'
    });

    expect(Array.isArray(embeddings)).toBe(true);
    expect(embeddings.length).toBeGreaterThan(0);
  });

  it('should handle errors gracefully', async () => {
    await expect(
      client.generate({
        provider: 'nonexistent',
        model: 'fake',
        prompt: 'Test',
        stream: false
      })
    ).rejects.toThrow();
  });

  it('should report health status', async () => {
    const health = await client.ping();

    expect(health.providers).toBeDefined();
    expect(health.uptime_ms).toBeGreaterThan(0);
  });
});
```

### Run IPC Tests
```bash
# Build worker first
npm run worker:build

# Run IPC test suite
npx vitest tests/integration/ai-provider-worker-ipc.test.ts
```

**Status**: âœ… Passes = Client-worker communication works

---

## Level 5: System Integration Tests (With AIProviderDaemon)

Test through the full system with AIProviderDaemon routing.

### System Integration Test
```typescript
// tests/integration/ai-provider-system.test.ts
import { describe, it, expect, beforeAll } from 'vitest';
import { Commands } from '@system/core/shared/Commands';

describe('AIProviderDaemon with Rust Worker', () => {
  beforeAll(async () => {
    // Ensure system is running with worker enabled
    process.env.USE_RUST_AI_PROVIDER = 'true';
  });

  it('should route generation to Rust worker', async () => {
    const result = await Commands.execute('ai/generate', {
      provider: 'ollama',
      model: 'llama3.2:latest',
      messages: [
        { role: 'user', content: 'Say hello in 3 words' }
      ]
    });

    expect(result.success).toBe(true);
    expect(result.text).toBeTruthy();
  });

  it('should fallback to TypeScript on worker failure', async () => {
    // Simulate worker failure by killing it
    const { exec } = require('child_process');
    exec('pkill -f ai-provider-worker');

    await new Promise(resolve => setTimeout(resolve, 1000));

    // Should still work via fallback
    const result = await Commands.execute('ai/generate', {
      provider: 'ollama',
      model: 'llama3.2:latest',
      messages: [{ role: 'user', content: 'Test' }]
    });

    expect(result.success).toBe(true);
  });
});
```

### Run System Tests
```bash
# Ensure system is running
npm start

# Run system integration tests
npx vitest tests/integration/ai-provider-system.test.ts
```

**Status**: âœ… Passes = Full system integration works

---

## Level 6: End-to-End Tests (PersonaUser)

Test with real PersonaUser autonomous loop.

### E2E Test
```typescript
// tests/e2e/persona-ai-inference.test.ts
import { describe, it, expect } from 'vitest';
import { Commands } from '@system/core/shared/Commands';

describe('PersonaUser AI Inference (E2E)', () => {
  it('should complete autonomous task with streaming', async () => {
    // Create test task for Helper AI
    const task = await Commands.execute('task/create', {
      assignee: 'helper-ai-id',
      description: 'Explain what Rust is in 2 sentences',
      priority: 0.8
    });

    // Wait for persona to process
    await new Promise(resolve => setTimeout(resolve, 10000));

    // Verify task completed
    const taskResult = await Commands.execute('task/get', {
      taskId: task.id
    });

    expect(taskResult.status).toBe('completed');
    expect(taskResult.outcome).toContain('Rust');
  });
});
```

### Run E2E Tests
```bash
npm start
npx vitest tests/e2e/persona-ai-inference.test.ts --timeout=30000
```

**Status**: âœ… Passes = Real AI agents working with Rust worker

---

## Testing Workflow (Build Order)

```bash
# Step 1: Unit tests (no dependencies)
cd workers/ai-provider
cargo test --lib
# âœ… Pass â†’ Continue

# Step 2: Integration tests (needs Ollama running)
ollama serve &
cargo test --test '*' -- --ignored
# âœ… Pass â†’ Continue

# Step 3: Build release binary
cargo build --release
# âœ… Builds â†’ Continue

# Step 4: Standalone client test
cargo run --release -- /tmp/test.sock &
npx tsx workers/ai-provider/test/standalone-client.ts
# âœ… Pass â†’ Continue

# Step 5: IPC tests (TypeScript â†” Rust)
npx vitest tests/integration/ai-provider-worker-ipc.test.ts
# âœ… Pass â†’ Continue

# Step 6: Deploy to system (flag OFF)
npm run build
npm start
# âœ… Worker runs but not used â†’ Continue

# Step 7: System integration tests (with fallback)
npx vitest tests/integration/ai-provider-system.test.ts
# âœ… Pass â†’ Continue

# Step 8: Enable flag and test
USE_RUST_AI_PROVIDER=true npm start
# âœ… System works â†’ Continue

# Step 9: E2E tests with real personas
npx vitest tests/e2e/persona-ai-inference.test.ts
# âœ… Pass â†’ Production ready!
```

## Test Coverage Goals

- **Unit tests**: 90%+ coverage of provider modules
- **Integration tests**: All providers + streaming + embeddings
- **IPC tests**: All message types + error handling
- **System tests**: Routing + fallback + health monitoring
- **E2E tests**: Real PersonaUser tasks complete successfully

## Continuous Testing

```typescript
// .github/workflows/ai-provider-worker-tests.yml
name: AI Provider Worker Tests

on: [push, pull_request]

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Rust unit tests
        run: |
          cd workers/ai-provider
          cargo test --lib

  integration-tests:
    runs-on: ubuntu-latest
    services:
      ollama:
        image: ollama/ollama:latest
    steps:
      - uses: actions/checkout@v2
      - name: Pull test model
        run: ollama pull llama3.2:latest
      - name: Rust integration tests
        run: |
          cd workers/ai-provider
          cargo test --test '*' -- --ignored

  ipc-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Build worker
        run: |
          cd workers/ai-provider
          cargo build --release
      - name: TypeScript IPC tests
        run: npx vitest tests/integration/ai-provider-worker-ipc.test.ts
```

## Quick Test Commands

```bash
# Test everything at once (requires Ollama running)
npm run test:ai-worker

# Test just unit tests (fast, no dependencies)
npm run test:ai-worker:unit

# Test with real APIs (slow, needs services)
npm run test:ai-worker:integration

# Test IPC only
npm run test:ai-worker:ipc

# Test system integration
npm run test:ai-worker:system

# Test E2E with personas
npm run test:ai-worker:e2e
```

## Summary

âœ… **Every component is testable in isolation**
âœ… **Each level builds on previous level's passing tests**
âœ… **No integration until unit tests pass**
âœ… **No system deployment until IPC tests pass**
âœ… **No production until E2E tests pass**

**Result: Confidence at every step, no surprises in production!**
