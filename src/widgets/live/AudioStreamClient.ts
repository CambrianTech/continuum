/**
 * AudioStreamClient - WebSocket client for real-time audio streaming
 *
 * Connects to the Rust streaming-core call server for:
 * - Joining/leaving calls
 * - Streaming microphone audio
 * - Receiving mix-minus audio (everyone except self)
 * - Playing back received audio
 */

// Import generated types from Rust (single source of truth)
// Generated by: cargo test -p streaming-core
import type { CallMessage } from '../../shared/generated/CallMessage';

// Audio constants - SINGLE SOURCE OF TRUTH
import {
  AUDIO_SAMPLE_RATE,
  AUDIO_FRAME_SIZE,
  CALL_SERVER_URL,
} from '../../shared/AudioConstants';

/** Binary frame type discriminator (first byte of every binary WebSocket message) */
const FRAME_KIND_AUDIO = 0x01;
const FRAME_KIND_VIDEO = 0x02;
const FRAME_KIND_AVATAR_STATE = 0x03;

/** Video frame header size in bytes (matches Rust VideoFrameHeader::WIRE_SIZE) */
const VIDEO_HEADER_SIZE = 16;

/** Transcription result from Whisper STT */
export interface TranscriptionResult {
  userId: string;
  displayName: string;
  text: string;
  confidence: number;
  language: string;
}

/** Decoded video frame from the server */
export interface VideoFrameEvent {
  width: number;
  height: number;
  /** Pixel format: 0=RGBA8, 1=NV12, 2=VP8, 3=H264, 4=JPEG */
  pixelFormat: number;
  timestampMs: number;
  sequence: number;
  /** Raw pixel data (Uint8Array view into the ArrayBuffer) */
  data: Uint8Array;
}

/** Avatar state update from the server */
export interface AvatarUpdateEvent {
  persona_id: string;
  speaking: boolean;
  listening: boolean;
  emotion: string;
  viseme: number;
  viseme_weight: number;
  head_rotation: [number, number, number];
  gaze_target: [number, number];
}

interface AudioStreamClientOptions {
  /** WebSocket server URL (default: CALL_SERVER_URL from AudioConstants) */
  serverUrl?: string;
  /** Sample rate for audio (default: AUDIO_SAMPLE_RATE from AudioConstants) */
  sampleRate?: number;
  /** Frame size in samples (default: AUDIO_FRAME_SIZE from AudioConstants) */
  frameSize?: number;
  /** Callback when participant joins */
  onParticipantJoined?: (userId: string, displayName: string) => void;
  /** Callback when participant leaves */
  onParticipantLeft?: (userId: string) => void;
  /** Callback for connection status changes */
  onConnectionChange?: (connected: boolean) => void;
  /** Callback for microphone audio level (0.0 to 1.0, called ~30x/sec) */
  onMicLevel?: (level: number) => void;
  /** Callback when speech is transcribed (VAD-triggered, Whisper STT) */
  onTranscription?: (result: TranscriptionResult) => void;
  /** Callback when a video frame arrives from another participant */
  onVideoFrame?: (frame: VideoFrameEvent) => void;
  /** Callback when an avatar state update arrives */
  onAvatarUpdate?: (update: AvatarUpdateEvent) => void;
}

export class AudioStreamClient {
  private ws: WebSocket | null = null;
  private audioContext: AudioContext | null = null;
  private mediaStream: MediaStream | null = null;

  // Mic capture worklet (off main thread)
  private micWorkletNode: AudioWorkletNode | null = null;

  // Playback worklet (off main thread) - decodes AND plays
  private playbackWorkletNode: AudioWorkletNode | null = null;

  private micWorkletReady = false;
  private playbackWorkletReady = false;

  // Speaker (output) state
  private speakerMuted = false;
  private speakerVolume = 1.0;

  // Mic mute state (tracked locally for defense in depth)
  private micMuted = false;

  private serverUrl: string;
  private sampleRate: number;
  private frameSize: number;
  private options: AudioStreamClientOptions;

  private callId: string | null = null;
  private userId: string | null = null;
  private displayName: string | null = null;

  constructor(options: AudioStreamClientOptions = {}) {
    this.serverUrl = options.serverUrl || CALL_SERVER_URL;
    this.sampleRate = options.sampleRate || AUDIO_SAMPLE_RATE;
    this.frameSize = options.frameSize || AUDIO_FRAME_SIZE;
    this.options = options;
  }

  /**
   * Connect to the call server and join a call
   */
  async join(callId: string, userId: string, displayName: string): Promise<void> {
    this.callId = callId;
    this.userId = userId;
    this.displayName = displayName;

    // Initialize audio context and playback worklet FIRST (before WebSocket)
    // This ensures we're ready to play audio as soon as it arrives
    await this.initializePlayback();

    // Create WebSocket connection
    return new Promise((resolve, reject) => {
      try {
        this.ws = new WebSocket(this.serverUrl);
        // CRITICAL: Set binary type to arraybuffer for raw audio data
        // This eliminates base64 encoding overhead (~33%) for real-time audio
        this.ws.binaryType = 'arraybuffer';

        this.ws.onopen = () => {
          console.log('AudioStreamClient: Connected to call server');
          this.options.onConnectionChange?.(true);

          // Send join message (browser clients are always human, not AI)
          const joinMsg: CallMessage = {
            type: 'Join',
            call_id: callId,
            user_id: userId,
            display_name: displayName,
            is_ai: false,
          };
          this.ws?.send(JSON.stringify(joinMsg));
          resolve();
        };

        this.ws.onmessage = (event) => {
          // Binary frames: FrameKind-prefixed (audio, video, avatar state)
          if (event.data instanceof ArrayBuffer) {
            this.handleBinaryFrame(event.data);
          } else {
            // Text frames are JSON (transcriptions, join/leave notifications)
            this.handleMessage(event.data);
          }
        };

        this.ws.onerror = (error) => {
          console.error('AudioStreamClient: WebSocket error:', error);
          reject(error);
        };

        this.ws.onclose = () => {
          console.log('AudioStreamClient: Disconnected from call server');
          this.options.onConnectionChange?.(false);
          this.cleanup();
        };
      } catch (error) {
        reject(error);
      }
    });
  }

  /**
   * Initialize playback worklet - runs on AUDIO THREAD, not main thread
   * Decoding and playback all happen off main thread
   */
  private async initializePlayback(): Promise<void> {
    if (!this.audioContext) {
      this.audioContext = new AudioContext({ sampleRate: this.sampleRate });
    }

    // Load playback worklet module
    if (!this.playbackWorkletReady) {
      try {
        const workletUrl = new URL('./audio-playback-worklet.js', import.meta.url).href;
        await this.audioContext.audioWorklet.addModule(workletUrl);
        this.playbackWorkletReady = true;
      } catch (error) {
        console.error('AudioStreamClient: Failed to load playback worklet:', error);
        throw error;
      }
    }

    // Create playback worklet node - ALL audio processing on audio thread
    this.playbackWorkletNode = new AudioWorkletNode(this.audioContext, 'playback-processor');

    // Connect to destination (speakers)
    this.playbackWorkletNode.connect(this.audioContext.destination);

    // Set initial mute/volume state
    this.playbackWorkletNode.port.postMessage({ type: 'mute', muted: this.speakerMuted });
    this.playbackWorkletNode.port.postMessage({ type: 'volume', volume: this.speakerVolume });

    console.log('AudioStreamClient: Playback worklet ready (off main thread)');
  }

  /**
   * Leave the current call
   */
  leave(): void {
    if (this.ws && this.ws.readyState === WebSocket.OPEN) {
      const leaveMsg: CallMessage = { type: 'Leave' };
      this.ws.send(JSON.stringify(leaveMsg));
    }
    this.cleanup();
  }

  /**
   * Start streaming microphone audio
   * Uses AudioWorklet for processing on audio thread (NOT main thread)
   */
  async startMicrophone(): Promise<void> {
    // Idempotent: already streaming, nothing to do
    if (this.micWorkletNode) return;

    if (!this.ws || this.ws.readyState !== WebSocket.OPEN) {
      throw new Error('Not connected to call server');
    }

    // Ensure audio context exists
    if (!this.audioContext) {
      this.audioContext = new AudioContext({ sampleRate: this.sampleRate });
    }

    // Load mic capture worklet module (runs on audio rendering thread, not main thread)
    if (!this.micWorkletReady) {
      try {
        const workletUrl = new URL('./audio-worklet-processor.js', import.meta.url).href;
        await this.audioContext.audioWorklet.addModule(workletUrl);
        this.micWorkletReady = true;
      } catch (error) {
        console.error('AudioStreamClient: Failed to load mic worklet:', error);
        throw error;
      }
    }

    // Get microphone stream
    this.mediaStream = await navigator.mediaDevices.getUserMedia({
      audio: {
        sampleRate: this.sampleRate,
        channelCount: 1,
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
      },
    });

    // Create audio processing pipeline using AudioWorklet (off main thread)
    const source = this.audioContext.createMediaStreamSource(this.mediaStream);

    // Create AudioWorkletNode - processing happens on audio thread
    this.micWorkletNode = new AudioWorkletNode(this.audioContext, 'microphone-processor');

    // Listen for audio frames from the worklet (via MessagePort)
    // Worklet sends { frame: Float32Array, level: number }
    this.micWorkletNode.port.onmessage = (event) => {
      const { frame, level } = event.data;
      // Send audio to server
      this.sendAudioFrame(frame);
      // Notify UI of mic level for visual feedback
      this.options.onMicLevel?.(level);
    };

    // Connect: mic -> worklet -> (nowhere, we just capture)
    source.connect(this.micWorkletNode);
    // Don't connect to destination - we're just capturing, not playing back locally

    console.log('AudioStreamClient: Microphone streaming started (AudioWorklet - off main thread)');
  }

  /**
   * Stop streaming microphone audio
   */
  stopMicrophone(): void {
    if (this.micWorkletNode) {
      this.micWorkletNode.disconnect();
      this.micWorkletNode.port.close();
      this.micWorkletNode = null;
    }

    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach((track) => track.stop());
      this.mediaStream = null;
    }

    console.log('AudioStreamClient: Microphone streaming stopped');
  }

  /**
   * Set mic mute status (your input to others)
   * Tracked both client-side (to stop sending) and server-side (to skip processing)
   */
  setMuted(muted: boolean): void {
    this.micMuted = muted; // Track locally to stop sending audio
    if (this.ws && this.ws.readyState === WebSocket.OPEN) {
      const muteMsg: CallMessage = { type: 'Mute', muted };
      this.ws.send(JSON.stringify(muteMsg));
      console.log(`AudioStreamClient: Mute set to ${muted}`);
    }
  }

  /**
   * Set speaker muted (your output - what you hear)
   */
  setSpeakerMuted(muted: boolean): void {
    this.speakerMuted = muted;
    // Send to playback worklet (runs on audio thread)
    if (this.playbackWorkletNode) {
      this.playbackWorkletNode.port.postMessage({ type: 'mute', muted });
    }
  }

  /**
   * Set speaker volume (0.0 to 1.0)
   */
  setSpeakerVolume(volume: number): void {
    this.speakerVolume = Math.max(0, Math.min(1, volume));
    // Send to playback worklet (runs on audio thread)
    if (this.playbackWorkletNode) {
      this.playbackWorkletNode.port.postMessage({ type: 'volume', volume: this.speakerVolume });
    }
  }

  /**
   * Check if connected
   */
  get isConnected(): boolean {
    return this.ws?.readyState === WebSocket.OPEN;
  }

  /**
   * Handle incoming JSON WebSocket messages (transcriptions, join/leave notifications)
   * Audio now comes as binary frames - see handleBinaryAudio()
   */
  private handleMessage(data: string): void {
    try {
      const msg = JSON.parse(data) as CallMessage;

      switch (msg.type) {
        case 'ParticipantJoined':
          this.options.onParticipantJoined?.(msg.user_id, msg.display_name);
          break;
        case 'ParticipantLeft':
          this.options.onParticipantLeft?.(msg.user_id);
          break;
        case 'Transcription':
          // [STEP 8] Browser received transcription from Rust WebSocket
          console.log(`[STEP 8] ðŸŽ§ Browser received transcription: "${msg.text.slice(0, 50)}..." from ${msg.display_name}`);
          this.options.onTranscription?.({
            userId: msg.user_id,
            displayName: msg.display_name,
            text: msg.text,
            confidence: msg.confidence,
            language: msg.language,
          });
          break;
        case 'AvatarUpdate':
          this.options.onAvatarUpdate?.(msg as unknown as AvatarUpdateEvent);
          break;
      }
    } catch (error) {
      console.error('AudioStreamClient: Failed to parse message:', error);
    }
  }

  /**
   * Send audio frame to server as BINARY WebSocket frame with FrameKind prefix.
   * Wire format: [0x01 (FrameKind::Audio)][PCM16 i16 LE bytes]
   */
  private sendAudioFrame(samples: Float32Array): void {
    if (!this.ws || this.ws.readyState !== WebSocket.OPEN) return;
    if (this.micMuted) return; // Don't send audio when muted (client-side check)

    // Convert Float32 (-1 to 1) to Int16 (-32768 to 32767)
    const int16Data = new Int16Array(samples.length);
    for (let i = 0; i < samples.length; i++) {
      int16Data[i] = Math.max(-32768, Math.min(32767, Math.round(samples[i] * 32767)));
    }

    // Prepend FrameKind::Audio byte (0x01) before PCM data
    const frameBuffer = new ArrayBuffer(1 + int16Data.byteLength);
    const frameView = new Uint8Array(frameBuffer);
    frameView[0] = FRAME_KIND_AUDIO;
    frameView.set(new Uint8Array(int16Data.buffer), 1);

    this.ws.send(frameBuffer);
  }

  /**
   * Handle binary frames from server.
   * First byte is FrameKind discriminator:
   *   0x01 = Audio (PCM16 i16 LE)
   *   0x02 = Video (VideoFrameHeader + pixel data)
   *   0x03 = AvatarState (JSON)
   * Legacy: if first byte is not a valid FrameKind, treat as raw audio (backward compat)
   */
  private handleBinaryFrame(arrayBuffer: ArrayBuffer): void {
    if (arrayBuffer.byteLength < 2) return;

    const view = new DataView(arrayBuffer);
    const frameKind = view.getUint8(0);

    switch (frameKind) {
      case FRAME_KIND_AUDIO:
        this.handleAudioPayload(arrayBuffer, 1); // skip FrameKind byte
        break;

      case FRAME_KIND_VIDEO:
        this.handleVideoPayload(arrayBuffer, 1); // skip FrameKind byte
        break;

      case FRAME_KIND_AVATAR_STATE:
        // JSON-encoded avatar state (unlikely as binary, but handle it)
        break;

      default:
        // Legacy: no FrameKind prefix, entire payload is raw audio
        this.handleAudioPayload(arrayBuffer, 0);
        break;
    }
  }

  /**
   * Handle audio payload â€” PCM16 i16 little-endian samples
   */
  private handleAudioPayload(arrayBuffer: ArrayBuffer, offset: number): void {
    // Ensure audio context is running (needed after user interaction)
    if (this.audioContext?.state === 'suspended') {
      this.audioContext.resume();
    }

    if (!this.playbackWorkletNode) return;

    // Create Int16Array view starting at offset (zero-copy)
    const int16Data = new Int16Array(arrayBuffer, offset);

    // Convert Int16 to Float32 for Web Audio API
    const samples = new Float32Array(int16Data.length);
    for (let i = 0; i < int16Data.length; i++) {
      samples[i] = int16Data[i] / 32768;
    }

    // Transfer Float32Array to worklet (zero-copy via transferable)
    this.playbackWorkletNode.port.postMessage(
      { type: 'audio', samples },
      [samples.buffer]  // Transfer ownership - zero-copy
    );
  }

  /**
   * Handle video payload â€” [VideoFrameHeader (16 bytes)][pixel data]
   * VideoFrameHeader layout (little-endian):
   *   bytes 0-1:  width (u16)
   *   bytes 2-3:  height (u16)
   *   byte  4:    pixelFormat (0=RGBA8, 1=NV12, 2=VP8, 3=H264, 4=JPEG)
   *   byte  5:    flags (reserved)
   *   bytes 6-9:  timestampMs (u32)
   *   bytes 10-13: sequence (u32)
   *   bytes 14-15: reserved
   */
  private handleVideoPayload(arrayBuffer: ArrayBuffer, offset: number): void {
    const remaining = arrayBuffer.byteLength - offset;
    if (remaining < VIDEO_HEADER_SIZE) return;

    const view = new DataView(arrayBuffer, offset);
    const width = view.getUint16(0, true); // little-endian
    const height = view.getUint16(2, true);
    const pixelFormat = view.getUint8(4);
    // byte 5: flags (reserved)
    const timestampMs = view.getUint32(6, true);
    const sequence = view.getUint32(10, true);
    // bytes 14-15: reserved

    const dataOffset = offset + VIDEO_HEADER_SIZE;
    const data = new Uint8Array(arrayBuffer, dataOffset);

    this.options.onVideoFrame?.({
      width,
      height,
      pixelFormat,
      timestampMs,
      sequence,
      data,
    });
  }

  /**
   * Cleanup resources
   */
  private cleanup(): void {
    this.stopMicrophone();

    // Cleanup playback worklet
    if (this.playbackWorkletNode) {
      this.playbackWorkletNode.disconnect();
      this.playbackWorkletNode.port.close();
      this.playbackWorkletNode = null;
    }

    if (this.audioContext) {
      this.audioContext.close();
      this.audioContext = null;
    }

    if (this.ws) {
      this.ws.close();
      this.ws = null;
    }

    this.micWorkletReady = false;
    this.playbackWorkletReady = false;
    this.callId = null;
    this.userId = null;
    this.displayName = null;
  }
}
