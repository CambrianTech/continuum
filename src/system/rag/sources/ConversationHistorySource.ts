/**
 * ConversationHistorySource - Loads chat message history for RAG context
 *
 * Features:
 * - Uses queryWithJoin to load messages + sender info in one query (4.5x faster)
 * - Role assignment: own messages = 'assistant', others = 'user'
 * - Chronological ordering (oldest first for LLM context)
 * - Media attachment metadata in message text
 * - Token budget-aware message limiting
 */

import type { RAGSource, RAGSourceContext, RAGSection } from '../shared/RAGSource';
import type { LLMMessage } from '../shared/RAGTypes';
import { ORM } from '../../../daemons/data-daemon/server/ORM';
import { ChatMessageEntity } from '../../data/entities/ChatMessageEntity';
import { Events } from '../../core/shared/Events';
import { Logger } from '../../core/logging/Logger';

const log = Logger.create('ConversationHistorySource', 'rag');

// Generous DB fetch limit — the allocated token budget is the real constraint.
// 500 messages is well beyond what any model's conversation budget can hold.
const DB_FETCH_LIMIT = 500;

// Patterns for detecting fabricated conversations within a single message body.
// These messages were generated by models that hallucinated entire multi-party
// conversations instead of responding as themselves. They poison LLM context
// and cause cascading failures (cloud AIs adopting "silence protocol").
//
// Formats seen in the wild:
//   "2/16/2026 2:24:03 PM Teacher AI: ..."     (date + time + speaker)
//   "[02:01] Teacher AI: ..."                   (bracketed time + speaker)
//   "[03:00] Helper AI: That's a good point..." (bracketed time + speaker)
//   "Gemini: I'm happy to chat..."              (single-word speaker prefix)
//   "Teacher AI: I think that's a great..."     (multi-word speaker prefix)

// Full date + time at line start
const FABRICATED_DATE_RE = /^\s*\d{1,4}[/-]\d{1,2}[/-]\d{1,4}\s+\d{1,2}:\d{2}\s+[A-Z]/gm;
// Bracketed time at line start: [02:01], [14:30], etc.
const FABRICATED_BRACKET_TIME_RE = /^\s*\[\d{1,2}:\d{2}\]\s+[A-Z]/gm;
// Multi-word speaker prefix: "Teacher AI:", "Helper AI:", "CodeReview AI:"
const FABRICATED_SPEAKER_RE = /^[A-Z][a-zA-Z]+\s+[A-Z][a-zA-Z]+(?:\s+[A-Z][a-zA-Z]+)*:\s+\S/gm;
// Single-word known AI speaker prefix: "Gemini:", "Groq:", "Together:", "Fireworks:"
const FABRICATED_SINGLE_SPEAKER_RE = /^(?:Gemini|Groq|Together|Fireworks|Claude|GPT|Local|Joel|Anonymous|Qwen|DeepSeek|Grok|Candle|Helper|Teacher|CodeReview):\s+\S/gm;

/**
 * Check if a message body is a fabricated multi-party conversation.
 * Returns true if the message contains 3+ timestamped lines,
 * 4+ multi-word speaker prefixes with 2+ distinct names, or
 * 3+ single-word known AI speaker prefixes.
 */
function isFabricatedConversation(text: string): boolean {
  if (!text || text.length < 60) return false;

  // Check 1: Full date+time timestamped speaker lines
  const dateMatches = text.match(FABRICATED_DATE_RE);
  if (dateMatches && dateMatches.length >= 3) return true;

  // Check 2: Bracketed [HH:MM] timestamped lines
  const bracketMatches = text.match(FABRICATED_BRACKET_TIME_RE);
  if (bracketMatches && bracketMatches.length >= 3) return true;

  // Check 3: Multi-word speaker prefixes with distinct names
  const speakerMatches = text.match(FABRICATED_SPEAKER_RE);
  if (speakerMatches && speakerMatches.length >= 4) {
    const names = new Set(speakerMatches.map(m => m.split(':')[0].trim()));
    if (names.size >= 2) return true;
  }

  // Check 4: Single-word known AI speaker prefixes
  const singleMatches = text.match(FABRICATED_SINGLE_SPEAKER_RE);
  if (singleMatches && singleMatches.length >= 3) {
    const names = new Set(singleMatches.map(m => m.split(':')[0].trim()));
    if (names.size >= 2) return true;
  }

  return false;
}

// ── Bare tool call detection ──────────────────────────────────────
// When an AI outputs a tool call as plain text (not a proper tool_use block),
// it gets saved as a chat message. Other AIs see it in history and copy the
// broken format, causing cascading hallucination of invalid tool syntax.
//
// These patterns catch common failed tool call formats:
// - "code/tree {"path":"."}"  (tool name + JSON)
// - "code/verify filePath:"..."" (tool name + key:value params)
// - '{"maxDepth":1,"path":"."}' (bare JSON)
// - "<function=code_tree>{...}</function>" (Groq function-style)

const TOOL_NAME_RE = /^[a-z][a-z0-9_]*(?:\/[a-z][a-z0-9_]*)+/;  // path-like: code/tree, data/list
const BARE_JSON_RE = /^\s*\{[\s\S]*\}\s*$/;
const FUNCTION_CALL_RE = /^<?function[=\s]/;

/**
 * Check if a message is a bare tool call attempt that would poison context.
 * Returns the tool name if detected, null otherwise.
 */
function detectBareToolCall(text: string): string | null {
  if (!text || text.length < 4 || text.length > 500) return null;

  const trimmed = text.trim();

  // Pattern 1: tool/name followed by JSON or key:value params (no preceding prose)
  const toolMatch = trimmed.match(TOOL_NAME_RE);
  if (toolMatch) {
    const afterName = trimmed.slice(toolMatch[0].length).trim();
    // Must have params (JSON or key:"value" or key:value) and no preceding text
    if (afterName.startsWith('{') || /^[a-zA-Z]+[:=]/.test(afterName)) {
      return toolMatch[0];
    }
    // Just the tool name alone with nothing else (e.g. "code/tree")
    if (afterName.length === 0 && trimmed === toolMatch[0]) {
      return toolMatch[0];
    }
  }

  // Pattern 2: Pure JSON (no surrounding text) — likely bare params
  if (BARE_JSON_RE.test(trimmed)) {
    return 'unknown';
  }

  // Pattern 3: Groq function-style: <function=name>{json}</function> or function=name>{json}
  if (FUNCTION_CALL_RE.test(trimmed)) {
    return trimmed.match(/function[=\s]+(\S+)/)?.[1] ?? 'unknown';
  }

  return null;
}

type MessageWithSender = ChatMessageEntity & { sender?: { displayName: string; userType: string } };

/** Cache entry for room messages — maintained by event subscription */
interface MessageCacheEntry {
  messages: MessageWithSender[];
  fetchedAt: number;
  limit: number;
}

/** In-flight request entry for single-flight coalescing */
interface InflightEntry {
  promise: Promise<MessageWithSender[]>;
  limit: number;
}

export class ConversationHistorySource implements RAGSource {
  readonly name = 'conversation-history';
  readonly priority = 80;  // High - conversation is core context
  readonly defaultBudgetPercent = 25;  // Gets largest share of budget

  // Room message cache: event-driven freshness. 30s TTL is a safety net only.
  // Primary freshness comes from event subscription updating cache entries.
  private static _roomCache: Map<string, MessageCacheEntry> = new Map();
  private static readonly CACHE_TTL_MS = 30_000;

  // Single-flight coalescing: when multiple personas query the same room
  // simultaneously, only ONE DB query fires. Others await the same promise.
  private static _inflight: Map<string, InflightEntry> = new Map();

  // Event subscription for real-time cache maintenance.
  // New messages update the cache immediately — no staleness, no DB re-query.
  private static _eventSubscribed = false;

  private static initEventSubscription(): void {
    if (ConversationHistorySource._eventSubscribed) return;
    ConversationHistorySource._eventSubscribed = true;

    Events.subscribe(`data:${ChatMessageEntity.collection}:created`, (entity: any) => {
      const msg = entity as ChatMessageEntity;
      if (!msg.roomId) return;

      const cached = ConversationHistorySource._roomCache.get(msg.roomId);
      if (cached) {
        // Prepend new message (cache is newest-first order, reversed later for LLM)
        cached.messages.unshift(msg as MessageWithSender);
        if (cached.messages.length > cached.limit + 10) {
          cached.messages.length = cached.limit; // Trim excess
        }
        cached.fetchedAt = Date.now(); // Reset TTL — cache is now fresh
      }
    });
  }

  /**
   * Access cached raw messages for a room (used by extractArtifacts to avoid duplicate DB query).
   * Returns null if cache is expired or empty — caller should fall back to DB.
   */
  static getCachedRawMessages(roomId: string): MessageWithSender[] | null {
    const cached = ConversationHistorySource._roomCache.get(roomId);
    if (cached && (Date.now() - cached.fetchedAt) < ConversationHistorySource.CACHE_TTL_MS) {
      return cached.messages;
    }
    return null;
  }

  isApplicable(_context: RAGSourceContext): boolean {
    // Always applicable - every RAG build needs conversation context
    return true;
  }

  async load(context: RAGSourceContext, allocatedBudget: number): Promise<RAGSection> {
    const startTime = performance.now();
    ConversationHistorySource.initEventSubscription();

    // The allocated token budget is the ONLY constraint. No guessed message counts.
    // Fetch a generous batch from DB, then trim to exactly fit the budget.
    const fetchLimit = DB_FETCH_LIMIT;

    log.debug(`Fetching up to ${fetchLimit} messages, token budget=${allocatedBudget}`);

    try {
      let messages: MessageWithSender[] = [];

      // Check completed cache first
      const cacheKey = context.roomId;
      const cached = ConversationHistorySource._roomCache.get(cacheKey);
      const now = Date.now();

      if (cached && (now - cached.fetchedAt) < ConversationHistorySource.CACHE_TTL_MS && cached.limit >= fetchLimit) {
        messages = cached.messages.slice(0, fetchLimit);
        log.debug(`Cache hit for room ${context.roomId?.slice(0, 8)} (${messages.length} messages)`);
      } else {
        // Cache miss — use single-flight coalescing to prevent thundering herd.
        // When 16 personas query the same room simultaneously, only the first
        // triggers a DB query. The other 15 await the same promise.
        const inflight = ConversationHistorySource._inflight.get(cacheKey);
        if (inflight && inflight.limit >= fetchLimit) {
          log.debug(`Coalescing request for room ${context.roomId?.slice(0, 8)}`);
          messages = (await inflight.promise).slice(0, fetchLimit);
        } else {
          const fetchPromise = this.fetchMessages(context.roomId, fetchLimit);
          ConversationHistorySource._inflight.set(cacheKey, {
            promise: fetchPromise,
            limit: fetchLimit
          });
          try {
            messages = await fetchPromise;
            ConversationHistorySource._roomCache.set(cacheKey, {
              messages,
              fetchedAt: Date.now(),
              limit: fetchLimit
            });
          } finally {
            ConversationHistorySource._inflight.delete(cacheKey);
          }
        }
      }

      if (messages.length === 0) {
        return this.emptySection(startTime);
      }

      // Messages arrive newest-first from DB. Filter and sanitize in that order.

      // Filter out fabricated conversation messages — hallucinated multi-party
      // conversations that poison context and cause cascading failures.
      let filteredCount = 0;
      const cleanMessages = messages.filter((msg: MessageWithSender) => {
        const text = msg.content?.text || '';
        if (isFabricatedConversation(text)) {
          filteredCount++;
          return false;
        }
        return true;
      });
      if (filteredCount > 0) {
        log.warn(`Filtered ${filteredCount} fabricated conversation messages from history`);
      }

      // Sanitize bare tool call messages — replace with contextual note
      // so other AIs know someone attempted a tool but don't copy the broken syntax
      let sanitizedCount = 0;
      for (const msg of cleanMessages) {
        const text = msg.content?.text || '';
        const toolName = detectBareToolCall(text);
        if (toolName && msg.senderId !== context.personaId) {
          const senderName = (msg as any).sender?.displayName || msg.senderName || 'Someone';
          msg.content = { ...msg.content, text: `[${senderName} used ${toolName}]` };
          sanitizedCount++;
        }
      }
      if (sanitizedCount > 0) {
        log.info(`Sanitized ${sanitizedCount} bare tool call messages from history`);
      }

      // Convert to LLM message format (still newest-first)
      const allLlmMessages: LLMMessage[] = cleanMessages.map((msg: MessageWithSender) => {
        let messageText = msg.content?.text || '';

        // Add media metadata to message text so AIs know images exist
        if (msg.content?.media && msg.content.media.length > 0) {
          const mediaDescriptions = msg.content.media.map((item: { type?: string; filename?: string; mimeType?: string }, idx: number) => {
            const parts = [
              `[${item.type || 'attachment'}${idx + 1}]`,
              item.filename || 'unnamed',
              item.mimeType ? `(${item.mimeType})` : ''
            ].filter(Boolean);
            return parts.join(' ');
          });

          const mediaNote = `\n[Attachments: ${mediaDescriptions.join(', ')} - messageId: ${msg.id}]`;
          messageText += mediaNote;
        }

        // Role assignment: own messages = 'assistant', others = 'user'
        const isOwnMessage = msg.senderId === context.personaId;
        const role = isOwnMessage ? 'assistant' as const : 'user' as const;

        // Get sender name from JOIN result or fallback
        const senderName = (msg as any).sender?.displayName || msg.senderName || 'Unknown';

        // Convert timestamp to number (milliseconds)
        let timestampMs: number | undefined;
        if (msg.timestamp) {
          if (typeof msg.timestamp === 'number') {
            timestampMs = msg.timestamp;
          } else if (typeof msg.timestamp === 'string') {
            timestampMs = new Date(msg.timestamp).getTime();
          } else if (msg.timestamp instanceof Date) {
            timestampMs = msg.timestamp.getTime();
          }
        }

        return {
          role,
          content: messageText,
          name: senderName,
          timestamp: timestampMs
        };
      });

      // ── TOKEN BUDGET ENFORCEMENT WITH CONSOLIDATION ────────────────
      // Two-tier strategy: recent messages verbatim, older messages consolidated.
      // Nothing is silently lost — the AI always sees the full conversation arc.
      //
      // Budget split: 85% for recent verbatim, 15% reserved for consolidated older messages.
      // If everything fits in 85%, the remaining budget rolls into verbatim (no consolidation needed).

      const verbatimBudget = Math.floor(allocatedBudget * 0.85);
      const consolidationBudget = allocatedBudget - verbatimBudget;

      // Pass 1: Fill recent messages verbatim (newest-first) until verbatim budget exhausted
      let verbatimTokens = 0;
      let verbatimCutoff = allLlmMessages.length;
      for (let i = 0; i < allLlmMessages.length; i++) {
        const msgTokens = this.estimateTokens(allLlmMessages[i].content);
        if (verbatimTokens + msgTokens > verbatimBudget) {
          verbatimCutoff = i;
          break;
        }
        verbatimTokens += msgTokens;
      }

      // If everything fit, no consolidation needed — use full budget for verbatim
      if (verbatimCutoff === allLlmMessages.length) {
        // Try to fit more with the consolidation budget too
        let totalTokens = verbatimTokens;
        // Already have all messages, just reverse to chronological
        const budgetedMessages = allLlmMessages.slice().reverse();

        const loadTimeMs = performance.now() - startTime;
        log.debug(`Loaded ${budgetedMessages.length}/${allLlmMessages.length} messages in ${loadTimeMs.toFixed(1)}ms (~${totalTokens}/${allocatedBudget} token budget, all fit)`);

        return {
          sourceName: this.name,
          tokenCount: totalTokens,
          loadTimeMs,
          messages: budgetedMessages,
          metadata: {
            messageCount: budgetedMessages.length,
            totalAvailable: allLlmMessages.length,
            roomId: context.roomId,
            personaId: context.personaId
          }
        };
      }

      // Pass 2: Consolidate older messages that didn't fit verbatim.
      // Compress each to "SenderName: first line..." — preserves conversation
      // arc and topic awareness without consuming full token budget.
      const olderMessages = allLlmMessages.slice(verbatimCutoff); // newest-first still
      const consolidatedLines: string[] = [];
      let consolidatedTokens = 0;

      // Walk oldest-to-newest through the overflow messages
      for (let i = olderMessages.length - 1; i >= 0; i--) {
        const msg = olderMessages[i];
        const firstLine = msg.content.split('\n')[0].slice(0, 120);
        const compressed = `${msg.name}: ${firstLine}`;
        const lineTokens = this.estimateTokens(compressed + '\n');
        if (consolidatedTokens + lineTokens > consolidationBudget) break;
        consolidatedLines.push(compressed);
        consolidatedTokens += lineTokens;
      }

      // Build final message array: consolidated summary + verbatim recent
      const resultMessages: LLMMessage[] = [];
      const totalTokens = verbatimTokens + consolidatedTokens;

      if (consolidatedLines.length > 0) {
        const skippedCount = olderMessages.length - consolidatedLines.length;
        const header = skippedCount > 0
          ? `[Earlier conversation (${olderMessages.length} messages, ${skippedCount} omitted for space):]`
          : `[Earlier conversation (${olderMessages.length} messages):]`;

        resultMessages.push({
          role: 'user' as const,
          content: header + '\n' + consolidatedLines.join('\n'),
          name: 'system-context'
        });
      }

      // Verbatim messages: reverse to chronological (oldest-first)
      const verbatimMessages = allLlmMessages.slice(0, verbatimCutoff).reverse();
      resultMessages.push(...verbatimMessages);

      const loadTimeMs = performance.now() - startTime;
      log.debug(`Loaded ${verbatimMessages.length} verbatim + ${consolidatedLines.length} consolidated (of ${olderMessages.length} older) in ${loadTimeMs.toFixed(1)}ms (~${totalTokens}/${allocatedBudget} token budget)`);

      return {
        sourceName: this.name,
        tokenCount: totalTokens,
        loadTimeMs,
        messages: resultMessages,
        metadata: {
          messageCount: resultMessages.length,
          verbatimCount: verbatimMessages.length,
          consolidatedCount: consolidatedLines.length,
          totalAvailable: allLlmMessages.length,
          roomId: context.roomId,
          personaId: context.personaId
        }
      };
    } catch (error: any) {
      log.error(`Failed to load conversation history: ${error.message}`);
      return this.emptySection(startTime, error.message);
    }
  }

  /** Fetch messages from DB (extracted for caching) */
  private async fetchMessages(roomId: string, maxMessages: number): Promise<MessageWithSender[]> {
    // Try queryWithJoin first (4.5x faster), fall back to regular query
    try {
      const result = await ORM.queryWithJoin<MessageWithSender>({
        collection: ChatMessageEntity.collection,
        filter: { roomId },
        joins: [{
          collection: 'users',
          alias: 'sender',
          localField: 'senderId',
          foreignField: 'id',
          type: 'left',
          select: ['displayName', 'userType']
        }],
        sort: [{ field: 'timestamp', direction: 'desc' }],
        limit: maxMessages
      });

      if (result.success && result.data && result.data.length > 0) {
        return result.data.map((record: { data: MessageWithSender }) => record.data);
      }
    } catch (joinError: any) {
      // queryWithJoin not supported - fall back to regular query
      log.debug(`queryWithJoin not available (${joinError.message}), using regular query`);

      const result = await ORM.query<ChatMessageEntity>({
        collection: ChatMessageEntity.collection,
        filter: { roomId },
        sort: [{ field: 'timestamp', direction: 'desc' }],
        limit: maxMessages
      });

      if (result.success && result.data && result.data.length > 0) {
        return result.data.map((record: { data: ChatMessageEntity }) => record.data as MessageWithSender);
      }
    }
    return [];
  }

  private emptySection(startTime: number, error?: string): RAGSection {
    return {
      sourceName: this.name,
      tokenCount: 0,
      loadTimeMs: performance.now() - startTime,
      messages: [],
      metadata: error ? { error } : {}
    };
  }

  private estimateTokens(text: string): number {
    // Llama tokenizer averages ~3 chars/token (not 4). Use conservative
    // estimate so we don't overshoot the budget.
    return Math.ceil(text.length / 3);
  }
}
